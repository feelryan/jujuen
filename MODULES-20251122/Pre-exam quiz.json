[
  {
    "no": "1",
    "level": "Medium",
    "keywords": "VPC, Firewall Rules, Autoscaling, Service Accounts, Network Tags",
    "question": [
      {
        "t": "你的網路應用程式有數個 VM 執行個體在 VPC 中運作。",
        "en": "Your web application has several VM instances running within a VPC.",
        "wg": [
          { "t": "網路應用程式", "en": "web application", "ps": "noun" },
          { "t": "VM 執行個體", "en": "VM instances", "ps": "noun" },
          { "t": "虛擬私有雲", "en": "VPC", "ps": "noun" }
        ]
      },
      {
        "t": "你想限制執行個體之間的通訊，只允許經授權的 VM 和連接埠，但因為應用程式會自動縮放，所以不想依賴靜態 IP 位址或子網路。",
        "en": "You want to restrict communications between instances to only the VMs and ports you authorize, but you don't want to rely on static IP addresses or subnets because the app can autoscale.",
        "wg": [
          { "t": "限制通訊", "en": "restrict communications", "ps": "verb phrase" },
          { "t": "授權", "en": "authorize", "ps": "verb" },
          { "t": "靜態 IP 位址", "en": "static IP addresses", "ps": "noun" },
          { "t": "自動縮放", "en": "autoscale", "ps": "verb" }
        ]
      },
      {
        "t": "你應該如何限制通訊？",
        "en": "How should you restrict communications?",
        "wg": [
          { "t": "如何", "en": "How", "ps": "adverb" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "使用服務帳戶並設定網路應用程式特定的服務帳戶以擁有存取權。",
        "en": "Use service accounts and configure the web application particular service accounts to have access.",
        "wg": [
          { "t": "服務帳戶", "en": "service accounts", "ps": "noun" },
          { "t": "存取權", "en": "access", "ps": "noun" }
        ]
      },
      {
        "t": "使用 Cloud DNS 且只允許來自授權主機名稱的連線。",
        "en": "Use Cloud DNS and only allow connections from authorized hostnames.",
        "wg": [
          { "t": "Cloud DNS", "en": "Cloud DNS", "ps": "product" },
          { "t": "主機名稱", "en": "hostnames", "ps": "noun" }
        ]
      },
      {
        "t": "使用基於附加到運算執行個體的服務帳戶或網路標記的防火牆規則。",
        "en": "Use firewall rules based on service account or network tags attached to the compute instances.",
        "wg": [
          { "t": "防火牆規則", "en": "firewall rules", "ps": "noun" },
          { "t": "網路標記", "en": "network tags", "ps": "noun" },
          { "t": "運算執行個體", "en": "compute instances", "ps": "noun" }
        ]
      },
      {
        "t": "使用獨立的 VPC 來限制流量。",
        "en": "Use separate VPCs to restrict traffic.",
        "wg": [
          { "t": "獨立的 VPC", "en": "separate VPCs", "ps": "noun" },
          { "t": "流量", "en": "traffic", "ps": "noun" }
        ]
      }
    ],
    "answer": "Option 3",
    "why": {
      "t": "在 GCP 中，處理自動縮放環境的最佳實踐是使用防火牆規則，並透過「服務帳戶」或「網路標記」來定義來源與目的地，而非依賴變動的 IP 位址。服務帳戶比標記更安全且管理更嚴謹。Cloud DNS 負責名稱解析而非存取控制；單獨依賴 IAM 服務帳戶存取權不足以在網路層級阻擋流量。",
      "en": "In GCP, the best practice for autoscaling environments is to use firewall rules that target 'Service Accounts' or 'Network Tags' rather than ephemeral IP addresses. Service accounts are preferred over tags for better security and management. Cloud DNS handles resolution, not access control; IAM service account permissions alone do not block network traffic.",
      "wg": [
        { "t": "最佳實踐", "en": "best practice", "ps": "noun" },
        { "t": "變動的", "en": "ephemeral", "ps": "adjective" }
      ]
    }
  },
  {
    "no": "2",
    "level": "Hard",
    "keywords": "GKE, Machine Utilization, Deployment, Docker, DevOps",
    "question": [
      {
        "t": "你有一個 Python 網路應用程式，具有許多相依性，在生產環境中需要 0.1 CPU 核心和 128 MB 記憶體才能運作。",
        "en": "You have a Python web application with many dependencies that requires 0.1 CPU cores and 128 MB of memory to operate in production.",
        "wg": [
          { "t": "相依性", "en": "dependencies", "ps": "noun" },
          { "t": "CPU 核心", "en": "CPU cores", "ps": "noun" }
        ]
      },
      {
        "t": "你想要監控並最大化機器利用率，同時也想要可靠地部署應用程式的新版本。",
        "en": "You want to monitor and maximize machine utilization. You also want to reliably deploy new versions of the application.",
        "wg": [
          { "t": "最大化", "en": "maximize", "ps": "verb" },
          { "t": "機器利用率", "en": "machine utilization", "ps": "noun" },
          { "t": "部署", "en": "deploy", "ps": "verb" }
        ]
      },
      {
        "t": "你應該採取哪一組步驟？",
        "en": "Which set of steps should you take?",
        "wg": []
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "建立一個使用 f1-micro 類型機器的託管執行個體群組... (使用啟動腳本部署)",
        "en": "Create a managed instance group with f1-micro type machines... (use startup script to deploy)",
        "wg": [
          { "t": "託管執行個體群組", "en": "managed instance group", "ps": "noun" },
          { "t": "啟動腳本", "en": "startup script", "ps": "noun" }
        ]
      },
      {
        "t": "建立一個使用 n1-standard-1 類型機器的託管執行個體群組... (建置映像檔部署)",
        "en": "Create a managed instance group with n1-standard-1 type machines... (build image to deploy)",
        "wg": [
          { "t": "映像檔", "en": "image", "ps": "noun" }
        ]
      },
      {
        "t": "建立一個使用 n1-standard-1 類型機器的 Kubernetes Engine 叢集。從生產分支建置 Docker 映像檔並標記版本號。在預備環境 (staging) 建立 Deployment，測試後推廣至生產環境。",
        "en": "Create a Kubernetes Engine cluster with n1-standard-1 type machines. Build a Docker image from the production branch with all of the dependencies, and tag it with the version number. Create a Kubernetes Deployment in the staging namespace, and then promote it to the production namespace after testing.",
        "wg": [
          { "t": "Kubernetes Engine 叢集", "en": "Kubernetes Engine cluster", "ps": "noun" },
          { "t": "命名空間", "en": "namespace", "ps": "noun" }
        ]
      },
      {
        "t": "建立一個使用 n1-standard-4 類型機器的 Kubernetes Engine 叢集... (標記為 latest 並重啟 pod)",
        "en": "Create a Kubernetes Engine cluster with n1-standard-4 type machines... (tag as 'latest' and restart pods)",
        "wg": [
          { "t": "標記", "en": "tag", "ps": "verb" }
        ]
      }
    ],
    "answer": "Option 3",
    "why": {
      "t": "應用程式資源需求極低 (0.1 CPU/128MB)，使用 VM (MIG) 會造成資源浪費 (因 VM 有最小規格限制且有 OS 開銷)。Kubernetes (GKE) 最適合這種情況，透過 Bin-packing (裝箱) 機制將多個 Pods 塞入節點以最大化利用率。選項 3 遵循 DevOps 最佳實踐：使用特定版本標籤 (非 latest) 並透過 staging 到 production 的流程確保可靠性。",
      "en": "The app has very low resource requirements (0.1 CPU/128MB). Using VMs (MIGs) would waste resources due to minimum VM sizing and OS overhead. Kubernetes (GKE) is ideal here for 'bin-packing' multiple pods onto nodes to maximize utilization. Option 3 follows DevOps best practices: using specific version tags (not 'latest') and a staging-to-production promotion workflow for reliability.",
      "wg": [
        { "t": "裝箱", "en": "bin-packing", "ps": "concept" },
        { "t": "開銷", "en": "overhead", "ps": "noun" }
      ]
    }
  },
  {
    "no": "3",
    "level": "Medium",
    "keywords": "BigQuery, IAM, Billing, Security, Data Warehouse",
    "question": [
      {
        "t": "你的公司使用 BigQuery 作為企業資料倉儲，資料分散在數個 Google Cloud 專案中。",
        "en": "Your company is using BigQuery as its enterprise data warehouse. Data is distributed over several Google Cloud projects.",
        "wg": [
          { "t": "資料倉儲", "en": "data warehouse", "ps": "noun" },
          { "t": "分散的", "en": "distributed", "ps": "adjective" }
        ]
      },
      {
        "t": "所有 BigQuery 上的查詢都需要在單一專案上計費，並確保包含資料的專案不會產生任何查詢費用。",
        "en": "All queries on BigQuery need to be billed on a single project. You want to make sure that no query costs are incurred on the projects that contain the data.",
        "wg": [
          { "t": "計費", "en": "billed", "ps": "verb" },
          { "t": "產生費用", "en": "incurred costs", "ps": "verb phrase" }
        ]
      },
      {
        "t": "使用者應該能夠查詢資料集，但不能編輯它們。你應該如何設定使用者的存取角色？",
        "en": "Users should be able to query the datasets, but not edit them. How should you configure users' access roles?",
        "wg": [
          { "t": "查詢", "en": "query", "ps": "verb" },
          { "t": "編輯", "en": "edit", "ps": "verb" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "將所有使用者加入群組。賦予該群組在計費專案上的 BigQuery user 角色，以及在資料專案上的 BigQuery user 角色。",
        "en": "Add all users to a group. Grant the group the role of BigQuery user on the billing project and BigQuery user on the projects that contain the data.",
        "wg": []
      },
      {
        "t": "賦予該群組在計費專案上的 BigQuery dataViewer 角色，以及在資料專案上的 BigQuery user 角色。",
        "en": "Grant the group the roles of BigQuery dataViewer on the billing project and BigQuery user on the projects that contain the data.",
        "wg": []
      },
      {
        "t": "賦予該群組在計費專案上的 BigQuery jobUser 角色，以及在資料專案上的 BigQuery dataViewer 角色。",
        "en": "Grant the group the roles of BigQuery jobUser on the billing project and BigQuery dataViewer on the projects that contain the data.",
        "wg": [
          { "t": "BigQuery 工作使用者", "en": "BigQuery jobUser", "ps": "role" },
          { "t": "BigQuery 資料檢視者", "en": "BigQuery dataViewer", "ps": "role" }
        ]
      },
      {
        "t": "賦予該群組在計費專案上的 BigQuery dataViewer 角色，以及在資料專案上的 BigQuery jobUser 角色。",
        "en": "Grant the group the roles of BigQuery dataViewer on the billing project and BigQuery jobUser on the projects that contain the data.",
        "wg": []
      }
    ],
    "answer": "Option 3",
    "why": {
      "t": "BigQuery 的計費歸屬於執行查詢作業 (Job) 的專案。為了將費用集中在單一專案，使用者必須在該計費專案擁有 `bigquery.jobUser` 角色 (允許執行查詢)。在資料專案上，為了避免產生費用且限制編輯，僅需賦予 `bigquery.dataViewer` (允許讀取資料但無法發起需付費的 Job)。",
      "en": "BigQuery billing is attributed to the project where the query job runs. To consolidate costs, users need the `bigquery.jobUser` role on the billing project (allowing them to run jobs). On the data projects, to prevent costs and editing, they only need `bigquery.dataViewer` (allowing data read access without permission to spawn billable jobs there).",
      "wg": [
        { "t": "歸屬於", "en": "attributed to", "ps": "verb phrase" },
        { "t": "整合", "en": "consolidate", "ps": "verb" }
      ]
    }
  },
  {
    "no": "4",
    "level": "Medium",
    "keywords": "GKE, Deployment, Service, Rolling Update, kubectl",
    "question": [
      {
        "t": "你有一個部署在 Kubernetes Engine 上的應用程式，使用名為 echo-deployment 的 Deployment，並透過 Service 公開。",
        "en": "You have an application deployed on Kubernetes Engine using a Deployment named echo-deployment. The deployment is exposed using a Service called echoservice.",
        "wg": [
          { "t": "部署", "en": "Deployment", "ps": "noun" },
          { "t": "服務", "en": "Service", "ps": "noun" }
        ]
      },
      {
        "t": "你需要以對應用程式造成最少停機時間的方式執行更新。你應該做什麼？",
        "en": "You need to perform an update to the application with minimal downtime to the application. What should you do?",
        "wg": [
          { "t": "最少停機時間", "en": "minimal downtime", "ps": "noun phrase" },
          { "t": "更新", "en": "update", "ps": "noun" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "使用 kubectl set image deployment/echo-deployment <new-image>",
        "en": "Use kubectl set image deployment/echo-deployment <new-image>",
        "wg": [
          { "t": "設定映像檔", "en": "set image", "ps": "command" }
        ]
      },
      {
        "t": "使用 Kubernetes 叢集後方執行個體群組的滾動更新功能",
        "en": "Use the rolling update functionality of the Instance Group behind the Kubernetes cluster",
        "wg": [
          { "t": "滾動更新", "en": "rolling update", "ps": "noun" }
        ]
      },
      {
        "t": "更新 deployment yaml 檔案... 使用 kubectl delete... 和 kubectl create...",
        "en": "Update the deployment yaml file with the new container image. Use kubectl delete deployment/echo-deployment and kubectl create -f <yaml-file>",
        "wg": [
          { "t": "刪除", "en": "delete", "ps": "command" }
        ]
      },
      {
        "t": "更新 service yaml 檔案... 使用 kubectl delete... 和 kubectl create...",
        "en": "Update the service yaml file which the new container image. Use kubectl delete service/echo-service and kubectl create -f <yaml-file>",
        "wg": []
      }
    ],
    "answer": "Option 1",
    "why": {
      "t": "`kubectl set image` 是觸發 Kubernetes Deployment 滾動更新 (Rolling Update) 的標準且宣告式方法。這會逐步替換 Pods，確保過程中服務不中斷。刪除並重新建立 Deployment (選項 3) 會導致停機。直接操作底層執行個體群組 (選項 2) 會破壞 GKE 的管理狀態，且不適用於更新容器映像檔。",
      "en": "`kubectl set image` is the standard, declarative way to trigger a rolling update for a Kubernetes Deployment. This progressively replaces Pods, ensuring zero downtime. Deleting and recreating the Deployment (Option 3) causes downtime. Manipulating the underlying Instance Group (Option 2) bypasses GKE management and is not the correct way to update application container images.",
      "wg": [
        { "t": "宣告式", "en": "declarative", "ps": "adjective" },
        { "t": "逐步替換", "en": "progressively replaces", "ps": "verb phrase" }
      ]
    }
  },
  {
    "no": "5",
    "level": "Hard",
    "keywords": "Compute Engine, Disaster Recovery, Load Balancing, Global, High Availability",
    "question": [
      {
        "t": "你有一個將在 Compute Engine 上運行的應用程式。",
        "en": "You have an application that will run on Compute Engine.",
        "wg": []
      },
      {
        "t": "你需要設計一個架構，考量到災難復原計畫，要求你的應用程式在區域性中斷的情況下故障轉移到另一個區域。你應該做什麼？",
        "en": "You need to design an architecture that takes into account a disaster recovery plan that requires your application to fail over to another region in case of a regional outage. What should you do?",
        "wg": [
          { "t": "災難復原計畫", "en": "disaster recovery plan", "ps": "noun" },
          { "t": "故障轉移", "en": "fail over", "ps": "verb" },
          { "t": "區域性中斷", "en": "regional outage", "ps": "noun" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "在同一個專案不同區域部署兩個 VM... 使用 HTTP 負載平衡服務...",
        "en": "Deploy the application on two Compute Engine instances in the same project but in a different region. Use the first instance to serve traffic, and use the HTTP load balancing service to fail over to the standby instance in case of a disaster.",
        "wg": []
      },
      {
        "t": "在 CE 執行個體部署應用程式... 故障轉移到地端 (on-premises)...",
        "en": "Deploy the application on a Compute Engine instance. Use the instance to serve traffic, and use the HTTP load balancing service to fail over to an instance on your premises in case of a disaster.",
        "wg": []
      },
      {
        "t": "在同一個專案不同區域部署兩個運算引擎執行個體群組 (MIGs)。使用第一個群組服務流量，並在災難時使用 HTTP 負載平衡服務故障轉移到備用群組。",
        "en": "Deploy the application on two Compute Engine instance groups, each in the same project but in a different region. Use the first instance group to serve traffic, and use the HTTP load balancing service to fail over to the standby instancegroup in case of a disaster.",
        "wg": [
          { "t": "執行個體群組", "en": "instance groups", "ps": "noun" },
          { "t": "備用", "en": "standby", "ps": "adjective" }
        ]
      },
      {
        "t": "在不同專案不同區域部署兩個運算引擎執行個體群組...",
        "en": "Deploy the application on two Compute Engine instance groups, each in separate project and a different region...",
        "wg": []
      }
    ],
    "answer": "Option 3",
    "why": {
      "t": "為了實現高可用性與區域級災難復原，最佳架構是使用全球 HTTP(S) 負載平衡器搭配多個區域的後端 (此處為託管執行個體群組 MIG)。負載平衡器會自動偵測健康狀況並將流量導向健康的區域。使用單一 VM (選項 1) 無法提供足夠的擴展性與可靠性；跨專案 (選項 4) 會增加管理複雜度但非必要。",
      "en": "To achieve high availability and regional disaster recovery, the best architecture is to use a Global HTTP(S) Load Balancer with backends (Managed Instance Groups) in multiple regions. The Load Balancer automatically detects health and routes traffic to healthy regions. Using single VMs (Option 1) lacks scalability and reliability; spanning separate projects (Option 4) adds unnecessary complexity.",
      "wg": [
        { "t": "全球", "en": "Global", "ps": "adjective" },
        { "t": "擴展性", "en": "scalability", "ps": "noun" }
      ]
    }
  },{
    "no": "6",
    "level": "Easy",
    "keywords": "App Engine, Cloud Datastore, Indexes, gcloud, Troubleshooting",
    "question": [
      {
        "t": "你發現你的 App Engine 應用程式出現錯誤，原因是缺少 Cloud Datastore 索引。",
        "en": "You have found an error in your App Engine application caused by missing Cloud Datastore indexes.",
        "wg": [
          { "t": "Cloud Datastore 索引", "en": "Cloud Datastore indexes", "ps": "noun" },
          { "t": "錯誤", "en": "error", "ps": "noun" }
        ]
      },
      {
        "t": "你已經建立了一個包含所需索引的 YAML 檔案，並想要將這些新索引部署到 Cloud Datastore。",
        "en": "You have created a YAML file with the required indexes and want to deploy these new indexes to Cloud Datastore.",
        "wg": [
          { "t": "YAML 檔案", "en": "YAML file", "ps": "noun" },
          { "t": "部署", "en": "deploy", "ps": "verb" }
        ]
      },
      {
        "t": "你應該做什麼？",
        "en": "What should you do?",
        "wg": []
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "將 gcloud datastore create-indexes 指向你的設定檔。",
        "en": "Point gcloud datastore create-indexes to your configuration file.",
        "wg": [
          { "t": "gcloud datastore create-indexes", "en": "gcloud datastore create-indexes", "ps": "command" },
          { "t": "設定檔", "en": "configuration file", "ps": "noun" }
        ]
      },
      {
        "t": "上傳設定檔到 App Engine 的預設 Cloud Storage 儲存桶，並讓 App Engine 偵測新索引。",
        "en": "Upload the configuration file the App Engine's default Cloud Storage bucket, and have App Engine detect the new indexes.",
        "wg": [
          { "t": "Cloud Storage 儲存桶", "en": "Cloud Storage bucket", "ps": "noun" }
        ]
      },
      {
        "t": "在 GCP Console 中，使用 Datastore Admin 刪除目前索引並上傳新設定檔。",
        "en": "In the GCP Console, use Datastore Admin to delete the current indexes and upload the new configuration file.",
        "wg": [
          { "t": "Datastore Admin", "en": "Datastore Admin", "ps": "tool" }
        ]
      },
      {
        "t": "建立一個 HTTP 請求到內建的 python 模組以傳送索引設定檔到你的應用程式。",
        "en": "Create an HTTP request to the built-in python module to send the index configuration file to your application.",
        "wg": []
      }
    ],
    "answer": "Option 1",
    "why": {
      "t": "當你需要更新 Datastore 索引而不重新部署整個應用程式時，正確的命令是 `gcloud datastore create-indexes index.yaml`。這是宣告式且標準的管理方式。選項 2、3 和 4 均非 GCP 支援的標準索引部署程序。",
      "en": "When you need to update Datastore indexes without redeploying the entire application, the correct command is `gcloud datastore create-indexes index.yaml`. This is the declarative and standard way to manage indexes. Options 2, 3, and 4 are not supported standard procedures for deploying indexes in GCP.",
      "wg": [
        { "t": "標準的", "en": "standard", "ps": "adjective" },
        { "t": "程序", "en": "procedures", "ps": "noun" }
      ]
    }
  },
  {
    "no": "7",
    "level": "Medium",
    "keywords": "Security, Secret Manager, Microservices, Credentials",
    "question": [
      {
        "t": "你正在設計一個包含 30 個微服務的大型分散式應用程式。",
        "en": "You are designing a large distributed application with 30 microservices.",
        "wg": [
          { "t": "分散式應用程式", "en": "distributed application", "ps": "noun" },
          { "t": "微服務", "en": "microservices", "ps": "noun" }
        ]
      },
      {
        "t": "你的每個分散式微服務都需要連接到資料庫後端。你想要安全地儲存憑證。",
        "en": "Each of your distributed microservices needs to connect to a database back-end. You want to store the credentials securely.",
        "wg": [
          { "t": "資料庫後端", "en": "database back-end", "ps": "noun" },
          { "t": "憑證", "en": "credentials", "ps": "noun" },
          { "t": "安全地", "en": "securely", "ps": "adverb" }
        ]
      },
      {
        "t": "你應該將憑證儲存在哪裡？",
        "en": "Where should you store the credentials?",
        "wg": []
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "在原始碼中。",
        "en": "In the source code.",
        "wg": [
          { "t": "原始碼", "en": "source code", "ps": "noun" }
        ]
      },
      {
        "t": "在環境變數中。",
        "en": "In an environment variable.",
        "wg": [
          { "t": "環境變數", "en": "environment variable", "ps": "noun" }
        ]
      },
      {
        "t": "在機密管理系統中。",
        "en": "In a secret management system.",
        "wg": [
          { "t": "機密管理系統", "en": "secret management system", "ps": "noun" }
        ]
      },
      {
        "t": "在透過 ACL 限制存取的設定檔中。",
        "en": "In a config file that has restricted access through ACLs.",
        "wg": [
          { "t": "設定檔", "en": "config file", "ps": "noun" },
          { "t": "存取控制清單", "en": "ACLs", "ps": "noun" }
        ]
      }
    ],
    "answer": "Option 3",
    "why": {
      "t": "儲存敏感憑證的最佳實踐是使用專用的機密管理解決方案 (如 Google Cloud Secret Manager)。這提供了加密、自動輪替、集中管理與稽核日誌。原始碼 (A) 極度不安全；環境變數 (B) 可能在偵錯或日誌中洩漏；設定檔 (D) 管理 ACL 容易出錯且缺乏機密輪替功能。",
      "en": "The best practice for storing sensitive credentials is to use a dedicated secret management solution (like Google Cloud Secret Manager). This offers encryption, automatic rotation, centralized management, and audit logging. Source code (A) is extremely insecure; environment variables (B) can leak in debug dumps or logs; config files (D) are prone to ACL mismanagement and lack secret rotation capabilities.",
      "wg": [
        { "t": "最佳實踐", "en": "best practice", "ps": "noun" },
        { "t": "洩漏", "en": "leak", "ps": "verb" },
        { "t": "輪替", "en": "rotation", "ps": "noun" }
      ]
    }
  },
  {
    "no": "8",
    "level": "Medium",
    "keywords": "BigQuery, Cloud Storage, Logging, Analytics, Disaster Recovery",
    "question": [
      {
        "t": "你的公司想要低風險地試用雲端。他們想要將大約 100 TB 的日誌資料封存到雲端，並測試那裡可用的分析功能，同時也保留該資料作為長期災難復原備份。",
        "en": "Your company wants to try out the cloud with low risk. They want to archive approximately 100 TB of their log data to the cloud and test the analytics features available to them there, while also retaining that data as a long-term disaster recovery backup.",
        "wg": [
          { "t": "封存", "en": "archive", "ps": "verb" },
          { "t": "分析功能", "en": "analytics features", "ps": "noun" },
          { "t": "災難復原備份", "en": "disaster recovery backup", "ps": "noun" }
        ]
      },
      {
        "t": "你應該採取哪兩個步驟？",
        "en": "Which two steps should you take?",
        "wg": []
      }
    ],
    "type": "多選題",
    "options": [
      {
        "t": "將日誌載入 Google BigQuery。",
        "en": "Load logs into Google BigQuery.",
        "wg": [
          { "t": "Google BigQuery", "en": "Google BigQuery", "ps": "product" }
        ]
      },
      {
        "t": "將日誌載入 Google Cloud SQL。",
        "en": "Load logs into Google Cloud SQL.",
        "wg": [
          { "t": "Google Cloud SQL", "en": "Google Cloud SQL", "ps": "product" }
        ]
      },
      {
        "t": "將日誌匯入 Google Cloud Monitoring。",
        "en": "Import logs into Google Cloud Monitoring.",
        "wg": [
          { "t": "Google Cloud Monitoring", "en": "Google Cloud Monitoring", "ps": "product" }
        ]
      },
      {
        "t": "將日誌插入 Google Cloud Bigtable。",
        "en": "Insert logs into Google Cloud Bigtable.",
        "wg": [
          { "t": "Google Cloud Bigtable", "en": "Google Cloud Bigtable", "ps": "product" }
        ]
      },
      {
        "t": "將日誌檔案上傳到 Google Cloud Storage。",
        "en": "Upload log files into Google Cloud Storage.",
        "wg": [
          { "t": "Google Cloud Storage", "en": "Google Cloud Storage", "ps": "product" }
        ]
      }
    ],
    "answer": "Option 1, Option 5",
    "why": {
      "t": "題目有兩個需求：1. 分析功能 -> BigQuery 是 GCP 上的無伺服器資料倉儲，專為大規模日誌分析設計。2. 長期災難復原備份 -> Cloud Storage (GCS) 是存放冷資料和備份檔案最具成本效益且持久的選擇。Cloud SQL 不適合 PB 級非結構化日誌；Bigtable 成本較高且操作複雜，主要用於高吞吐量寫入場景而非單純備份。",
      "en": "The requirement has two parts: 1. Analytics -> BigQuery is the serverless data warehouse on GCP designed for large-scale log analytics. 2. Long-term DR backup -> Cloud Storage (GCS) is the most cost-effective and durable option for storing cold data and backup files. Cloud SQL is not for PB-scale unstructured logs; Bigtable is more expensive and complex, used for high-throughput writes rather than simple backup.",
      "wg": [
        { "t": "成本效益", "en": "cost-effective", "ps": "adjective" },
        { "t": "高吞吐量", "en": "high-throughput", "ps": "adjective" }
      ]
    }
  },
  {
    "no": "9",
    "level": "Medium",
    "keywords": "VPC, Firewall Rules, Network Tags, N-tier Architecture",
    "question": [
      {
        "t": "你的組織在 Google Cloud Platform 上的同一個網路中部署了一個 3 層式網路應用程式。",
        "en": "Your organization has a 3-tier web application deployed in the same network on Google Cloud Platform.",
        "wg": [
          { "t": "3 層式", "en": "3-tier", "ps": "adjective" },
          { "t": "網路", "en": "network", "ps": "noun" }
        ]
      },
      {
        "t": "每一層 (網頁、API 和資料庫) 均獨立擴展。網路流量應從網頁層流向 API 層，再流向資料庫層。網頁層和資料庫層之間不應有流量流動。",
        "en": "Each tier (web, API, and database) scales independently of the others. Network traffic should flow through the web to the API tier and then on to the database tier. Traffic should not flow between the web and the database tier.",
        "wg": [
          { "t": "流量流動", "en": "traffic flow", "ps": "noun phrase" },
          { "t": "獨立擴展", "en": "scales independently", "ps": "verb phrase" }
        ]
      },
      {
        "t": "你應該如何設定網路？",
        "en": "How should you configure the network?",
        "wg": []
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "將每一層加入位於不同區域的個別子網路。",
        "en": "Add each tier to a different subnetwork in a separate regions.",
        "wg": [
          { "t": "子網路", "en": "subnetwork", "ps": "noun" },
          { "t": "區域", "en": "regions", "ps": "noun" }
        ]
      },
      {
        "t": "在個別 VM 上設定軟體式防火牆。",
        "en": "Set up software based firewalls on individual VMs.",
        "wg": [
          { "t": "軟體式防火牆", "en": "software based firewalls", "ps": "noun" }
        ]
      },
      {
        "t": "為每一層新增標記，並設定路由以允許所需的流量流動。",
        "en": "Add tags to each tier and set up routes to allow the desired traffic flow.",
        "wg": [
          { "t": "標記", "en": "tags", "ps": "noun" },
          { "t": "路由", "en": "routes", "ps": "noun" }
        ]
      },
      {
        "t": "為每一層新增標記，並設定防火牆規則以允許所需的流量流動。",
        "en": "Add tags to each tier and set up firewall rules to allow the desired traffic flow.",
        "wg": [
          { "t": "防火牆規則", "en": "firewall rules", "ps": "noun" }
        ]
      }
    ],
    "answer": "Option 4",
    "why": {
      "t": "在 GCP 中，控制 VM 之間流量存取權限的標準機制是「VPC 防火牆規則」。透過對不同層級的 VM (Web, API, DB) 加上「網路標記 (Tags)」，你可以撰寫特定的防火牆規則 (例如：允許 tag:web 到 tag:api，拒絕 tag:web 到 tag:db)。路由 (Routes) 是用來決定封包路徑，而非安全性存取控制。",
      "en": "In GCP, the standard mechanism to control access privileges between VMs is 'VPC Firewall Rules'. By applying 'Network Tags' to VMs in different tiers (Web, API, DB), you can write specific firewall rules (e.g., allow tag:web to tag:api, deny tag:web to tag:db). Routes determine the packet path, not security access control.",
      "wg": [
        { "t": "存取權限", "en": "access privileges", "ps": "noun" },
        { "t": "封包路徑", "en": "packet path", "ps": "noun" }
      ]
    }
  },
  {
    "no": "10",
    "level": "Medium",
    "keywords": "Cloud Trace, Microservices, Latency, Troubleshooting",
    "question": [
      {
        "t": "你的微服務架構應用程式有少量的 API 請求花費非常長的時間。",
        "en": "A small number of API requests to your microservices-based application take a very long time.",
        "wg": [
          { "t": "微服務架構", "en": "microservices-based", "ps": "adjective" },
          { "t": "API 請求", "en": "API requests", "ps": "noun" }
        ]
      },
      {
        "t": "你知道每個對 API 的請求都可能穿越許多服務。你想要知道在這些案例中哪個服務花費最長的時間。",
        "en": "You know that each request to the API can traverse many services. You want to know which service takes the longest in those cases.",
        "wg": [
          { "t": "穿越", "en": "traverse", "ps": "verb" },
          { "t": "服務", "en": "services", "ps": "noun" }
        ]
      },
      {
        "t": "你應該做什麼？",
        "en": "What should you do?",
        "wg": []
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "在你的應用程式設定逾時，以便更快讓請求失敗。",
        "en": "Set timeouts on your application so that you can fail requests faster.",
        "wg": [
          { "t": "逾時", "en": "timeouts", "ps": "noun" }
        ]
      },
      {
        "t": "為你的每個請求傳送自訂指標到 Cloud Monitoring。",
        "en": "Send custom metrics for each of your requests to Cloud Monitoring.",
        "wg": [
          { "t": "自訂指標", "en": "custom metrics", "ps": "noun" }
        ]
      },
      {
        "t": "使用 Cloud Monitoring 尋找顯示 API 延遲何時變高的洞察資訊。",
        "en": "Use Cloud Monitoring to look for insights that show when your API latencies are high.",
        "wg": [
          { "t": "洞察資訊", "en": "insights", "ps": "noun" },
          { "t": "延遲", "en": "latencies", "ps": "noun" }
        ]
      },
      {
        "t": "使用 Cloud Trace 檢測你的應用程式，以便分解每個微服務的請求延遲。",
        "en": "Instrument your application with Cloud Trace in order to break down the request latencies at each microservice.",
        "wg": [
          { "t": "Cloud Trace", "en": "Cloud Trace", "ps": "product" },
          { "t": "檢測", "en": "instrument", "ps": "verb" },
          { "t": "分解", "en": "break down", "ps": "verb" }
        ]
      }
    ],
    "answer": "Option 4",
    "why": {
      "t": "Cloud Trace 是 GCP 的分散式追蹤系統，專門設計用來收集跨微服務的延遲資料。它能產生瀑布圖，精確顯示請求在每個服務停留了多久 (span)。Cloud Monitoring (選項 2 & 3) 主要關注整體健康與聚合指標，較難追蹤單一請求在多個服務間的詳細路徑與個別耗時。",
      "en": "Cloud Trace is GCP's distributed tracing system, specifically designed to collect latency data across microservices. It generates waterfall charts that pinpoint exactly how long a request spent in each service (span). Cloud Monitoring (Options 2 & 3) focuses on overall health and aggregated metrics, making it difficult to trace the detailed path and individual duration of a single request across multiple services.",
      "wg": [
        { "t": "分散式追蹤系統", "en": "distributed tracing system", "ps": "noun" },
        { "t": "瀑布圖", "en": "waterfall charts", "ps": "noun" }
      ]
    }
  },{
    "no": "11",
    "level": "Hard",
    "keywords": "Docker, GKE, Optimization, Build Cache, Base Image",
    "question": [
      {
        "t": "你團隊中的一位開發人員在 Google Container Engine 中部署了應用程式，使用了下方的 Dockerfile。",
        "en": "One of the developers on your team deployed their application in Google Container Engine with the Dockerfile below.",
        "wg": [
          { "t": "開發人員", "en": "developers", "ps": "noun" },
          { "t": "Dockerfile", "en": "Dockerfile", "ps": "noun" }
        ]
      },
      {
        "t": "他們回報應用程式部署花費太長時間。你想要優化此 Dockerfile 以獲得更快的部署時間，且不影響應用程式的功能。",
        "en": "They report that their application deployments are taking too long. You want to optimize this Dockerfile for faster deployment times without adversely affecting the app's functionality.",
        "wg": [
          { "t": "優化", "en": "optimize", "ps": "verb" },
          { "t": "部署時間", "en": "deployment times", "ps": "noun" }
        ]
      },
      {
        "t": "你應該採取哪兩個動作？",
        "en": "Which two actions should you take?",
        "wg": []
      }
    ],
    "type": "多選題",
    "options": [
      {
        "t": "在執行 pip 後移除 Python。",
        "en": "Remove Python after running pip",
        "wg": []
      },
      {
        "t": "從 requirements.txt 移除相依套件。",
        "en": "Remove dependencies from requirements.txt",
        "wg": []
      },
      {
        "t": "使用像 Alpine Linux 這樣的精簡基礎映像檔。",
        "en": "Use a slimmed-down base image like Alpine Linux",
        "wg": [
          { "t": "精簡基礎映像檔", "en": "slimmed-down base image", "ps": "noun" }
        ]
      },
      {
        "t": "為你的 Google Container Engine 節點池使用較大的機器類型。",
        "en": "Use larger machine types for your Google Container Engine node pools",
        "wg": [
          { "t": "節點池", "en": "node pools", "ps": "noun" }
        ]
      },
      {
        "t": "在安裝套件相依性 (Python 和 pip) 之後才複製原始碼。",
        "en": "Copy the source after the package dependencies (Python and pip) are installed",
        "wg": [
          { "t": "複製原始碼", "en": "Copy the source", "ps": "verb phrase" },
          { "t": "套件相依性", "en": "package dependencies", "ps": "noun" }
        ]
      }
    ],
    "answer": "Option 3, Option 5",
    "why": {
      "t": "優化 Docker 建置速度與映像檔大小的兩個關鍵策略：1. 使用較小的基礎映像檔 (如 Alpine) 可以顯著減少網路傳輸時間與儲存空間。2. 優化層快取 (Layer Caching)，將變動頻率低的指令 (如安裝相依性) 放在變動頻率高的指令 (如 COPY 原始碼) 之前。這樣當原始碼改變時，Docker 可以重用之前安裝好套件的快取層，而不用每次都重新安裝。",
      "en": "Two key strategies for optimizing Docker build speed and image size: 1. Using a smaller base image (like Alpine) significantly reduces network transfer time and storage. 2. Optimizing Layer Caching by placing less frequently changed instructions (installing dependencies) before frequently changed ones (COPY source). This allows Docker to reuse the cached layer containing installed packages when source code changes, avoiding re-installation every time.",
      "wg": [
        { "t": "層快取", "en": "Layer Caching", "ps": "noun" },
        { "t": "重用", "en": "reuse", "ps": "verb" }
      ]
    }
  },
  {
    "no": "12",
    "level": "Hard",
    "keywords": "Cloud Bigtable, IoT, Time-series, High Throughput",
    "question": [
      {
        "t": "你想要優化一個精確、即時的天氣圖表應用程式的效能。",
        "en": "You want to optimize the performance of an accurate, real-time, weather-charting application.",
        "wg": [
          { "t": "即時", "en": "real-time", "ps": "adjective" },
          { "t": "天氣圖表", "en": "weather-charting", "ps": "adjective" }
        ]
      },
      {
        "t": "資料來自 50,000 個感測器，每秒傳送 10 筆讀數，格式為時間戳記和感測器讀數。",
        "en": "The data comes from 50,000 sensors sending 10 readings a second, in the format of a timestamp and sensor reading.",
        "wg": [
          { "t": "感測器", "en": "sensors", "ps": "noun" },
          { "t": "讀數", "en": "readings", "ps": "noun" },
          { "t": "時間戳記", "en": "timestamp", "ps": "noun" }
        ]
      },
      {
        "t": "你應該將資料儲存在哪裡？",
        "en": "Where should you store the data?",
        "wg": []
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "Google Cloud Bigtable",
        "en": "Google Cloud Bigtable",
        "wg": [
          { "t": "Bigtable", "en": "Bigtable", "ps": "product" }
        ]
      },
      {
        "t": "Google BigQuery",
        "en": "Google BigQuery",
        "wg": [
          { "t": "BigQuery", "en": "BigQuery", "ps": "product" }
        ]
      },
      {
        "t": "Google Cloud SQL",
        "en": "Google Cloud SQL",
        "wg": [
          { "t": "Cloud SQL", "en": "Cloud SQL", "ps": "product" }
        ]
      },
      {
        "t": "Google Cloud Storage",
        "en": "Google Cloud Storage",
        "wg": [
          { "t": "Cloud Storage", "en": "Cloud Storage", "ps": "product" }
        ]
      }
    ],
    "answer": "Option 1",
    "why": {
      "t": "計算寫入吞吐量：50,000 感測器 * 10 讀數/秒 = 500,000 寫入/秒 (IOPS)。Cloud Bigtable 是 GCP 上專為處理高吞吐量寫入、低延遲時間序列 (Time-series) 資料與 IoT 資料所設計的 NoSQL 資料庫。Cloud SQL 無法支撐此量級的寫入；BigQuery 適合分析而非即時高頻寫入；Cloud Storage 是物件儲存，不適合儲存個別的感測器讀數紀錄。",
      "en": "Calculating write throughput: 50,000 sensors * 10 readings/sec = 500,000 writes/sec (IOPS). Cloud Bigtable is the NoSQL database on GCP designed specifically for high-throughput writes, low-latency time-series data, and IoT data. Cloud SQL cannot support this scale of writes; BigQuery is for analytics, not real-time high-frequency ingestion; Cloud Storage is object storage, not suitable for storing individual sensor reading records.",
      "wg": [
        { "t": "吞吐量", "en": "throughput", "ps": "noun" },
        { "t": "時間序列", "en": "time-series", "ps": "noun" }
      ]
    }
  },
  {
    "no": "13",
    "level": "Medium",
    "keywords": "Cloud Storage, Lifecycle Management, Cost Optimization, gsutil",
    "question": [
      {
        "t": "你正在建立一個解決方案，以從你的 Cloud Storage 備份儲存桶中移除超過 90 天的備份檔案。",
        "en": "You are creating a solution to remove backup files older than 90 days from your backup Cloud Storage bucket.",
        "wg": [
          { "t": "備份檔案", "en": "backup files", "ps": "noun" },
          { "t": "Cloud Storage 儲存桶", "en": "Cloud Storage bucket", "ps": "noun" }
        ]
      },
      {
        "t": "你想要優化持續的 Cloud Storage 花費。你應該做什麼？",
        "en": "You want to optimize ongoing Cloud Storage spend. What should you do?",
        "wg": [
          { "t": "優化花費", "en": "optimize spend", "ps": "verb phrase" }
        ]
      },
      {
        "t": "你應該做什麼？",
        "en": "What should you do?",
        "wg": []
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "撰寫 XML 格式的生命週期管理規則，並使用 gsutil 推送到儲存桶。",
        "en": "Write a lifecycle management rule in XML and push it to the bucket with gsutil",
        "wg": [
          { "t": "生命週期管理規則", "en": "lifecycle management rule", "ps": "noun" }
        ]
      },
      {
        "t": "排程一個 cron 腳本，使用 gsutil ls -lr ... 尋找並移除超過 90 天的項目。",
        "en": "Schedule a cron script using gsutil ls -lr gs://backups/** to find and remove items older than 90 days",
        "wg": [
          { "t": "cron 腳本", "en": "cron script", "ps": "noun" }
        ]
      },
      {
        "t": "撰寫 JSON 格式的生命週期管理規則，並使用 gsutil 推送到儲存桶。",
        "en": "Write a lifecycle management rule in JSON and push it to the bucket with gsutil",
        "wg": [
          { "t": "JSON", "en": "JSON", "ps": "format" }
        ]
      },
      {
        "t": "排程一個 cron 腳本... (與選項 2 類似)",
        "en": "Schedule a cron script using gsutil ls -l gs://backups/** to find and remove items older than 90 days and schedule it with cron",
        "wg": []
      }
    ],
    "answer": "Option 3",
    "why": {
      "t": "Cloud Storage Object Lifecycle Management (物件生命週期管理) 是處理過期檔案刪除或轉移層級的內建功能，既免費又無需維護伺服器。Google 推薦使用 JSON 格式來定義規則，並透過 `gsutil lifecycle set config.json gs://bucket` 套用。使用 cron 腳本 (選項 2 & 4) 需要運算資源 (增加成本) 且要列舉所有檔案 (產生 Class A/B 操作費用)，效率低落。",
      "en": "Cloud Storage Object Lifecycle Management is a built-in feature for deleting or transitioning expired files, which is free and serverless. Google recommends using JSON format to define rules and applying them via `gsutil lifecycle set config.json gs://bucket`. Using cron scripts (Options 2 & 4) requires compute resources (adding cost) and enumerating all files (incurring Class A/B operation costs), making it inefficient.",
      "wg": [
        { "t": "內建功能", "en": "built-in feature", "ps": "noun" },
        { "t": "無需維護伺服器", "en": "serverless", "ps": "adjective" }
      ]
    }
  },
  {
    "no": "14",
    "level": "Medium",
    "keywords": "App Engine, Incident Response, Latency, Troubleshooting, Rollback",
    "question": [
      {
        "t": "你的客戶收到報告，指出他們最近更新的 Google App Engine 應用程式對某些使用者來說載入時間約為 30 秒。",
        "en": "Your customer is receiving reports that their recently updated Google App Engine application is taking approximately 30 seconds to load for some of their users.",
        "wg": [
          { "t": "載入時間", "en": "load time", "ps": "noun" }
        ]
      },
      {
        "t": "這個行為在更新前並未被回報。你應該採取什麼策略？",
        "en": "This behavior was not reported before the update. What strategy should you take?",
        "wg": [
          { "t": "策略", "en": "strategy", "ps": "noun" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "開啟支援票證要求網路封包擷取...",
        "en": "Open a support ticket to ask for network capture and flow data to diagnose the problem, then roll back your application",
        "wg": [
          { "t": "支援票證", "en": "support ticket", "ps": "noun" }
        ]
      },
      {
        "t": "先復原到早期已知良好的版本，然後在開發/測試/預備環境中使用 Cloud Trace 和 Logging 來診斷問題。",
        "en": "Roll back to an earlier known good release initially, then use Cloud Trace and Logging to diagnose the problem in a development/test/staging environment",
        "wg": [
          { "t": "復原", "en": "roll back", "ps": "verb" },
          { "t": "診斷", "en": "diagnose", "ps": "verb" }
        ]
      },
      {
        "t": "先復原到早期已知良好的版本，然後在較安靜的時段再次推送該版本以進行調查...",
        "en": "Roll back to an earlier known good release, then push the release again at a quieter period to investigate. Then use Cloud Trace and Logging to diagnose the problem",
        "wg": []
      },
      {
        "t": "與你的 ISP 合作診斷問題",
        "en": "Work with your ISP to diagnose the problem",
        "wg": [
          { "t": "ISP", "en": "ISP", "ps": "noun" }
        ]
      }
    ],
    "answer": "Option 2",
    "why": {
      "t": "在生產環境發生效能回歸 (Performance Regression) 時，首要任務是「恢復服務」，即立即復原 (Rollback) 到正常版本。接著，應在非生產環境中重現並診斷問題，使用 Cloud Trace (分析延遲) 和 Logging (檢查錯誤)。在生產環境重新部署有問題的版本 (選項 3) 會再次影響使用者，不是良好的營運策略。",
      "en": "When a performance regression occurs in production, the priority is 'Service Restoration', which means immediately rolling back to a known good version. Next, reproduce and diagnose the issue in a non-production environment using Cloud Trace (for latency) and Logging (for errors). Redeploying the buggy version to production (Option 3) would impact users again and is not a sound operational strategy.",
      "wg": [
        { "t": "效能回歸", "en": "performance regression", "ps": "noun" },
        { "t": "營運策略", "en": "operational strategy", "ps": "noun" }
      ]
    }
  },
  {
    "no": "15",
    "level": "Medium",
    "keywords": "MongoDB, Managed Service, Database, High Availability",
    "question": [
      {
        "t": "你的客戶需要一個包含 MongoDB 和 2 個複本的系統，並希望獲得最大的可用性和故障保護。",
        "en": "Your customer needs a system with a MongoDB and 2 replicas and wants maximum availability and protection against failures.",
        "wg": [
          { "t": "最大可用性", "en": "maximum availability", "ps": "noun phrase" },
          { "t": "故障保護", "en": "protection against failures", "ps": "noun phrase" }
        ]
      },
      {
        "t": "資料庫在美國運作且全天候 (24/7) 頻繁使用，因此無法選擇舒適的維護視窗。",
        "en": "The database operates in US and is actively used 24/7, hence you cannot select a comfortable maintenance window.",
        "wg": [
          { "t": "維護視窗", "en": "maintenance window", "ps": "noun" }
        ]
      },
      {
        "t": "你的建議會是什麼？",
        "en": "What would be your advice?",
        "wg": []
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "使用內部負載平衡服務搭配託管執行個體群組和區域永久磁碟。",
        "en": "use internal load balancing service with a managed instance group and regional Persistent Disks.",
        "wg": [
          { "t": "內部負載平衡服務", "en": "internal load balancing service", "ps": "product" }
        ]
      },
      {
        "t": "使用第三方 MongoDB 託管服務，例如 MongoDB Atlas。",
        "en": "use a 3rd party MongoDB Managed Service such as MongoDB Atlas",
        "wg": [
          { "t": "託管服務", "en": "Managed Service", "ps": "noun" }
        ]
      },
      {
        "t": "實作即時遷移並使用區域永久磁碟。",
        "en": "Implement Live Migration and use regional PDs",
        "wg": [
          { "t": "即時遷移", "en": "Live Migration", "ps": "feature" }
        ]
      },
      {
        "t": "使用內部 TCP/UDP 負載平衡搭配本機 SSD。",
        "en": "use internal TCP/UDP Load Balancing with local SSDs.",
        "wg": [
          { "t": "本機 SSD", "en": "local SSDs", "ps": "noun" }
        ]
      }
    ],
    "answer": "Option 2",
    "why": {
      "t": "在 Google Cloud 上，若要運作像 MongoDB 這類複雜的有狀態資料庫，並要求「最大可用性」與「無停機維護」，使用完全託管的服務 (如 MongoDB Atlas) 是最佳選擇。它能自動處理備份、修補、縮放與故障轉移，減少維運負擔。自行在 VM 上架設 (其他選項) 需要大量的手動維運工作來管理複本集與高可用性。",
      "en": "On Google Cloud, to run a complex stateful database like MongoDB with requirements for 'maximum availability' and 'no downtime maintenance', using a fully managed service (like MongoDB Atlas) is the best choice. It automates backups, patching, scaling, and failover, reducing operational toil. Self-hosting on VMs (other options) requires significant manual effort to manage replica sets and high availability.",
      "wg": [
        { "t": "完全託管的服務", "en": "fully managed service", "ps": "noun" },
        { "t": "維運負擔", "en": "operational toil", "ps": "noun" }
      ]
    }
  },{
    "no": "16",
    "level": "Medium",
    "keywords": "Compute Engine, BigQuery, Service Account, IAM, Troubleshooting",
    "question": [
      {
        "t": "你撰寫了一個 Python 腳本，從 Google Compute Engine 虛擬機器連接到 Google BigQuery。",
        "en": "You write a Python script to connect to Google BigQuery from a Google Compute Engine virtual machine.",
        "wg": [
          { "t": "虛擬機器", "en": "virtual machine", "ps": "noun" },
          { "t": "Python 腳本", "en": "Python script", "ps": "noun" }
        ]
      },
      {
        "t": "該腳本列印出無法連接到 BigQuery 的錯誤。你應該做什麼來修正腳本？",
        "en": "The script is printing errors that it cannot connect to BigQuery. What should you do to fix the script?",
        "wg": [
          { "t": "錯誤", "en": "errors", "ps": "noun" },
          { "t": "修正", "en": "fix", "ps": "verb" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "更新用於執行 VM 的服務帳戶權限。",
        "en": "Update privileges of the service account that is used to run the VM",
        "wg": [
          { "t": "服務帳戶", "en": "service account", "ps": "noun" },
          { "t": "權限", "en": "privileges", "ps": "noun" }
        ]
      },
      {
        "t": "安裝適用於 Python 的最新 BigQuery API 用戶端程式庫。",
        "en": "Install the latest BigQuery API client library for Python",
        "wg": [
          { "t": "用戶端程式庫", "en": "client library", "ps": "noun" }
        ]
      },
      {
        "t": "使用 gcloud components install bq 命令安裝 gcloud 的 bq 元件。",
        "en": "Install the bq component for gcloud with the command gcloud components install bq.",
        "wg": [
          { "t": "元件", "en": "component", "ps": "noun" }
        ]
      },
      {
        "t": "建立一個具有 BigQuery 存取權的新服務帳戶，並使用該使用者執行你的腳本。",
        "en": "Create a new service account with BigQuery access and execute your script with that user",
        "wg": []
      }
    ],
    "answer": "Option 1",
    "why": {
      "t": "在 GCP 中，在 Compute Engine 上執行的應用程式應使用該 VM 附加的「服務帳戶」來取得憑證。如果連線失敗，最常見的原因是預設服務帳戶缺少存取 BigQuery API 的 IAM 權限 (Scope 或 Role)。修正方式是更新該服務帳戶的權限，無需建立新帳戶或手動管理金鑰。",
      "en": "In GCP, applications running on Compute Engine should use the 'Service Account' attached to the VM to obtain credentials. If the connection fails, the most common reason is that the default service account lacks the IAM permissions (Scope or Role) to access the BigQuery API. The fix is to update the privileges of that service account, rather than creating a new one or manually managing keys.",
      "wg": [
        { "t": "預設", "en": "default", "ps": "adjective" },
        { "t": "憑證", "en": "credentials", "ps": "noun" }
      ]
    }
  },
  {
    "no": "17",
    "level": "Medium",
    "keywords": "GKE, Autoscaling, Cluster Autoscaler, gcloud",
    "question": [
      {
        "t": "你想要啟用執行中的 Google Kubernetes Engine 叢集，以隨著應用程式需求的變更而進行縮放。",
        "en": "You want to enable your running Google Kubernetes Engine cluster to scale as demand for your application changes.",
        "wg": [
          { "t": "啟用", "en": "enable", "ps": "verb" },
          { "t": "縮放", "en": "scale", "ps": "verb" }
        ]
      },
      {
        "t": "你應該做什麼？",
        "en": "What should you do?",
        "wg": []
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "使用 gcloud container clusters resize ... 加入額外節點",
        "en": "Add additional nodes to your Kubernetes Engine cluster using the following command: gcloud container clusters resize CLUSTER_Name --size 10",
        "wg": [
          { "t": "調整大小", "en": "resize", "ps": "command" }
        ]
      },
      {
        "t": "使用 gcloud compute instances add-tags ... 為執行個體加入標記",
        "en": "Add a tag to the instances in the cluster with the following command: gcloud compute instances add-tags INSTANCE --tags enableautoscaling max-nodes-10",
        "wg": [
          { "t": "加入標記", "en": "add-tags", "ps": "command" }
        ]
      },
      {
        "t": "使用 gcloud container clusters update ... --enable-autoscaling ... 更新現有的 Kubernetes Engine 叢集",
        "en": "Update the existing Kubernetes Engine cluster with the following command: gcloud container clusters update mycluster --enable-autoscaling --min-nodes=1 --max-nodes=10",
        "wg": [
          { "t": "叢集自動擴充", "en": "Cluster Autoscaler", "ps": "feature" }
        ]
      },
      {
        "t": "使用 gcloud alpha container clusters create ... 建立一個新的 Kubernetes Engine 叢集並重新部署",
        "en": "Create a new Kubernetes Engine cluster with the following command: gcloud alpha container clusters create mycluster --enable-autoscaling --min-nodes=1 --max-nodes=10 and redeploy your application",
        "wg": []
      }
    ],
    "answer": "Option 3",
    "why": {
      "t": "要在現有的 GKE 叢集啟用「叢集自動擴充器 (Cluster Autoscaler)」，正確的指令是 `gcloud container clusters update` 並加上 `--enable-autoscaling` 旗標以及設定最小與最大節點數。`resize` (選項 1) 是手動調整大小；不需要重新建立叢集 (選項 4)；直接對 VM 打標記 (選項 2) 無法啟用 GKE 的自動擴充邏輯。",
      "en": "To enable the 'Cluster Autoscaler' on an existing GKE cluster, the correct command is `gcloud container clusters update` with the `--enable-autoscaling` flag and specifying min and max nodes. `resize` (Option 1) is for manual scaling; recreating the cluster (Option 4) is unnecessary; adding tags directly to VMs (Option 2) does not trigger GKE's autoscaling logic.",
      "wg": [
        { "t": "旗標", "en": "flag", "ps": "noun" },
        { "t": "邏輯", "en": "logic", "ps": "noun" }
      ]
    }
  },
  {
    "no": "18",
    "level": "Hard",
    "keywords": "CI/CD, GKE, Global Load Balancing, Artifact Registry, Immutable",
    "question": [
      {
        "t": "Mountkirk Games 想要建立持續交付管道。他們的架構包含許多小型服務，希望能夠快速更新和復原。",
        "en": "Mountkirk Games wants to set up a continuous delivery pipeline. Their architecture includes many small services that they want to be able to update and roll back quickly.",
        "wg": [
          { "t": "持續交付管道", "en": "continuous delivery pipeline", "ps": "noun" },
          { "t": "復原", "en": "roll back", "ps": "verb" }
        ]
      },
      {
        "t": "Mountkirk Games 有以下需求：服務冗餘地部署在美國和歐洲的多個區域、只有前端服務公開在網際網路、為其服務機隊提供單一前端 IP、部署製品是不可變的。",
        "en": "Mountkirk Games has the following requirements: Services are deployed redundantly across multiple regions in the US and Europe; Only frontend services are exposed on the public internet; They can provide a single frontend IP for their fleet of services; Deployment artifacts are immutable.",
        "wg": [
          { "t": "冗餘地", "en": "redundantly", "ps": "adverb" },
          { "t": "不可變的", "en": "immutable", "ps": "adjective" }
        ]
      },
      {
        "t": "他們應該使用哪一組產品？",
        "en": "Which set of products should they use?",
        "wg": []
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "Google Cloud Storage, Google Cloud Dataflow, Google Compute Engine",
        "en": "Google Cloud Storage, Google Cloud Dataflow, Google Compute Engine",
        "wg": []
      },
      {
        "t": "Google Cloud Storage, Google App Engine, Google Network Load Balancer",
        "en": "Google Cloud Storage, Google App Engine, Google Network Load Balancer",
        "wg": []
      },
      {
        "t": "Artifact Registry, Google Kubernetes Engine, Google HTTP(S) Load Balancer",
        "en": "Artifact Registry, Google Kubernetes Engine, Google HTTP(S) Load Balancer",
        "wg": [
          { "t": "Artifact Registry", "en": "Artifact Registry", "ps": "product" },
          { "t": "HTTP(S) 負載平衡器", "en": "HTTP(S) Load Balancer", "ps": "product" }
        ]
      },
      {
        "t": "Google Cloud Functions, Google Cloud Pub/Sub, Google Cloud Deployment Manager",
        "en": "Google Cloud Functions, Google Cloud Pub/Sub, Google Cloud Deployment Manager",
        "wg": []
      }
    ],
    "answer": "Option 3",
    "why": {
      "t": "針對「許多小型服務」、「不可變製品 (Immutable Artifacts)」、「快速更新與復原」的需求，容器化方案 (GKE) 搭配 Artifact Registry (儲存 Docker 映像檔) 是最佳解。為了滿足「單一前端 IP」同時跨「多個區域 (美國/歐洲)」，必須使用 Anycast IP 的全球 HTTP(S) 負載平衡器。App Engine (選項 2) 雖然不錯，但 Network Load Balancer 是區域性的，不符單一全球 IP 需求。",
      "en": "For requirements like 'many small services', 'immutable artifacts', and 'quick updates/rollbacks', a containerized solution (GKE) with Artifact Registry (for Docker images) is the best fit. To meet the 'single frontend IP' across 'multiple regions (US/Europe)', a Global HTTP(S) Load Balancer with Anycast IP is required. App Engine (Option 2) is good, but Network Load Balancer is regional, failing the single global IP requirement.",
      "wg": [
        { "t": "容器化", "en": "containerized", "ps": "adjective" },
        { "t": "單一前端 IP", "en": "single frontend IP", "ps": "noun phrase" }
      ]
    }
  },
  {
    "no": "19",
    "level": "Hard",
    "keywords": "Security, IoT, Zero Trust, TPM",
    "question": [
      {
        "t": "你的農業部門正在實驗全自動駕駛車輛。",
        "en": "Your agricultural division is experimenting with fully autonomous vehicles.",
        "wg": [
          { "t": "全自動駕駛車輛", "en": "fully autonomous vehicles", "ps": "noun" }
        ]
      },
      {
        "t": "你希望你的架構在車輛運作期間促進強大的安全性。",
        "en": "You want your architecture to promote strong security during vehicle operation.",
        "wg": [
          { "t": "強大的安全性", "en": "strong security", "ps": "noun phrase" }
        ]
      },
      {
        "t": "你應該考慮哪兩個架構？(選擇兩項)",
        "en": "Which two architectures should you consider? (Choose two.)",
        "wg": []
      }
    ],
    "type": "多選題",
    "options": [
      {
        "t": "將車輛上模組之間的每個微服務呼叫視為不受信任。",
        "en": "Treat every micro service call between modules on the vehicle as untrusted.",
        "wg": [
          { "t": "不受信任", "en": "untrusted", "ps": "adjective" }
        ]
      },
      {
        "t": "要求 IPv6 連線以確保安全的位址空間。",
        "en": "Require IPv6 for connectivity to ensure a secure address space.",
        "wg": [
          { "t": "IPv6", "en": "IPv6", "ps": "protocol" }
        ]
      },
      {
        "t": "使用信賴平台模組 (TPM) 並在開機時驗證韌體和二進位檔。",
        "en": "Use a trusted platform module (TPM) and verify firmware and binaries on boot.",
        "wg": [
          { "t": "信賴平台模組", "en": "trusted platform module", "ps": "noun" },
          { "t": "驗證", "en": "verify", "ps": "verb" }
        ]
      },
      {
        "t": "使用函數式程式語言來隔離程式碼執行週期。",
        "en": "Use a functional programming language to isolate code execution cycles.",
        "wg": []
      },
      {
        "t": "使用多個連線子系統進行冗餘。",
        "en": "Use multiple connectivity subsystems for redundancy.",
        "wg": []
      },
      {
        "t": "將車輛的驅動電子設備封裝在法拉第籠中以隔離晶片。",
        "en": "Enclose the vehicle's drive electronics in a Faraday cage to isolate chips.",
        "wg": []
      }
    ],
    "answer": "Option 1, Option 3",
    "why": {
      "t": "為了確保邊緣裝置 (如自駕車) 的高安全性，應採用深度防禦 (Defense in Depth) 與零信任 (Zero Trust) 原則。選項 3 (TPM) 確保硬體與開機程序的完整性，防止被篡改。選項 1 (視內部呼叫為不信任) 實踐零信任模型，即使單一模組被攻破，攻擊者也無法輕易橫向移動。IPv6 (選項 2) 解決位址枯竭問題，本身不保證安全性。",
      "en": "To ensure high security for edge devices (like autonomous vehicles), Defense in Depth and Zero Trust principles should be applied. Option 3 (TPM) ensures hardware and boot integrity, preventing tampering. Option 1 (treating calls as untrusted) implements a Zero Trust model, so even if one module is compromised, the attacker cannot easily move laterally. IPv6 (Option 2) solves address exhaustion, not security inherently.",
      "wg": [
        { "t": "深度防禦", "en": "Defense in Depth", "ps": "concept" },
        { "t": "零信任", "en": "Zero Trust", "ps": "concept" }
      ]
    }
  },
  {
    "no": "20",
    "level": "Medium",
    "keywords": "Cloud Storage, BigQuery, Cost Optimization, Data Lifecycle, IoT",
    "question": [
      {
        "t": "一家自動駕駛公司在所有汽車上安裝了伺服器和感測器來收集遙測資料。",
        "en": "An AutonomousDrive company has equipped all cars with servers and sensors to collect telemetry data.",
        "wg": [
          { "t": "遙測資料", "en": "telemetry data", "ps": "noun" }
        ]
      },
      {
        "t": "明年他們想要使用這些資料來訓練機器學習模型。他們希望在降低成本的同時將資料儲存在雲端。",
        "en": "Next year they want to use the data to train machine learning models. They want to store this data in the cloud while reducing costs.",
        "wg": [
          { "t": "訓練", "en": "train", "ps": "verb" },
          { "t": "機器學習模型", "en": "machine learning models", "ps": "noun" },
          { "t": "降低成本", "en": "reducing costs", "ps": "verb phrase" }
        ]
      },
      {
        "t": "他們應該做什麼？",
        "en": "What should you do?",
        "wg": []
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "讓車輛的電腦以每小時快照的方式壓縮資料，並儲存在 Google Cloud Storage (GCS) Nearline 儲存桶中。",
        "en": "Have the vehicle's computer compress the data in hourly snapshots, and store it in a Google Cloud Storage (GCS) Nearline bucket",
        "wg": [
          { "t": "壓縮", "en": "compress", "ps": "verb" },
          { "t": "快照", "en": "snapshots", "ps": "noun" }
        ]
      },
      {
        "t": "將遙測資料即時推送至串流 dataflow 作業，壓縮資料並儲存在 Google BigQuery 中。",
        "en": "Push the telemetry data in real-time to a streaming dataflow job that compresses the data, and store it in Google BigQuery",
        "wg": [
          { "t": "即時", "en": "real-time", "ps": "adjective" }
        ]
      },
      {
        "t": "將遙測資料即時推送至串流 dataflow 作業，壓縮資料並儲存在 Cloud Bigtable 中。",
        "en": "Push the telemetry data in real-time to a streaming dataflow job that compresses the data, and store it in Cloud Bigtable",
        "wg": []
      },
      {
        "t": "讓車輛的電腦以每小時快照的方式壓縮資料，並儲存在 GCS Coldline 儲存桶中。",
        "en": "Have the vehicle's computer compress the data in hourly snapshots, and store it in a GCS Coldline bucket",
        "wg": [
          { "t": "Coldline", "en": "Coldline", "ps": "class" }
        ]
      }
    ],
    "answer": "Option 4",
    "why": {
      "t": "關鍵字是「明年使用」和「降低成本」。這表示資料在寫入後會長時間存放且極少存取 (冷資料)。GCS Coldline 的儲存成本比 Nearline 低，適合存取頻率低於每季一次的資料。BigQuery 和 Bigtable (選項 2 & 3) 的儲存成本都遠高於 GCS，且即時串流 (Streaming) 成本也較批次上傳高。",
      "en": "Key keywords are 'use next year' and 'reducing costs'. This implies the data will be stored for a long time with very low access frequency (cold data). GCS Coldline has lower storage costs than Nearline and is suitable for data accessed less than once a quarter. BigQuery and Bigtable (Options 2 & 3) have much higher storage costs than GCS, and real-time streaming is more expensive than batch uploads.",
      "wg": [
        { "t": "冷資料", "en": "cold data", "ps": "noun" },
        { "t": "儲存成本", "en": "storage costs", "ps": "noun" }
      ]
    }
  },
  {
    "no": "21",
    "level": "Medium",
    "keywords": "Storage Transfer Service, AWS S3, Migration, Sync",
    "question": [
      {
        "t": "你的公司目前託管一個 AWS S3 儲存桶。你需要將此儲存桶的內容與一個新的 GCS 儲存桶保持同步。",
        "en": "Your company currently hosts an AWS S3 bucket. You need to keep the contents of this bucket in sync with a new GCS bucket.",
        "wg": [
          { "t": "保持同步", "en": "keep ... in sync", "ps": "verb phrase" },
          { "t": "AWS S3 儲存桶", "en": "AWS S3 bucket", "ps": "noun" }
        ]
      },
      {
        "t": "達成此目標的建議方法是什麼？",
        "en": "What is the recommended method to achieve it?",
        "wg": []
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "每天使用 gsutil cp 一次，從 AWS 複製新檔案到 GCP。",
        "en": "Use 'gsutil cp' once per day to copy new files from AWS to GCP",
        "wg": [
          { "t": "gsutil cp", "en": "gsutil cp", "ps": "command" }
        ]
      },
      {
        "t": "使用 gsutil rsync 命令保持兩地同步。",
        "en": "Use 'gsutil rsync' command to keep both locations in sync",
        "wg": [
          { "t": "gsutil rsync", "en": "gsutil rsync", "ps": "command" }
        ]
      },
      {
        "t": "使用 Storage Transfer Service 同步資料。",
        "en": "Use Storage Transfer Service to sync the data.",
        "wg": [
          { "t": "Storage Transfer Service", "en": "Storage Transfer Service", "ps": "product" }
        ]
      },
      {
        "t": "每小時使用 gsutil -m cp 平行複製資料。",
        "en": "Use 'gsutil -m cp' to copy data in parallel every hour.",
        "wg": []
      }
    ],
    "answer": "Option 3",
    "why": {
      "t": "從其他雲端供應商 (如 AWS S3) 傳輸大量資料到 GCP 的最佳託管服務是「Storage Transfer Service」。它支援排程同步、增量傳輸、以及無需管理任何傳輸伺服器 (Serverless)。雖然 `gsutil rsync` (選項 2) 功能上可行，但它需要你自行維護執行命令的機器，不適合作為企業級長期同步方案。",
      "en": "The best managed service for transferring large amounts of data from other cloud providers (like AWS S3) to GCP is 'Storage Transfer Service'. It supports scheduled syncs, incremental transfers, and is serverless (no transfer servers to manage). While `gsutil rsync` (Option 2) is functionally capable, it requires you to maintain the machine running the command, making it less suitable as an enterprise-grade long-term sync solution.",
      "wg": [
        { "t": "託管服務", "en": "managed service", "ps": "noun" },
        { "t": "增量傳輸", "en": "incremental transfers", "ps": "noun" }
      ]
    }
  },
  {
    "no": "22",
    "level": "Hard",
    "keywords": "GDPR, BigQuery, Cloud Storage, Data Retention, Lifecycle Management",
    "question": [
      {
        "t": "為了符合歐洲 GDPR 法規，EuroCar 公司被要求在歐洲客戶產生的資料包含個人資料時，於 36 個月後刪除。",
        "en": "To be compliant with European GDPR regulation, EuroCar company is required to delete data generated from its European customers after a period of 36 months when it contains personal data.",
        "wg": [
          { "t": "GDPR 法規", "en": "GDPR regulation", "ps": "noun" },
          { "t": "刪除", "en": "delete", "ps": "verb" }
        ]
      },
      {
        "t": "這些資料儲存在 Cloud Storage 和 BigQuery 中。你應該做什麼？",
        "en": "This data is stored in both Cloud Storage and BigQuery. What should you do?",
        "wg": []
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "為歐洲資料建立 BigQuery 資料表，並設定資料表保留期限為 36 個月。對於 Cloud Storage，使用 gsutil 啟用含有 36 個月年齡條件的 DELETE 動作的生命週期管理。",
        "en": "Create a BigQuery table for the European data, and set the table retention period to 36 months. For Cloud Storage, use gsutil to enable lifecycle management using a DELETE action with an Age condition of 36 months.",
        "wg": [
          { "t": "資料表保留期限", "en": "table retention period", "ps": "noun" }
        ]
      },
      {
        "t": "建立 BigQuery 資料表... 對於 Cloud Storage，使用 SetStorageClass 為 NONE 動作...",
        "en": "Create a BigQuery table... For Cloud Storage, use gsutil to create a SetStorageClass to NONE action...",
        "wg": []
      },
      {
        "t": "為歐洲資料建立 BigQuery 時間分區資料表 (time-partitioned table)，並設定分區過期時間 (partition expiration) 為 36 個月。對於 Cloud Storage，使用 gsutil 啟用含有 36 個月年齡條件的 DELETE 動作的生命週期管理。",
        "en": "Create a BigQuery time-partitioned table for the European data, and set the partition expiration period to 36 months. For Cloud Storage, use gsutil to enable lifecycle management using a DELETE action with an Age condition of 36 months.",
        "wg": [
          { "t": "時間分區資料表", "en": "time-partitioned table", "ps": "noun" },
          { "t": "分區過期時間", "en": "partition expiration period", "ps": "noun" }
        ]
      },
      {
        "t": "建立 BigQuery 時間分區資料表... 對於 Cloud Storage，使用 SetStorageClass 為 NONE 動作...",
        "en": "Create a BigQuery time-partitioned table... For Cloud Storage, use gsutil to create a SetStorageClass to NONE action...",
        "wg": []
      }
    ],
    "answer": "Option 3",
    "why": {
      "t": "對於 BigQuery，要自動刪除特定時間後的資料，最佳做法是使用「分區資料表 (Partitioned Table)」並設定「分區過期 (Partition Expiration)」。如果只設定資料表保留期 (Table Retention)，會刪除整個資料表或依賴時間戳記，不如分區過期精確且效能好。對於 Cloud Storage，使用 Object Lifecycle Management 的 `DELETE` 動作 (Age=36 months) 是標準做法。",
      "en": "For BigQuery, to automatically delete data after a specific time, the best practice is to use a 'Partitioned Table' and set a 'Partition Expiration'. Setting simple table retention might apply to the whole table or rely on ingestion time, which is less precise/performant than partition expiration. For Cloud Storage, using Object Lifecycle Management with a `DELETE` action (Age=36 months) is the standard approach.",
      "wg": [
        { "t": "分區過期", "en": "Partition Expiration", "ps": "feature" },
        { "t": "精確", "en": "precise", "ps": "adjective" }
      ]
    }
  },
  {
    "no": "23",
    "level": "Medium",
    "keywords": "Cloud Logging, Aggregated Sink, Folder, Security",
    "question": [
      {
        "t": "一個組織有一個獨立的資料夾用於生產環境專案。",
        "en": "An organization has a separate folder for production projects.",
        "wg": [
          { "t": "資料夾", "en": "folder", "ps": "noun" },
          { "t": "生產環境專案", "en": "production projects", "ps": "noun" }
        ]
      },
      {
        "t": "如何確保所有生產專案 (現有及未來建立的) 的日誌都能被收集並提供給安全團隊？安全團隊對非生產日誌不感興趣。",
        "en": "How to assure that logs from all production projects (existing and created in the future) will be collected and made available for Security team? Security team is not interested in viewing non-production logs.",
        "wg": [
          { "t": "收集", "en": "collected", "ps": "verb" },
          { "t": "安全團隊", "en": "Security team", "ps": "noun" }
        ]
      },
      {
        "t": "你應該做什麼？",
        "en": "What should you do?",
        "wg": []
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "為每個現有/新專案建立 Pub/Sub 複製機制...",
        "en": "Create Pub/Sub-based replication mechanism for logs generated in each of the existing / new projects to a central logging project.",
        "wg": []
      },
      {
        "t": "在資料夾層級設定聚合接收器 (Aggregated Sink)。",
        "en": "Configure aggregated sink on folder level.",
        "wg": [
          { "t": "聚合接收器", "en": "aggregated sink", "ps": "feature" }
        ]
      },
      {
        "t": "在生產專案層級設定日誌聚合。",
        "en": "Configure logs aggregation on production projects level",
        "wg": []
      },
      {
        "t": "將日誌重新導向到地端日誌軟體。",
        "en": "Re-route logs to on-premises logging software",
        "wg": []
      }
    ],
    "answer": "Option 2",
    "why": {
      "t": "Cloud Logging 的「聚合接收器 (Aggregated Sink)」功能允許你在組織或資料夾層級設定，將該層級下所有子專案 (包含未來建立的) 的日誌自動匯出到指定目的地 (如 BigQuery 或 Pub/Sub)。這符合「自動包含未來專案」且「僅限生產資料夾」的需求，無需對每個專案手動設定。",
      "en": "Cloud Logging's 'Aggregated Sink' feature allows you to configure exports at the Organization or Folder level. This automatically captures logs from all child projects (including future ones) under that level. This meets the requirement of 'including future projects' and 'limiting to the production folder', without manual configuration for each project.",
      "wg": [
        { "t": "匯出", "en": "export", "ps": "verb" },
        { "t": "子專案", "en": "child projects", "ps": "noun" }
      ]
    }
  },
  {
    "no": "24",
    "level": "Medium",
    "keywords": "Organization Policy, Resource Location, Compliance, Constraint",
    "question": [
      {
        "t": "一家公司想要制定一項政策，限制在其實際存在的地區以外建立雲端資源的可能性。",
        "en": "A company wants to create a policy that limits the possibility to create cloud resources to regions where it's physically present.",
        "wg": [
          { "t": "政策", "en": "policy", "ps": "noun" },
          { "t": "限制", "en": "limits", "ps": "verb" },
          { "t": "地區", "en": "regions", "ps": "noun" }
        ]
      },
      {
        "t": "你將如何強制執行此政策？",
        "en": "How would you enforce such a policy?",
        "wg": [
          { "t": "強制執行", "en": "enforce", "ps": "verb" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "為每個資源建立使用自訂 Terraform 驗證器。",
        "en": "By using custom Terraform validators for each resource creation.",
        "wg": [
          { "t": "Terraform 驗證器", "en": "Terraform validators", "ps": "tool" }
        ]
      },
      {
        "t": "透過設定 IAM 條件。",
        "en": "By configuring IAM Conditions.",
        "wg": [
          { "t": "IAM 條件", "en": "IAM Conditions", "ps": "feature" }
        ]
      },
      {
        "t": "設定在不允許區域建立資源時發出警報。",
        "en": "By configuring alert when such resources are created in disallowed region.",
        "wg": [
          { "t": "警報", "en": "alert", "ps": "noun" }
        ]
      },
      {
        "t": "透過設定組織政策 (Organization Policy) 阻擋在特定區域建立資源。",
        "en": "By configuring Organization policy blocking resource creation in certain regions.",
        "wg": [
          { "t": "組織政策", "en": "Organization policy", "ps": "feature" }
        ]
      }
    ],
    "answer": "Option 4",
    "why": {
      "t": "Google Cloud Organization Policy Service (組織政策服務) 提供了 `gcp.resourceLocations` 限制條件 (Constraint)，專門用來限制資源可以建立的實體位置 (Regions)。這是集中強制執行合規性規則的標準方法，比 Terraform (僅限 IaC 階段) 或 IAM 條件更全面且不易被繞過。",
      "en": "Google Cloud Organization Policy Service provides the `gcp.resourceLocations` constraint, specifically designed to restrict the physical locations (Regions) where resources can be created. This is the standard method for centrally enforcing compliance rules, and is more comprehensive and robust than Terraform validators (IaC only) or IAM conditions.",
      "wg": [
        { "t": "限制條件", "en": "constraint", "ps": "noun" },
        { "t": "合規性", "en": "compliance", "ps": "noun" }
      ]
    }
  },
  {
    "no": "25",
    "level": "Medium",
    "keywords": "MIG, Scaling, CPU Usage, Troubleshooting, Capacity",
    "question": [
      {
        "t": "使用者回報在 MIG (託管執行個體群組) 中的數個 VM 上執行的應用程式有嚴重的效能問題。",
        "en": "Users have reported serious performance problems with an application running on several VMs grouped in MIG.",
        "wg": [
          { "t": "效能問題", "en": "performance problems", "ps": "noun" },
          { "t": "託管執行個體群組", "en": "MIG", "ps": "noun" }
        ]
      },
      {
        "t": "指標顯示每台機器上的單一應用程式處理程序佔用了 100% 的運算能力。此外，MIG 已達到執行個體上限且無法擴充更多。",
        "en": "Metrics state that on every machine single application process occupies 100% of computing power. Additionally MIG has already reached its upper limit of instances and cannot expand more.",
        "wg": [
          { "t": "佔用", "en": "occupies", "ps": "verb" },
          { "t": "運算能力", "en": "computing power", "ps": "noun" },
          { "t": "上限", "en": "upper limit", "ps": "noun" }
        ]
      },
      {
        "t": "另一方面，相鄰元件沒有檢測到問題... 你建議如何快速解決此問題？",
        "en": "On the other hand no issues were detected in neighbouring components... What would you recommend to quickly resolve the problem?",
        "wg": []
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "規劃群組內 VM 的交錯重新啟動。",
        "en": "Plan staggered restart of VMs within this group",
        "wg": []
      },
      {
        "t": "透過 ssh 登入每個 VM 並重新啟動應用程式處理程序。",
        "en": "Login to each VM via ssh and restart the application process",
        "wg": []
      },
      {
        "t": "增加給定 MIG 的執行個體數量上限。",
        "en": "Increase upper limit of number of instances within given MIG",
        "wg": [
          { "t": "數量上限", "en": "upper limit", "ps": "noun" }
        ]
      },
      {
        "t": "將 MIG 縮放指標改為基於記憶體使用量。",
        "en": "Change the MIG scaling metric to be based on memory usage instead of CPU usage",
        "wg": []
      }
    ],
    "answer": "Option 3",
    "why": {
      "t": "問題的核心在於「MIG 已達到上限」導致無法繼續水平擴展 (Scale Out)，而所有現有實例 CPU 都已滿載 (100%)。最快速的緩解措施是提高 MIG 的最大實例數限制，讓 Autoscaler 能夠啟動更多 VM 來分擔負載。重啟 (選項 1 & 2) 無法解決負載過高的根本問題；改變指標 (選項 4) 對 CPU 飽和問題無效。",
      "en": "The core issue is that the 'MIG has reached its upper limit', preventing further Scale Out, while all existing instances are at 100% CPU. The quickest mitigation is to increase the maximum instance limit of the MIG, allowing the Autoscaler to spin up more VMs to distribute the load. Restarts (Options 1 & 2) do not solve the root cause of overload; changing metrics (Option 4) won't help with CPU saturation.",
      "wg": [
        { "t": "水平擴展", "en": "Scale Out", "ps": "verb" },
        { "t": "緩解措施", "en": "mitigation", "ps": "noun" }
      ]
    }
  },{
    "no": "26",
    "level": "Hard",
    "keywords": "Cloud Spanner, Global, Relational, Autoscaling, CPU Utilization",
    "question": [
      {
        "t": "你正在設計 GCP 上的關聯式資料存放庫。資料將具備交易一致性，並從世界各地新增。",
        "en": "You are designing a relational data repository on GCP. The data will be transactionally consistent and added from any location in the world.",
        "wg": [
          { "t": "關聯式資料存放庫", "en": "relational data repository", "ps": "noun" },
          { "t": "交易一致性", "en": "transactionally consistent", "ps": "noun phrase" }
        ]
      },
      {
        "t": "你想要監控並根據無法預測的流量峰值調整處理能力。你應該做什麼？",
        "en": "You want to monitor and adjust processing capabilities based on traffic, which can spike unpredictably. What should you do?",
        "wg": [
          { "t": "處理能力", "en": "processing capabilities", "ps": "noun" },
          { "t": "流量峰值", "en": "traffic spike", "ps": "noun" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "使用 Cloud Spanner。監控儲存空間使用量，當使用量超過約 70% 時增加節點數量。",
        "en": "Use Cloud Spanner. Monitor storage usage and increase node count when more than ~70% storage is utilized.",
        "wg": [
          { "t": "儲存空間使用量", "en": "storage usage", "ps": "noun" }
        ]
      },
      {
        "t": "使用 Cloud Spanner。監控 CPU 使用率，如果使用的時間範圍內超過約 70%，則增加節點數量。",
        "en": "Use Cloud Spanner. Monitor CPU utilization and increase the node count if more than ~70% used for your time span.",
        "wg": [
          { "t": "CPU 使用率", "en": "CPU utilization", "ps": "noun" }
        ]
      },
      {
        "t": "使用 Cloud Bigtable。監控資料儲存量...",
        "en": "Use Cloud Bigtable. Monitor data stored...",
        "wg": [
          { "t": "Cloud Bigtable", "en": "Cloud Bigtable", "ps": "product" }
        ]
      },
      {
        "t": "使用 Cloud Bigtable。監控 CPU 使用率...",
        "en": "Use Cloud Bigtable. Monitor CPU utilization...",
        "wg": []
      }
    ],
    "answer": "Option 2",
    "why": {
      "t": "題目需求為「全球」、「關聯式」且「交易一致」，這直接指向 Cloud Spanner (Bigtable 是 NoSQL，不支援跨列交易)。針對「處理能力 (Processing Capabilities)」，Spanner 的擴展依據是 CPU 使用率 (尤其是高優先級 CPU)。雖然 Google 建議多區域部署的 CPU 閾值應低於 45% (以便故障轉移)，但以此題選項而言，監控 CPU 並增加節點是唯一正確的邏輯方向 (監控儲存量是針對容量而非處理效能)。",
      "en": "The requirements 'global', 'relational', and 'transactionally consistent' point directly to Cloud Spanner (Bigtable is NoSQL and lacks cross-row transactions). For 'processing capabilities', Spanner scaling is driven by CPU utilization (specifically High Priority CPU). Although Google recommends a lower threshold (<45% for multi-region) for failover safety, monitoring CPU to scale nodes is the only logically correct approach among the options (monitoring storage is for capacity, not performance).",
      "wg": [
        { "t": "閾值", "en": "threshold", "ps": "noun" },
        { "t": "故障轉移", "en": "failover", "ps": "noun" }
      ]
    }
  },
  {
    "no": "27",
    "level": "Hard",
    "keywords": "Istio, Microservices, Fault Injection, Testing, Chaos Engineering",
    "question": [
      {
        "t": "開發團隊正在對以微服務架構建立的應用程式執行開發測試。",
        "en": "Development team is performing development tests of an application built in microservice architecture.",
        "wg": [
          { "t": "微服務架構", "en": "microservice architecture", "ps": "noun" },
          { "t": "開發測試", "en": "development tests", "ps": "noun" }
        ]
      },
      {
        "t": "特別是他們想要測試在其他服務所依賴的單一微服務無法使用的情況下，應用程式的行為。這樣的測試該如何執行？",
        "en": "In particular they want to test how the application behaves in case of unavailability of a single microservice that other services rely upon. How such a test may be performed?",
        "wg": [
          { "t": "無法使用", "en": "unavailability", "ps": "noun" },
          { "t": "依賴", "en": "rely upon", "ps": "verb" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "手動刪除適當叢集的節點。",
        "en": "Manually deleting proper cluster's node",
        "wg": [
          { "t": "刪除", "en": "deleting", "ps": "verb" }
        ]
      },
      {
        "t": "使用 Istio - 設定流量管理規則，讓該微服務完全不被呼叫。",
        "en": "Using Istio - configure traffic management rules so that the given microservice is not called at all",
        "wg": [
          { "t": "流量管理規則", "en": "traffic management rules", "ps": "noun" }
        ]
      },
      {
        "t": "使用 Istio 錯誤注入 (Fault Injection) 機制來模擬特定服務的無法使用。",
        "en": "Using Istio Fault Injection mechanism to simulate unavailability of given service",
        "wg": [
          { "t": "錯誤注入", "en": "Fault Injection", "ps": "feature" },
          { "t": "模擬", "en": "simulate", "ps": "verb" }
        ]
      },
      {
        "t": "在選定的節點/Pod 上使用汙點 (Taints)。",
        "en": "Use taints on chosen nodes / pods",
        "wg": [
          { "t": "汙點", "en": "taints", "ps": "noun" }
        ]
      }
    ],
    "answer": "Option 3",
    "why": {
      "t": "Istio 的「錯誤注入 (Fault Injection)」功能允許開發者在服務網格層模擬故障 (如 HTTP 500 錯誤或延遲)，而無需實際終止 Pod 或修改程式碼。這是測試微服務韌性 (Resiliency) 的標準方法。手動刪除節點 (選項 1) 破壞性太大且不精確；修改流量規則不呼叫 (選項 2) 無法模擬錯誤回應；Taints (選項 4) 是調度層級的控制，不適合模擬執行時期的服務錯誤。",
      "en": "Istio's 'Fault Injection' feature allows developers to simulate failures (like HTTP 500 errors or latency) at the service mesh layer without actually killing pods or modifying code. This is the standard approach for testing microservice resiliency. Deleting nodes (Option 1) is too destructive and imprecise; traffic rules (Option 2) just stop calls rather than simulating error responses; Taints (Option 4) are for scheduling, not for simulating runtime service errors.",
      "wg": [
        { "t": "服務網格", "en": "service mesh", "ps": "noun" },
        { "t": "韌性", "en": "resiliency", "ps": "noun" }
      ]
    }
  },
  {
    "no": "28",
    "level": "Medium",
    "keywords": "BeyondCorp, IAP, Security, SSO, Hybrid Connectivity",
    "question": [
      {
        "t": "EHR 銷售員工是遠端工作團隊，需要前往不同地點工作。無論地點為何，他們都需要存取位於 EHR 資料中心的網頁工具。",
        "en": "The EHR Sales employees are a remote-based workforce that travels to different locations to do their job. Regardless of the location, they need to access web-based tools located in the EHR data center.",
        "wg": [
          { "t": "遠端工作團隊", "en": "remote-based workforce", "ps": "noun" },
          { "t": "資料中心", "en": "data center", "ps": "noun" }
        ]
      },
      {
        "t": "EHR 正在淘汰目前的 VPN 基礎架構，並需要將網頁銷售工具轉移到 GCP 的 BeyondCorp 存取模式。每位銷售員工都有 Google Workspace 帳戶並用於 SSO。你應該做什麼？",
        "en": "EHR is retiring the current VPN infrastructure and needs to move the web-based sales tools to a BeyondCorp access model in GCP. Each sales employee has a Google Workspace account and uses that for SSO. What should you do?",
        "wg": [
          { "t": "淘汰", "en": "retiring", "ps": "verb" },
          { "t": "BeyondCorp", "en": "BeyondCorp", "ps": "concept" },
          { "t": "單一登入", "en": "SSO", "ps": "noun" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "建立一個指向銷售工具應用程式的身分識別感知 Proxy (IAP) 連接器。",
        "en": "Create an Identity-Aware Proxy (IAP) connector that points to the sales tool application.",
        "wg": [
          { "t": "IAP 連接器", "en": "IAP connector", "ps": "component" }
        ]
      },
      {
        "t": "為銷售工具應用程式建立 Google 群組，並將該群組升級為安全群組。",
        "en": "Create a Google groups for the sales tool application, and upgrade that group to a security group.",
        "wg": []
      },
      {
        "t": "部署外部 HTTP(S) 負載平衡器並為銷售工具應用程式建立自訂 Cloud Armor 政策。",
        "en": "Deploy and external HTTP(S) load balancer and create a custom Cloud Armor policy for the sales tool application.",
        "wg": [
          { "t": "Cloud Armor", "en": "Cloud Armor", "ps": "product" }
        ]
      },
      {
        "t": "授與每位需要存取銷售網頁工具的銷售員工預先定義的 App Engine viewer 角色。",
        "en": "Grant the predefined App Engine viewer role for every sales employee who needs access to the sales web tool.",
        "wg": []
      }
    ],
    "answer": "Option 1",
    "why": {
      "t": "題目要求實現「BeyondCorp」模式並「淘汰 VPN」。BeyondCorp 的核心實作是透過 Identity-Aware Proxy (IAP)。對於位於「地端資料中心 (EHR data center)」的應用程式，GCP 提供了「IAP Connector (On-prem app connector)」，它允許透過 GCP 的 IAP 服務安全地存取地端應用程式，而無需 VPN，並整合 Google Workspace SSO 進行身分驗證。",
      "en": "The requirement is to implement 'BeyondCorp' model and 'retire VPN'. The core implementation of BeyondCorp is Identity-Aware Proxy (IAP). For applications located in an 'on-premises data center (EHR data center)', GCP provides the 'IAP Connector', which allows secure access to on-prem apps via GCP's IAP service without a VPN, leveraging Google Workspace SSO for authentication.",
      "wg": [
        { "t": "地端", "en": "on-premises", "ps": "adjective" },
        { "t": "身分驗證", "en": "authentication", "ps": "noun" }
      ]
    }
  },{
    "no": "29",
    "level": "Medium",
    "keywords": "Load Balancer, URL Maps, API Versioning, Backend Services",
    "question": [
      {
        "t": "你的公司已決定對其 API 進行重大修訂，以便為開發人員創造更好的體驗。",
        "en": "Your company has decided to make a major revision of their API in order to create a better experience for their developers.",
        "wg": [
          { "t": "重大修訂", "en": "major revision", "ps": "noun" }
        ]
      },
      {
        "t": "他們需要保持舊版 API 仍然可用，同時允許新客戶和測試人員存取新版。他們應該做什麼？",
        "en": "They need to keep the old version of the API still available, while allowing new customers and testers to access the new one. What should they do?",
        "wg": [
          { "t": "舊版", "en": "old version", "ps": "noun" },
          { "t": "新版", "en": "new one", "ps": "noun" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "為新版 API 設定新的負載平衡器。",
        "en": "Configure a new load balancer for the new version of the API.",
        "wg": []
      },
      {
        "t": "重新設定舊用戶端以使用新 API 的新端點。",
        "en": "Reconfigure the old clients to use the new endpoint for the new API.",
        "wg": []
      },
      {
        "t": "讓舊 API 根據路徑將流量轉送到新 API。",
        "en": "Have the old API forward traffic to the new API based on the path.",
        "wg": []
      },
      {
        "t": "在同一個負載平衡器後方，為每個 API 路徑使用獨立的後端服務。",
        "en": "Use separate backend services for each API path behind the same load balancer.",
        "wg": [
          { "t": "後端服務", "en": "backend services", "ps": "noun" },
          { "t": "路徑", "en": "path", "ps": "noun" }
        ]
      }
    ],
    "answer": "Option 4",
    "why": {
      "t": "HTTP(S) 負載平衡器支援 URL 對應 (URL Maps)，可以根據請求的路徑 (例如 `/v1/` 或 `/v2/`) 將流量導向不同的後端服務。這允許使用單一 IP 位址和網域來同時服務新舊版本的 API，無需更改 DNS 或管理多個 SSL 憑證 (選項 1 的缺點)，也不需要強迫舊客戶端更新 (選項 2 的缺點)。",
      "en": "HTTP(S) Load Balancers support URL Maps, which can route traffic to different backend services based on the request path (e.g., `/v1/` vs `/v2/`). This allows serving both old and new API versions behind a single IP and domain, without changing DNS or managing multiple SSL certificates (flaws in Option 1), and without forcing old clients to update (flaw in Option 2).",
      "wg": [
        { "t": "URL 對應", "en": "URL Maps", "ps": "feature" },
        { "t": "導向", "en": "route", "ps": "verb" }
      ]
    }
  },
  {
    "no": "30",
    "level": "Medium",
    "keywords": "Cloud Storage, gsutil, Performance Optimization, Data Transfer",
    "question": [
      {
        "t": "你正在從地端位置將非常大量的「小檔案」傳輸到 GCS 儲存桶。",
        "en": "You are transferring a very large number of small files to GCS bucket from on-premises location.",
        "wg": [
          { "t": "小檔案", "en": "small files", "ps": "noun" },
          { "t": "地端", "en": "on-premises", "ps": "adjective" }
        ]
      },
      {
        "t": "你需要加快傳輸時間。假設網路連線速度很快，你可以採取哪兩個動作？(選擇兩項)",
        "en": "You need to speed up transfer times. Assuming a fast network connection, which two actions can you take? (choose 2)",
        "wg": [
          { "t": "加快", "en": "speed up", "ps": "verb" }
        ]
      }
    ],
    "type": "多選題",
    "options": [
      {
        "t": "在傳輸前壓縮並合併檔案。",
        "en": "Compress and combine files before transferring.",
        "wg": [
          { "t": "壓縮", "en": "Compress", "ps": "verb" },
          { "t": "合併", "en": "combine", "ps": "verb" }
        ]
      },
      {
        "t": "針對大型傳輸使用 \"-r\" 選項。",
        "en": "Use the '-r' option for large transfers.",
        "wg": []
      },
      {
        "t": "一次以較大的區塊複製檔案。",
        "en": "Copy the files in bigger pieces at a time.",
        "wg": []
      },
      {
        "t": "使用 \"-m\" 選項進行平行處理。",
        "en": "Use the '-m' option for parallel processing.",
        "wg": [
          { "t": "平行處理", "en": "parallel processing", "ps": "noun" }
        ]
      }
    ],
    "answer": "Option 1, Option 4",
    "why": {
      "t": "傳輸大量小檔案時，每個檔案的 API 請求開銷 (Overhead) 會顯著降低整體吞吐量。解決方案有二：1. 使用 `gsutil -m` 啟用多執行緒平行上傳，充分利用頻寬。2. 先將小檔案「打包 (Combine/Tar)」成大檔案再傳輸，減少 API 呼叫次數，到達雲端後再解開。單純使用 `-r` (遞迴) 只是基本功能，並不會加速。",
      "en": "When transferring a large number of small files, the per-file API request overhead significantly reduces overall throughput. Two solutions are: 1. Use `gsutil -m` to enable multi-threaded parallel uploads, saturating the bandwidth. 2. 'Combine/Tar' small files into larger archives before transfer to reduce the number of API calls, then unpack in the cloud. Using `-r` (recursive) is just functionality, not an optimization.",
      "wg": [
        { "t": "開銷", "en": "overhead", "ps": "noun" },
        { "t": "打包", "en": "combine", "ps": "verb" }
      ]
    }
  },
  {
    "no": "31",
    "level": "Easy",
    "keywords": "App Engine, Cloud Datastore, Web Application, Managed Service",
    "question": [
      {
        "t": "你的行銷部門想要發送一個促銷活動。開發團隊想要將直接的維運管理降到最低。",
        "en": "Your marketing department wants to send out a promotional campaign. The development team wants to minimize direct operation management.",
        "wg": [
          { "t": "維運管理", "en": "operation management", "ps": "noun" },
          { "t": "降到最低", "en": "minimize", "ps": "verb" }
        ]
      },
      {
        "t": "他們預估客戶回應範圍很廣，從每天 100 到 500,000 次點擊。連結會導向一個解釋促銷並收集使用者資訊和偏好的簡單網站。",
        "en": "They project a wide range of customer responses, ranging from 100 to 500,000 click-throughs per day. The link leads to a simple website that explains the promotion and collects user information and preferences.",
        "wg": [
          { "t": "點擊", "en": "click-throughs", "ps": "noun" }
        ]
      },
      {
        "t": "你應該使用哪些服務來部署這樣的系統？",
        "en": "Which services would you use to deploy such a system?",
        "wg": []
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "使用單一 GCE VM 託管網頁伺服器，後端使用 Cloud SQL。",
        "en": "Use a single GCE VM to host a web server, backed by Cloud SQL.",
        "wg": []
      },
      {
        "t": "使用 GKE 叢集服務網站，並將資料儲存在永久磁碟上。",
        "en": "Use a GKE cluster to serve the website and stor data on a Persistent Disk.",
        "wg": []
      },
      {
        "t": "使用託管服務群組 (MIG) 服務網站，並使用 Cloud Bigtable 儲存使用者資料。",
        "en": "Use a managed service group to serve the website and Cloud Bigtable to store user data.",
        "wg": []
      },
      {
        "t": "使用 App Engine 服務網站，並使用 Datastore 儲存使用者資料。",
        "en": "Use App Engine to serve the website and Datastore to store user data.",
        "wg": [
          { "t": "App Engine", "en": "App Engine", "ps": "product" },
          { "t": "Datastore", "en": "Datastore", "ps": "product" }
        ]
      }
    ],
    "answer": "Option 4",
    "why": {
      "t": "關鍵需求是「將維運管理降到最低 (minimize operation management)」以及處理「大幅變動的流量 (100 - 500k)」。App Engine (標準環境) 是全託管的 PaaS，能自動從零擴展到極大規模，完美符合需求。Datastore 是與 App Engine 高度整合的 NoSQL 資料庫，適合儲存簡單的使用者資料與偏好。VM 或 GKE 需要較多的配置與維護；Bigtable 對於這種簡單應用來說是大材小用且成本過高。",
      "en": "The key requirements are 'minimize operation management' and handling 'widely ranging traffic (100 - 500k)'. App Engine (Standard) is a fully managed PaaS that autoscales from zero to massive scale, fitting perfectly. Datastore is a NoSQL database tightly integrated with App Engine, ideal for storing simple user data and preferences. VMs or GKE require more configuration/maintenance; Bigtable is overkill and too expensive for this simple use case.",
      "wg": [
        { "t": "全託管", "en": "fully managed", "ps": "adjective" },
        { "t": "大材小用", "en": "overkill", "ps": "noun" }
      ]
    }
  },
  {
    "no": "32",
    "level": "Hard",
    "keywords": "VPC Service Controls, Security, Data Exfiltration, Hybrid",
    "question": [
      {
        "t": "一家大型資料分析公司使用 BigQuery, Bigtable, Dataproc 和 GCS 服務。他們使用包含地端和 GCP 的混合基礎架構。",
        "en": "One of the large data analytics companies uses BigQuery, Bigtable, Dataproc and GCS services. The use a hybrid infrastructure involving on-premises and GCP.",
        "wg": [
          { "t": "混合基礎架構", "en": "hybrid infrastructure", "ps": "noun" }
        ]
      },
      {
        "t": "主要的挑戰之一是減緩源自身分遭竊、IAM 設定錯誤、惡意內部人員等的資料外洩風險。他們可以使用什麼 GCP 服務來解決這些挑戰？",
        "en": "One of the main challenges is mitigating data exfiltration risks originating from stolen identities, misconfigured IAM, malicious insiders etc. What GCP service can they use to address those challenges?",
        "wg": [
          { "t": "資料外洩", "en": "data exfiltration", "ps": "noun" },
          { "t": "減緩", "en": "mitigating", "ps": "verb" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "共用 VPC (Shared VPC)",
        "en": "Shared VPC",
        "wg": []
      },
      {
        "t": "Cloud Armor",
        "en": "Cloud Armor",
        "wg": []
      },
      {
        "t": "VPC 服務控制 (VPC Service Controls)",
        "en": "VPC Service Controls",
        "wg": [
          { "t": "VPC 服務控制", "en": "VPC Service Controls", "ps": "product" }
        ]
      },
      {
        "t": "資源管理員 (Resource Manager)",
        "en": "Resource Manager",
        "wg": []
      }
    ],
    "answer": "Option 3",
    "why": {
      "t": "VPC Service Controls 是 GCP 專門用於防止資料外洩 (Data Exfiltration) 的安全產品。它允許你在 GCP 託管服務 (如 BigQuery, GCS) 周圍定義安全邊界 (Perimeter)，即使 IAM 權限被盜用，只要請求來自邊界外部 (未授權網路)，存取就會被拒絕。Shared VPC 是網路管理工具；Cloud Armor 是 DDoS/WAF 防護；Resource Manager 是組織層級管理工具。",
      "en": "VPC Service Controls is the GCP product specifically designed to prevent Data Exfiltration. It allows you to define security perimeters around GCP managed services (like BigQuery, GCS). Even if IAM credentials are stolen, access is denied if the request originates from outside the perimeter (unauthorized networks). Shared VPC is for network management; Cloud Armor is for DDoS/WAF; Resource Manager is for organizational hierarchy.",
      "wg": [
        { "t": "安全邊界", "en": "security perimeters", "ps": "noun" },
        { "t": "未授權網路", "en": "unauthorized networks", "ps": "noun phrase" }
      ]
    }
  },
  {
    "no": "33",
    "level": "Medium",
    "keywords": "IAP, TCP Forwarding, Security, SSH, RDP, Bastion",
    "question": [
      {
        "t": "你在一家跨國公司工作，並使用 SSH 和 RDP 協定管理許多 GCE 執行個體。",
        "en": "You work for an international company and manage many GCE instances using SSH and RDP protocols.",
        "wg": [
          { "t": "管理", "en": "manage", "ps": "verb" }
        ]
      },
      {
        "t": "基於安全考量，管理層要求 VM 不再擁有外部 IP。你該如何滿足此要求，同時仍能管理這些 VM？",
        "en": "For security reasons, management asks you that VMs can no longer have external IPs. How can you fulfil this request, while still being able to manage those VMs?",
        "wg": [
          { "t": "外部 IP", "en": "external IPs", "ps": "noun" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "使用 Bastion 主機。",
        "en": "Use Bastion Hosts",
        "wg": [
          { "t": "Bastion 主機", "en": "Bastion Hosts", "ps": "noun" }
        ]
      },
      {
        "t": "使用 NAT 執行個體。",
        "en": "Use NAT instances",
        "wg": []
      },
      {
        "t": "使用 IAP TCP 轉送 (IAP TCP Forwarding)。",
        "en": "Use IAP TCP Forwarding",
        "wg": [
          { "t": "IAP TCP 轉送", "en": "IAP TCP Forwarding", "ps": "feature" }
        ]
      },
      {
        "t": "使用 Security Command Center。",
        "en": "Use Security Command Center",
        "wg": []
      }
    ],
    "answer": "Option 3",
    "why": {
      "t": "Identity-Aware Proxy (IAP) 的 TCP 轉送功能允許使用者透過 HTTPS 連線建立 SSH 或 RDP 通道到沒有外部 IP 的 VM。Google 的基礎架構會代理這些流量，並在允許連線前驗證使用者身分 (IAM)。這比傳統的 Bastion Host 更安全且易於管理 (不需要維護跳板機)。",
      "en": "Identity-Aware Proxy (IAP) TCP forwarding allows users to establish SSH or RDP tunnels over HTTPS to VMs that do not have external IPs. Google's infrastructure proxies this traffic and authenticates the user (IAM) before allowing the connection. This is more secure and easier to manage than traditional Bastion Hosts (no jump servers to maintain).",
      "wg": [
        { "t": "代理", "en": "proxies", "ps": "verb" },
        { "t": "跳板機", "en": "jump servers", "ps": "noun" }
      ]
    }
  },
  {
    "no": "34",
    "level": "Hard",
    "keywords": "Hybrid Connectivity, Cloud Interconnect, Bandwidth, RFC1918",
    "question": [
      {
        "t": "你的公司已決定在 GCP 上建立其地端使用者驗證 PostgreSQL 資料庫的備份複本。",
        "en": "Your company has decided to build a backup replica of their on-premises user authentication PostgreSQL database on GCP.",
        "wg": [
          { "t": "備份複本", "en": "backup replica", "ps": "noun" }
        ]
      },
      {
        "t": "資料庫大小為 4 TB，且頻繁進行大量更新。解決方案需要 RFC1918 位址空間。哪種網路連線方式是最佳選擇？",
        "en": "The database is 4 TB and large updates are frequent. The solution requires RFC1918 address space. Which networking approach would be the best choice?",
        "wg": [
          { "t": "頻繁進行大量更新", "en": "large updates are frequent", "ps": "adjective phrase" },
          { "t": "RFC1918", "en": "RFC1918", "ps": "standard" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "建立兩個 VPN 通道...",
        "en": "Create two VPN tunnels with the same Cloud VPN gateway to the same destination VPN gateway.",
        "wg": []
      },
      {
        "t": "使用 Direct Peering。",
        "en": "Use Direct Peering.",
        "wg": []
      },
      {
        "t": "使用 Dedicated 或 Partner Interconnect。",
        "en": "Use Dedicated or Partner Interconnect.",
        "wg": [
          { "t": "Dedicated Interconnect", "en": "Dedicated Interconnect", "ps": "product" },
          { "t": "Partner Interconnect", "en": "Partner Interconnect", "ps": "product" }
        ]
      },
      {
        "t": "使用 Cloud VPN 連接到資料中心網路。",
        "en": "Connect to datacenter network using Cloud VPN.",
        "wg": []
      }
    ],
    "answer": "Option 3",
    "why": {
      "t": "題目強調「4 TB 資料」與「頻繁大量更新」，這暗示需要高頻寬與穩定的連線。Cloud VPN 透過公共網際網路傳輸，效能與穩定性不如專線。Direct Peering 不支援 RFC1918 (私有 IP) 位址空間延伸。Cloud Interconnect (Dedicated 或 Partner) 提供高頻寬、低延遲的私有專線連線，並支援 RFC1918，是最佳選擇。",
      "en": "The emphasis on '4 TB data' and 'frequent large updates' implies a need for high bandwidth and stability. Cloud VPN transmits over the public internet, which offers less consistent performance than a dedicated line. Direct Peering does not support RFC1918 (private IP) address extension. Cloud Interconnect (Dedicated or Partner) provides high-bandwidth, low-latency private connections and supports RFC1918, making it the best choice.",
      "wg": [
        { "t": "高頻寬", "en": "high bandwidth", "ps": "noun" },
        { "t": "專線", "en": "dedicated line", "ps": "noun" }
      ]
    }
  },
  {
    "no": "35",
    "level": "Hard",
    "keywords": "Compute Engine, High Availability, Zone Failure, Overprovisioning",
    "question": [
      {
        "t": "為了確保你的應用程式即使在整個區域 (Zone) 故障時也能處理負載，你應該做什麼？(選擇 2 項)",
        "en": "To ensure your application will handle the load even if an entire zone fails, what should you do? (choose 2)",
        "wg": [
          { "t": "區域故障", "en": "zone fails", "ps": "event" },
          { "t": "處理負載", "en": "handle the load", "ps": "verb phrase" }
        ]
      }
    ],
    "type": "多選題",
    "options": [
      {
        "t": "建立託管執行個體群組時不要選擇「多區域」選項。",
        "en": "Don't select 'multizone' option when creating managed instance groups.",
        "wg": []
      },
      {
        "t": "將託管執行個體群組分散到兩個區域，並過度佈建 100% (針對兩個區域)。",
        "en": "Spread your managed instance group over two zones and overprovision by 100% (for two zones).",
        "wg": [
          { "t": "過度佈建", "en": "overprovision", "ps": "verb" }
        ]
      },
      {
        "t": "建立區域性、非託管執行個體群組...",
        "en": "Create a regional, unmanaged instance group and spread your instances across multiple zones.",
        "wg": []
      },
      {
        "t": "將區域性託管執行個體群組過度佈建至少 50% (針對三個區域)。",
        "en": "Overprovision your regional managed instance group by at least 50% (for three zones).",
        "wg": []
      }
    ],
    "answer": "Option 2, Option 4",
    "why": {
      "t": "為了容忍 N 個區域中的 1 個故障，剩餘的 N-1 個區域必須能承載總流量 (T)。\n1. 若使用 2 個區域：剩餘 1 個需承載 100% 流量。因此平時每個區域需有 T 容量 (總容量 2T)。過度佈建率 = (2T-T)/T = 100%。\n2. 若使用 3 個區域：剩餘 2 個需各承載 50% 流量。因此平時每個區域需有 0.5T 容量 (總容量 1.5T)。過度佈建率 = (1.5T-T)/T = 50%。\n這兩個選項在數學與架構上都是正確的高可用性 (HA) 策略。",
      "en": "To tolerate the failure of 1 out of N zones, the remaining N-1 zones must handle the total traffic (T).\n1. With 2 zones: The remaining 1 must handle 100%. So normally each zone needs capacity T (Total 2T). Overprovisioning = 100%.\n2. With 3 zones: The remaining 2 must handle 50% each. So normally each zone needs capacity 0.5T (Total 1.5T). Overprovisioning = 50%.\nBoth options are mathematically and architecturally valid High Availability (HA) strategies.",
      "wg": [
        { "t": "容忍", "en": "tolerate", "ps": "verb" },
        { "t": "總流量", "en": "total traffic", "ps": "noun" }
      ]
    }
  },
  {
    "no": "36",
    "level": "Medium",
    "keywords": "Cloud Bigtable, IoT, Time Series, High Throughput",
    "question": [
      {
        "t": "你想要優化一個精確、即時的天氣圖表應用程式的效能。資料來自 50,000 個感測器，每秒傳送 10 筆讀數。",
        "en": "You want to optimize the performance of an accurate, real-time, weather-charting application. The data comes from 50,000 sensors sending 10 readings a second.",
        "wg": [
          { "t": "天氣圖表", "en": "weather-charting", "ps": "adjective" }
        ]
      },
      {
        "t": "你應該將資料儲存在哪裡？",
        "en": "Where should you store the data?",
        "wg": []
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "BigQuery",
        "en": "BigQuery",
        "wg": []
      },
      {
        "t": "Cloud SQL",
        "en": "Cloud SQL",
        "wg": []
      },
      {
        "t": "Bigtable",
        "en": "Bigtable",
        "wg": [
          { "t": "Bigtable", "en": "Bigtable", "ps": "product" }
        ]
      },
      {
        "t": "GCS",
        "en": "GCS",
        "wg": []
      }
    ],
    "answer": "Option 3",
    "why": {
      "t": "50,000 * 10 = 500,000 IOPS (每秒寫入次數)。這屬於高吞吐量、時間序列資料。Cloud Bigtable 是 GCP 上唯一能輕鬆處理此規模寫入的 NoSQL 資料庫，專為 IoT 與時間序列設計。Cloud SQL 無法負荷；BigQuery 適合分析；GCS 不適合頻繁的小寫入。",
      "en": "50,000 * 10 = 500,000 IOPS. This is high-throughput, time-series data. Cloud Bigtable is the only NoSQL database on GCP designed to easily handle this scale of writes, specifically for IoT and time-series. Cloud SQL cannot handle the load; BigQuery is for analytics; GCS is not for frequent small writes.",
      "wg": [
        { "t": "IOPS", "en": "IOPS", "ps": "metric" },
        { "t": "規模", "en": "scale", "ps": "noun" }
      ]
    }
  },
  {
    "no": "37",
    "level": "Medium",
    "keywords": "Cloud Storage, Lifecycle Management, Cost Optimization, JSON",
    "question": [
      {
        "t": "你正在建立一個解決方案，以從你的 Cloud Storage 備份儲存桶中移除超過 90 天的備份檔案。你想要優化持續的花費。",
        "en": "You are creating a solution to remove backup files older than 90 days from your backup Cloud Storage bucket. You want to optimize ongoing Cloud Storage spend.",
        "wg": [
          { "t": "優化花費", "en": "optimize spend", "ps": "verb phrase" }
        ]
      },
      {
        "t": "你應該做什麼？",
        "en": "What should you do?",
        "wg": []
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "撰寫 XML 格式的生命週期管理規則...",
        "en": "Write a lifecycle management rule in XML and push it to the bucket with gsutil",
        "wg": []
      },
      {
        "t": "撰寫 JSON 格式的生命週期管理規則，並使用 gsutil 推送到儲存桶。",
        "en": "Write a lifecycle management rule in JSON and push it to the bucket with gsutil",
        "wg": [
          { "t": "JSON", "en": "JSON", "ps": "format" }
        ]
      },
      {
        "t": "排程一個 cron 腳本使用 gsutil ls -lr...",
        "en": "Schedule a cron script using gsutil ls -lr gs://backups/** to find and remove items older than 90 days",
        "wg": []
      },
      {
        "t": "排程一個 cron 腳本使用 gsutil ls -l...",
        "en": "Schedule a cron script using gsutil Is -I gs://backups/** to find and remove items older than 90 days and schedule it with cron",
        "wg": []
      }
    ],
    "answer": "Option 2",
    "why": {
      "t": "GCP 建議使用 JSON 格式來定義物件生命週期管理 (Lifecycle Management) 規則 (雖然 XML 也支援，但 JSON 是現代標準)。這是無伺服器且免費的解決方案。使用 cron 腳本需要運算資源與支付列舉 API 費用，不符合成本優化原則。",
      "en": "GCP recommends using JSON format to define Object Lifecycle Management rules (though XML is supported, JSON is the modern standard). This is a serverless and free solution. Using cron scripts requires compute resources and incurs enumeration API costs, failing the cost optimization requirement.",
      "wg": [
        { "t": "現代標準", "en": "modern standard", "ps": "noun" }
      ]
    }
  },
  {
    "no": "38",
    "level": "Hard",
    "keywords": "Dataflow, Stream Processing, Batch Processing, Migration",
    "question": [
      {
        "t": "你的公司已成功遷移到雲端，並想要分析其資料串流以優化營運。他們沒有任何現有的分析程式碼。",
        "en": "Your company has successfully migrated to the cloud and wants to analyze their data stream to optimize operations. They do not have any existing code for this analysis.",
        "wg": [
          { "t": "資料串流", "en": "data stream", "ps": "noun" },
          { "t": "分析", "en": "analyze", "ps": "verb" }
        ]
      },
      {
        "t": "這些選項包含批次和串流處理的混合，因為他們正在執行一些每小時的工作和即時處理部分資料。",
        "en": "These options include a mix of batch and stream processing, as they are running some hourly jobs and live processing some data as it comes in.",
        "wg": [
          { "t": "批次", "en": "batch", "ps": "noun" },
          { "t": "串流", "en": "stream", "ps": "noun" }
        ]
      },
      {
        "t": "他們應該使用哪種技術？",
        "en": "Which technology should they use for this?",
        "wg": []
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "Dataproc",
        "en": "Dataproc",
        "wg": []
      },
      {
        "t": "Dataflow",
        "en": "Dataflow",
        "wg": [
          { "t": "Dataflow", "en": "Dataflow", "ps": "product" }
        ]
      },
      {
        "t": "GKE with Bigtable",
        "en": "GKE with Bigtable",
        "wg": []
      },
      {
        "t": "GCE with BigQuery",
        "en": "GCE with BigQuery",
        "wg": []
      }
    ],
    "answer": "Option 2",
    "why": {
      "t": "Google Cloud Dataflow 是一個統一的程式設計模型 (基於 Apache Beam)，能夠同時處理「批次 (Batch)」和「串流 (Stream)」資料，且無需修改程式碼邏輯。對於「沒有現有程式碼」的新專案，Dataflow 是全託管且最推薦的選擇。Dataproc 適合遷移現有的 Hadoop/Spark 工作負載。",
      "en": "Google Cloud Dataflow is a unified programming model (based on Apache Beam) capable of handling both 'Batch' and 'Stream' processing with the same code logic. For a new project with 'no existing code', Dataflow is the fully managed and recommended choice. Dataproc is better suited for migrating existing Hadoop/Spark workloads.",
      "wg": [
        { "t": "統一的", "en": "unified", "ps": "adjective" },
        { "t": "工作負載", "en": "workloads", "ps": "noun" }
      ]
    }
  },
  {
    "no": "39",
    "level": "Medium",
    "keywords": "Compute Engine, Persistent Disk, Resize, Linux, Downtime",
    "question": [
      {
        "t": "Google Compute Engine 上的生產環境資料庫虛擬機器有一個 ext4 格式的永久磁碟用於資料檔案。資料庫儲存空間即將用盡。",
        "en": "A production database virtual machine on Google Compute Engine has an ext4-formatted persistent disk for data files. The database is about to run out of storage space.",
        "wg": [
          { "t": "生產環境", "en": "production", "ps": "adjective" },
          { "t": "儲存空間", "en": "storage space", "ps": "noun" }
        ]
      },
      {
        "t": "你如何能以最少的停機時間修正此問題？",
        "en": "How can you remediate the problem with the least amount of downtime?",
        "wg": [
          { "t": "修正", "en": "remediate", "ps": "verb" },
          { "t": "最少的停機時間", "en": "least amount of downtime", "ps": "noun phrase" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "關閉 VM，增加磁碟大小，重新啟動 VM。",
        "en": "Shut down the virtual machine, use the Cloud Platform Console to increase the persistent disk size, then restart the virtual machine",
        "wg": []
      },
      {
        "t": "在主控台增加磁碟大小，並在 Linux 中使用 resize2fs 命令。",
        "en": "In the Cloud Platform Console, increase the size of the persistent disk and use the resize2fs command in Linux.",
        "wg": [
          { "t": "resize2fs", "en": "resize2fs", "ps": "command" }
        ]
      },
      {
        "t": "建立新磁碟，搬移檔案...",
        "en": "In the Cloud Platform Console, create a new persistent disk... move the files...",
        "wg": []
      },
      {
        "t": "建立快照，還原到新磁碟...",
        "en": "In the Cloud Platform Console, create a snapshot... restore... to a new larger disk...",
        "wg": []
      }
    ],
    "answer": "Option 2",
    "why": {
      "t": "GCP 支援線上調整永久磁碟大小 (Online Resizing)，無需關閉 VM。步驟是：1. 在 GCP 控制台或 API 增加磁碟大小。2. SSH 進入 VM，使用 `resize2fs` (針對 ext4) 擴展檔案系統以使用新增的空間。此方法實現了零停機時間，優於其他需要關機或卸載磁碟的選項。",
      "en": "GCP supports Online Resizing of Persistent Disks, requiring no VM shutdown. The steps are: 1. Increase disk size via Console or API. 2. SSH into the VM and use `resize2fs` (for ext4) to extend the file system to use the new space. This achieves zero downtime, superior to other options that require shutdown or unmounting.",
      "wg": [
        { "t": "線上調整大小", "en": "Online Resizing", "ps": "feature" },
        { "t": "擴展", "en": "extend", "ps": "verb" }
      ]
    }
  },
  {
    "no": "40",
    "level": "Medium",
    "keywords": "CI/CD, Deployment Strategy, Rollback, Green-Blue, Microservices",
    "question": [
      {
        "t": "你需要減少公司網站託管平台錯誤的生產環境部署導致的非計畫性復原次數。",
        "en": "You need to reduce the number of unplanned rollbacks of erroneous production deployments in your company's web hosting platform.",
        "wg": [
          { "t": "非計畫性復原", "en": "unplanned rollbacks", "ps": "noun" },
          { "t": "錯誤的", "en": "erroneous", "ps": "adjective" }
        ]
      },
      {
        "t": "你可以採取哪兩個額外的方法來進一步優化發布流程？(選擇 2 項)",
        "en": "Which additional two approaches can you take to further optimize the release process? Choose 2 answers.",
        "wg": [
          { "t": "發布流程", "en": "release process", "ps": "noun" }
        ]
      }
    ],
    "type": "多選題",
    "options": [
      {
        "t": "引入藍綠部署模式 (Green-Blue Deployment)。",
        "en": "Introduce a green-blue deployment model",
        "wg": [
          { "t": "藍綠部署", "en": "green-blue deployment", "ps": "strategy" }
        ]
      },
      {
        "t": "用金絲雀發布取代 QA 環境。",
        "en": "Replace the QA environment with canary releases",
        "wg": []
      },
      {
        "t": "將單體平台碎片化為微服務。",
        "en": "Fragment the monolithic platform into microservices",
        "wg": [
          { "t": "單體", "en": "monolithic", "ps": "adjective" },
          { "t": "微服務", "en": "microservices", "ps": "noun" }
        ]
      },
      {
        "t": "減少平台對關聯式資料庫的依賴。",
        "en": "Reduce the platform's dependency on relational database systems",
        "wg": []
      },
      {
        "t": "將資料庫替換為 NoSQL。",
        "en": "Replace the platform's relational database systems with a NoSQL database",
        "wg": []
      }
    ],
    "answer": "Option 1, Option 3",
    "why": {
      "t": "1. 「藍綠部署」允許你在切換流量前在生產環境驗證新版本，若有問題可立即切回，大幅降低部署風險。2. 「將單體轉為微服務」能縮小每次變更的範圍 (Blast Radius)，使部署更獨立且易於測試/復原。取消 QA (選項 2) 會增加風險；資料庫選擇 (選項 4 & 5) 與部署復原率無直接關聯。",
      "en": "1. 'Green-Blue Deployment' allows verifying the new version in production before switching traffic, and enables instant rollback, significantly reducing deployment risk. 2. 'Fragmenting monolithic into microservices' reduces the blast radius of changes, making deployments more independent and easier to test/rollback. Removing QA (Option 2) increases risk; database choices (Options 4 & 5) are not directly related to deployment rollback rates.",
      "wg": [
        { "t": "爆炸半徑", "en": "Blast Radius", "ps": "concept" },
        { "t": "驗證", "en": "verifying", "ps": "verb" }
      ]
    }
  },
  {
    "no": "41",
    "level": "Easy",
    "keywords": "NoSQL, Database Selection, IoT, Scalability",
    "question": [
      {
        "t": "你的公司想要追蹤是否有人在預約的會議室中。共有 1000 間會議室，每秒回報狀態。",
        "en": "Your company wants to track whether someone is present in a meeting room reserved for a scheduled meeting. There are 1000 meeting rooms... reports its status every second.",
        "wg": [
          { "t": "會議室", "en": "meeting room", "ps": "noun" }
        ]
      },
      {
        "t": "資料僅包含感測器 ID 和幾個離散項目。分析師將使用這些資料...",
        "en": "The data... includes only a sensor ID and several different discrete items of information. Analysts will use this data...",
        "wg": [
          { "t": "離散項目", "en": "discrete items", "ps": "noun" }
        ]
      },
      {
        "t": "你應該使用哪種資料庫類型？",
        "en": "Which database type should you use?",
        "wg": []
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "一般檔案 (Flat file)",
        "en": "Flat file",
        "wg": []
      },
      {
        "t": "NoSQL",
        "en": "NoSQL",
        "wg": [
          { "t": "NoSQL", "en": "NoSQL", "ps": "type" }
        ]
      },
      {
        "t": "關聯式 (Relational)",
        "en": "Relational",
        "wg": []
      },
      {
        "t": "Blobstore",
        "en": "Blobstore",
        "wg": []
      }
    ],
    "answer": "Option 2",
    "why": {
      "t": "資料特性為：高寫入頻率 (1000 寫入/秒)、結構簡單 (ID + 狀態)、需要擴展性。NoSQL 資料庫 (如 Cloud Bigtable 或 Datastore) 最適合處理這種 IoT 感測器資料流。關聯式資料庫成本較高且擴展較難；Flat file 和 Blobstore 不適合儲存和查詢大量的微小狀態更新紀錄。",
      "en": "Data characteristics: High write frequency (1000 writes/sec), simple structure (ID + status), needs scalability. NoSQL databases (like Cloud Bigtable or Datastore) are best suited for handling this kind of IoT sensor data stream. Relational databases are more expensive and harder to scale; Flat files and Blobstores are not suitable for storing and querying massive amounts of tiny status update records.",
      "wg": [
        { "t": "高寫入頻率", "en": "high write frequency", "ps": "noun phrase" },
        { "t": "擴展性", "en": "scalability", "ps": "noun" }
      ]
    }
  },
  {
    "no": "42",
    "level": "Medium",
    "keywords": "Security, CI/CD, DevSecOps, Vulnerability Scanning",
    "question": [
      {
        "t": "你的公司非常重視快速回應和滿足客戶需求... 你想要減少意外引入安全錯誤的機會。",
        "en": "Your company places a high value on being responsive... You want to reduce the chance of security errors being accidentally introduced.",
        "wg": [
          { "t": "安全錯誤", "en": "security errors", "ps": "noun" }
        ]
      },
      {
        "t": "你可以採取哪兩個動作？(選擇 2 項)",
        "en": "Which two actions can you take? Choose 2 answers.",
        "wg": []
      }
    ],
    "type": "多選題",
    "options": [
      {
        "t": "確保每次程式碼簽入都由安全專家 (SME) 審查。",
        "en": "Ensure every code check-in is peer reviewed by a security SME",
        "wg": []
      },
      {
        "t": "在 CI/CD 管道中使用原始碼安全分析器。",
        "en": "Use source code security analyzers as part of the CI/CD pipeline",
        "wg": [
          { "t": "安全分析器", "en": "security analyzers", "ps": "tool" }
        ]
      },
      {
        "t": "確保有 stub 來單元測試所有介面。",
        "en": "Ensure you have stubs to unit test all interfaces between components",
        "wg": []
      },
      {
        "t": "啟用程式碼簽章...",
        "en": "Enable code signing and a trusted binary repository...",
        "wg": []
      },
      {
        "t": "在 CI/CD 管道中執行漏洞安全掃描器。",
        "en": "Run a vulnerability security scanner as part of your continuous-integration /continuous-delivery (CI/CD) pipeline",
        "wg": [
          { "t": "漏洞安全掃描器", "en": "vulnerability security scanner", "ps": "tool" }
        ]
      }
    ],
    "answer": "Option 2, Option 5",
    "why": {
      "t": "為了在不犧牲「速度與敏捷性」的前提下提高安全性，必須採用「自動化」安全測試 (DevSecOps)。1. 原始碼分析 (SAST) 能在建置前發現程式碼漏洞。2. 漏洞掃描器 (DAST/Container scanning) 能在部署前檢查成品或容器的已知漏洞。人工審查 (選項 1) 會嚴重拖慢發布速度，違背題目首要目標。",
      "en": "To improve security without sacrificing 'speed and agility', 'automated' security testing (DevSecOps) is essential. 1. Source Code Analysis (SAST) catches code vulnerabilities before build. 2. Vulnerability Scanners (DAST/Container scanning) check artifacts for known issues before deployment. Manual reviews (Option 1) would severely slow down releases, contradicting the primary business objective.",
      "wg": [
        { "t": "犧牲", "en": "sacrificing", "ps": "verb" },
        { "t": "自動化", "en": "automated", "ps": "adjective" }
      ]
    }
  },
  {
    "no": "43",
    "level": "Hard",
    "keywords": "Compute Engine, MIG, Health Check, Troubleshooting, SSH",
    "question": [
      {
        "t": "你的 Compute Engine 託管執行個體群組 (MIG) 發生中斷：所有執行個體在 5 秒後不斷重新啟動。",
        "en": "You have an outage in your Compute Engine managed instance group: all instance keep restarting after 5 seconds.",
        "wg": [
          { "t": "中斷", "en": "outage", "ps": "noun" },
          { "t": "重新啟動", "en": "restarting", "ps": "verb" }
        ]
      },
      {
        "t": "你有設定健康狀態檢查 (Health Check)。你的同事想要登入 VM 進行疑難排解。你需要確保他能存取 VM。你應該做什麼？",
        "en": "You have a health check configured... Your colleague... offered to look into the issue. You need to make sure that he can access the VMs. What should you do?",
        "wg": [
          { "t": "健康狀態檢查", "en": "Health Check", "ps": "feature" },
          { "t": "疑難排解", "en": "troubleshoot", "ps": "verb" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "授與同事 Project Viewer 角色。",
        "en": "Grant your colleague the IAM role of project Viewer",
        "wg": []
      },
      {
        "t": "執行滾動重啟。",
        "en": "Perform a rolling restart on the instance group",
        "wg": []
      },
      {
        "t": "停用執行個體群組的健康狀態檢查。將他的 SSH 金鑰新增至全專案 SSH 金鑰。",
        "en": "Disable the health check for the instance group. Add his SSH key to the project-wide SSH keys",
        "wg": [
          { "t": "停用", "en": "Disable", "ps": "verb" }
        ]
      },
      {
        "t": "停用自動擴展...",
        "en": "Disable autoscaling for the instance group...",
        "wg": []
      }
    ],
    "answer": "Option 3",
    "why": {
      "t": "VM 不斷重啟的原因通常是「健康狀態檢查失敗」導致 MIG 的「自動修復 (Auto-healing)」功能判定 VM 不健康而將其重建。因為重啟循環太快 (5秒)，同事根本來不及 SSH 登入。解決方法是暫時「停用健康狀態檢查」以停止自動重啟，並確保 SSH 金鑰正確設定，讓同事能登入檢查日誌。",
      "en": "The constant restart loop is typically caused by 'Health Check failures' triggering the MIG's 'Auto-healing' feature, which recreates the VM. Because the cycle is too fast (5s), there's no time to SSH in. The solution is to temporarily 'disable the health check' to stop auto-healing, and ensure SSH keys are added so the colleague can log in and inspect logs.",
      "wg": [
        { "t": "自動修復", "en": "Auto-healing", "ps": "feature" },
        { "t": "循環", "en": "cycle", "ps": "noun" }
      ]
    }
  },
  {
    "no": "44",
    "level": "Medium",
    "keywords": "Cost Management, Startup, Discounts, Free Tier",
    "question": [
      {
        "t": "你正在分析並定義支援新創公司試用 GCP 的業務流程，目前還不知道產品的消費者需求。",
        "en": "You are analyzing and defining business processes to support your startup's trial usage of GCP, and you don't yet know what consumer demand for your product will be.",
        "wg": [
          { "t": "新創公司", "en": "startup", "ps": "noun" },
          { "t": "試用", "en": "trial usage", "ps": "noun" }
        ]
      },
      {
        "t": "你的經理要求你將 GCP 服務成本降到最低，並遵守 Google 最佳實踐。你應該做什麼？",
        "en": "Your manager requires you to minimize GCP service costs and adhere to Google best practices. What should you do?",
        "wg": [
          { "t": "最佳實踐", "en": "best practices", "ps": "noun" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "利用免費層級和持續使用折扣 (Sustained Use Discounts)。提供職位負責成本管理。",
        "en": "Utilize free tier and sustained use discounts. Provision a staff position for service cost management.",
        "wg": []
      },
      {
        "t": "利用免費層級和持續使用折扣。提供團隊關於服務成本管理的教育訓練。",
        "en": "Utilize free tier and sustained use discounts. Provide training to the team about service cost management.",
        "wg": [
          { "t": "持續使用折扣", "en": "sustained use discounts", "ps": "feature" },
          { "t": "教育訓練", "en": "training", "ps": "noun" }
        ]
      },
      {
        "t": "利用免費層級和承諾使用折扣 (Committed Use Discounts)。提供職位...",
        "en": "Utilize free tier and committed use discounts. Provision a staff position...",
        "wg": []
      },
      {
        "t": "利用免費層級和承諾使用折扣。提供團隊教育訓練...",
        "en": "Utilize free tier and committed use discounts. Provide training...",
        "wg": []
      }
    ],
    "answer": "Option 2",
    "why": {
      "t": "由於需求未知 (Unknown demand)，不適合使用需綁約 1-3 年的「承諾使用折扣 (Committed Use)」。GCP 的「持續使用折扣 (Sustained Use)」是自動套用的，適合不確定的工作負載。對於新創公司，聘請專職人員管理成本太昂貴，根據 DevOps 文化，提供團隊「教育訓練」讓工程師具備成本意識是更佳且低成本的做法。",
      "en": "Since demand is 'unknown', 'Committed Use Discounts' (which require 1-3 year contracts) are risky. GCP's 'Sustained Use Discounts' apply automatically and are better for uncertain workloads. For a startup, hiring a dedicated staff for cost management is expensive; providing 'training' to the team aligns with DevOps culture and is a more cost-effective best practice.",
      "wg": [
        { "t": "綁約", "en": "contracts", "ps": "noun" },
        { "t": "成本意識", "en": "cost-aware", "ps": "adjective" }
      ]
    }
  },
  {
    "no": "45",
    "level": "Medium",
    "keywords": "IAM, Organization Structure, Folders, Policy Management",
    "question": [
      {
        "t": "你的組織想要獨立但集中地控制不同部門的 IAM 政策。",
        "en": "Your organization wants to control IAM policies for different departments independently, but centrally.",
        "wg": [
          { "t": "獨立但集中地", "en": "independently, but centrally", "ps": "adverb phrase" },
          { "t": "IAM 政策", "en": "IAM policies", "ps": "noun" }
        ]
      },
      {
        "t": "你應該採取哪種方法？",
        "en": "Which approach should you take?",
        "wg": []
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "多個組織搭配多個資料夾",
        "en": "Multiple Organizations with multiple Folders",
        "wg": []
      },
      {
        "t": "多個組織，每個部門一個",
        "en": "Multiple Organizations, one for each department",
        "wg": []
      },
      {
        "t": "單一組織，每個部門一個資料夾",
        "en": "A single Organization with Folders for each department",
        "wg": [
          { "t": "單一組織", "en": "Single Organization", "ps": "noun" },
          { "t": "資料夾", "en": "Folders", "ps": "noun" }
        ]
      },
      {
        "t": "單一組織，多個專案...",
        "en": "A single Organization with multiple projects, each with a central owner",
        "wg": []
      }
    ],
    "answer": "Option 3",
    "why": {
      "t": "Google Cloud 的資源階層最佳實踐是使用「單一組織 (Single Organization)」來進行集中管理，並使用「資料夾 (Folders)」來對映部門結構。這允許你在資料夾層級設定獨立的 IAM 政策 (滿足部門獨立性)，同時組織管理員仍保有上層控制權 (滿足集中管理)。多個組織會造成管理破碎化。",
      "en": "The best practice for Google Cloud resource hierarchy is using a 'Single Organization' for centralized management, and 'Folders' to map to the departmental structure. This allows setting independent IAM policies at the Folder level (meeting departmental independence) while Organization Admins retain top-level control (meeting centralization). Multiple Organizations create fragmentation.",
      "wg": [
        { "t": "資源階層", "en": "resource hierarchy", "ps": "noun" },
        { "t": "破碎化", "en": "fragmentation", "ps": "noun" }
      ]
    }
  },
  {
    "no": "46",
    "level": "Medium",
    "keywords": "VPC, Firewall Rules, Network Tags, N-tier",
    "question": [
      {
        "t": "你的組織在 GCP 上的同一個網路中部署了 3 層式網路應用程式。流量應從 Web -> API -> Database 流動。不應允許 Web 直接存取 Database。",
        "en": "Your organization has a 3-tier web application deployed in the same network on Google Cloud Platform... Traffic should flow through the web to the API tier and then on to the database tier... Traffic should not flow between the web and the database tier.",
        "wg": [
          { "t": "3 層式", "en": "3-tier", "ps": "adjective" }
        ]
      },
      {
        "t": "你應該如何設定網路？",
        "en": "How should you configure the network?",
        "wg": []
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "將每一層加入不同的子網路。",
        "en": "Add each tier to a different subnetwork",
        "wg": []
      },
      {
        "t": "設定軟體防火牆...",
        "en": "Set up software based firewalls on individual VMs",
        "wg": []
      },
      {
        "t": "使用網路標記與路由...",
        "en": "Add network tags to each tier and set up routes to allow the desired traffic flow",
        "wg": []
      },
      {
        "t": "使用網路標記與防火牆規則...",
        "en": "Add network tags to each tier and set up firewall rules to allow the desired traffic flow",
        "wg": [
          { "t": "網路標記", "en": "network tags", "ps": "noun" },
          { "t": "防火牆規則", "en": "firewall rules", "ps": "noun" }
        ]
      }
    ],
    "answer": "Option 4",
    "why": {
      "t": "在 VPC 內控制流量的標準方式是使用「防火牆規則 (Firewall Rules)」。透過「網路標記 (Network Tags)」，你可以定義抽象的群組 (如 'web', 'api') 並撰寫規則 (如 'Allow from tag:web to tag:api')。路由 (Routes) 是用於改變路徑 (如 VPN, NAT)，而非用於細粒度的存取控制。",
      "en": "The standard way to control traffic within a VPC is using 'Firewall Rules'. With 'Network Tags', you can define abstract groups (e.g., 'web', 'api') and write rules (e.g., 'Allow from tag:web to tag:api'). Routes are for path manipulation (VPN, NAT), not for fine-grained access control.",
      "wg": [
        { "t": "細粒度", "en": "fine-grained", "ps": "adjective" }
      ]
    }
  },
  {
    "no": "47",
    "level": "Medium",
    "keywords": "BigQuery, GCS, Logging, Analytics, Archive",
    "question": [
      {
        "t": "你的公司想要低風險地試用雲端。他們想要封存約 100 TB 的日誌資料，測試分析功能，並保留作為長期災難復原備份。",
        "en": "Your company wants to try out the cloud with low risk. They want to archive approximately 100 TB of their log data... test the analytics features... while also retaining that data as a long-term disaster recovery backup.",
        "wg": [
          { "t": "封存", "en": "archive", "ps": "verb" },
          { "t": "分析功能", "en": "analytics features", "ps": "noun" }
        ]
      },
      {
        "t": "你應該採取哪兩個步驟？(選擇 2 項)",
        "en": "Which two steps should you take? Choose 2 answers.",
        "wg": []
      }
    ],
    "type": "多選題",
    "options": [
      {
        "t": "將日誌載入 Google BigQuery。",
        "en": "Load logs into Google BigQuery",
        "wg": [
          { "t": "BigQuery", "en": "BigQuery", "ps": "product" }
        ]
      },
      {
        "t": "將日誌載入 Cloud SQL。",
        "en": "Load logs into Google Cloud SQL",
        "wg": []
      },
      {
        "t": "匯入 Operations Suite...",
        "en": "Import logs into Google Cloud Operations Suite",
        "wg": []
      },
      {
        "t": "插入 Bigtable...",
        "en": "Insert logs into Google Cloud Bigtable",
        "wg": []
      },
      {
        "t": "將日誌檔案上傳到 Google Cloud Storage。",
        "en": "Upload log files into Google Cloud Storage",
        "wg": [
          { "t": "Cloud Storage", "en": "Cloud Storage", "ps": "product" }
        ]
      }
    ],
    "answer": "Option 1, Option 5",
    "why": {
      "t": "1. 針對「分析功能」，BigQuery 是最佳選擇，它能快速查詢 PB 級日誌。2. 針對「長期災難復原備份」與「封存」，Cloud Storage (GCS) 是最便宜且耐久的儲存方案。這組合能滿足分析與低成本備份的雙重需求。",
      "en": "1. For 'analytics features', BigQuery is the best choice, capable of querying PB-scale logs quickly. 2. For 'long-term disaster recovery backup' and 'archiving', Cloud Storage (GCS) is the most cost-effective and durable solution. This combination meets both analytics and low-cost backup requirements.",
      "wg": [
        { "t": "雙重需求", "en": "dual requirements", "ps": "noun phrase" }
      ]
    }
  },
  {
    "no": "48",
    "level": "Medium",
    "keywords": "Deployment Pipeline, Self-healing, Rollback, Incident Response",
    "question": [
      {
        "t": "你建立了一個管道，將程式碼變更部署到自我修復的執行個體群組。其中一個變更對 KPI 產生負面影響。你不確定如何修復，調查可能需要一週。",
        "en": "You created a pipeline that can deploy your source code changes to your infrastructure in instance groups for self-healing. One of the changes negatively affects your key performance indicator. You are not sure how to fix it, and investigation could take up to a week.",
        "wg": [
          { "t": "自我修復", "en": "self-healing", "ps": "adjective" },
          { "t": "負面影響", "en": "negatively affects", "ps": "verb phrase" }
        ]
      },
      {
        "t": "你應該做什麼？",
        "en": "What should you do?",
        "wg": []
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "登入伺服器並在地端迭代修復。",
        "en": "Log in to a server, and iterate on the fox locally",
        "wg": []
      },
      {
        "t": "還原原始碼變更，並重新執行部署管道。",
        "en": "Revert the source code change, and rerun the deployment pipeline",
        "wg": [
          { "t": "還原", "en": "Revert", "ps": "verb" },
          { "t": "部署管道", "en": "deployment pipeline", "ps": "noun" }
        ]
      },
      {
        "t": "登入伺服器換回舊程式碼...",
        "en": "Log into the servers with the bad code change, and swap in the previous code",
        "wg": []
      },
      {
        "t": "更改範本並刪除所有實例...",
        "en": "Change the instance group template to the previous one, and delete all instances",
        "wg": []
      }
    ],
    "answer": "Option 2",
    "why": {
      "t": "在 DevOps 與 CI/CD 最佳實踐中，當生產環境出現問題且無法立即修復時，標準程序是「快速失敗並復原 (Fail fast and rollback)」。透過 Git Revert 原始碼並觸發自動化管道重新部署舊版本，是唯一能確保「基礎設施即程式碼 (IaC)」一致性且可稽核的方法。手動登入伺服器 (選項 1 & 3) 會造成配置漂移 (Configuration Drift)。",
      "en": "In DevOps and CI/CD best practices, when a production issue occurs and cannot be immediately fixed, the standard procedure is 'Fail fast and rollback'. Using Git Revert on the source code to trigger the automated pipeline to redeploy the old version is the only way to ensure 'Infrastructure as Code (IaC)' consistency and auditability. Manually logging into servers (Options 1 & 3) causes Configuration Drift.",
      "wg": [
        { "t": "配置漂移", "en": "Configuration Drift", "ps": "term" },
        { "t": "可稽核", "en": "auditability", "ps": "noun" }
      ]
    }
  },
  {
    "no": "49",
    "level": "Medium",
    "keywords": "Compute Engine, Preemptible VMs, Shutdown Script, Metadata",
    "question": [
      {
        "t": "你建立了數個先佔式 (Pre-emptible) Linux VM。你想要在 VM 被搶佔之前正確地關閉應用程式。",
        "en": "You have created several pre-emptible Linux virtual machine instances... You want to properly shut down your application before the virtual machines are preempted.",
        "wg": [
          { "t": "先佔式", "en": "Pre-emptible", "ps": "adjective" },
          { "t": "搶佔", "en": "preempted", "ps": "verb" }
        ]
      },
      {
        "t": "你應該做什麼？",
        "en": "What should you do?",
        "wg": []
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "在 /etc/rc.6.d/ 建立關機腳本...",
        "en": "Create a shutdown script named k99.shutdown in the /etc/rc.6.d/ directory",
        "wg": []
      },
      {
        "t": "註冊 xinetd 服務...",
        "en": "Create a shutdown script registered as a xinetd service...",
        "wg": []
      },
      {
        "t": "建立關機腳本，並在建立新 VM 時將其用作鍵值為 'shutdown-script' 的中繼資料項目。",
        "en": "Create a shutdown script and use it as the value for a new metadata entry with the key shutdown-script in the Cloud Platform Console when you create the new virtual machine instance",
        "wg": [
          { "t": "關機腳本", "en": "shutdown script", "ps": "feature" },
          { "t": "中繼資料", "en": "metadata", "ps": "noun" }
        ]
      },
      {
        "t": "使用 gcloud 指定服務 URL...",
        "en": "Create a shutdown script... specify the service URL...",
        "wg": []
      }
    ],
    "answer": "Option 3",
    "why": {
      "t": "Compute Engine 提供了一種專門機制來處理 VM 關閉或搶佔：啟動與關機腳本 (Startup/Shutdown Scripts)。只要在 VM 中繼資料 (Metadata) 中設定鍵值 `shutdown-script`，Google 提供的 Guest Environment 就會在收到 ACPI 關機訊號 (搶佔發生時) 自動執行該腳本，給你 30 秒時間進行優雅關機。這比配置 Linux 系統層級的 rc.d (選項 1) 更雲端原生且易於管理。",
      "en": "Compute Engine provides a specific mechanism to handle VM shutdown or preemption: Startup/Shutdown Scripts. By setting the metadata key `shutdown-script`, the Google-provided Guest Environment automatically executes the script upon receiving the ACPI shutdown signal (when preemption occurs), giving you 30 seconds for graceful shutdown. This is more cloud-native and easier to manage than configuring Linux system-level rc.d (Option 1).",
      "wg": [
        { "t": "雲端原生", "en": "cloud-native", "ps": "adjective" },
        { "t": "優雅關機", "en": "graceful shutdown", "ps": "noun phrase" }
      ]
    }
  },
  {
    "no": "50",
    "level": "Hard",
    "keywords": "Compute Engine, Local SSD, Backup, Performance, MySQL",
    "question": [
      {
        "t": "你的公司在單一 MySQL 實例上執行數個資料庫。他們需要定期備份特定資料庫。備份活動需要盡快完成，且不能影響磁碟效能。",
        "en": "Your company runs several databases on a single MySQL instance... take backups of a specific database at regular intervals. The backup activity needs to complete as quickly as possible and cannot be allowed to impact disk performance.",
        "wg": [
          { "t": "影響", "en": "impact", "ps": "verb" },
          { "t": "磁碟效能", "en": "disk performance", "ps": "noun" }
        ]
      },
      {
        "t": "你應該如何設定儲存？",
        "en": "How should you configure the storage?",
        "wg": []
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "使用 gcloud 進行永久磁碟快照。",
        "en": "Configure a cron job to use the gcloud tool to take regular backups using persistent disk snapshots.",
        "wg": []
      },
      {
        "t": "掛載 Local SSD 磁碟區作為備份位置。備份完成後，使用 gsutil 將備份移至 Cloud Storage。",
        "en": "Mount a Local SSD volume as the backup location. After the backup is complete, use gsutil to move the backup to Google Cloud Storage.",
        "wg": [
          { "t": "Local SSD", "en": "Local SSD", "ps": "storage type" }
        ]
      },
      {
        "t": "使用 gcsfuse 掛載 GCS 儲存桶...",
        "en": "Use gcsfuse to mount a Google Cloud Storage bucket...",
        "wg": []
      },
      {
        "t": "掛載額外的永久磁碟並組 RAID10...",
        "en": "Mount additional persistent disk volumes... RAID10...",
        "wg": []
      }
    ],
    "answer": "Option 2",
    "why": {
      "t": "關鍵需求是「盡快完成」與「不影響 (生產環境) 磁碟效能」。Local SSD 提供極高的 IOPS 和吞吐量，且獨立於永久磁碟 (PD) 的頻寬。將備份寫入 Local SSD 可以避免佔用 PD 的 IOPS (不會影響資料庫運作)。PD 快照 (選項 1) 雖然方便但可能影響效能；gcsfuse (選項 3) 寫入效能較差；RAID10 (選項 4) 增加複雜度且仍使用 PD 頻寬。",
      "en": "Key requirements are 'complete as quickly as possible' and 'not impact (production) disk performance'. Local SSDs offer extremely high IOPS and throughput, independent of Persistent Disk (PD) bandwidth. Writing backups to Local SSD avoids consuming PD IOPS (won't impact DB operations). PD snapshots (Option 1) can impact performance; gcsfuse (Option 3) has poor write performance; RAID10 (Option 4) adds complexity and still uses PD bandwidth.",
      "wg": [
        { "t": "頻寬", "en": "bandwidth", "ps": "noun" }
      ]
    }
  }
]