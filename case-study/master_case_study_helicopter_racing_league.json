[
  {
    "no": "1",
    "level": "hard",
    "keywords": "API Management, Partner Integration, Apigee",
    "question": [
      {
        "t": "Helicopter Racing League (HRL) 計畫向其全球合作夥伴開放預測模型，以增加品牌曝光度並創造新的獲利機會。根據業務需求，該解決方案必須能夠有效管理合作夥伴的存取權限、提供詳細的使用分析報告，同時確保後端系統的安全性，並將運維團隊的手動配置負擔降至最低。考量到現有的技術環境與對未來擴展的需求，您應該推薦哪種架構？",
        "en": "Helicopter Racing League (HRL) plans to expose its predictive models to global partners to increase brand visibility and create new monetization opportunities. According to business requirements, the solution must effectively manage partner access, provide detailed usage analytics, and ensure backend security while minimizing the manual configuration burden on the operations team. Considering the existing environment and future scaling needs, which architecture should you recommend?",
        "wg": [
          { "t": "合作夥伴", "en": "partners", "ps": "n" },
          { "t": "獲利機會", "en": "monetization opportunities", "ps": "n" },
          { "t": "手動配置", "en": "manual configuration", "ps": "n" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "(A) 在 Compute Engine 實例上部署自管型的 NGINX 反向代理伺服器，並使用 Cloud IAM 服務帳號為每個合作夥伴手動分配權限，透過監控日誌來產出分析報告。",
        "en": "(A) Deploy self-managed NGINX reverse proxy servers on Compute Engine instances, manually assign permissions to each partner using Cloud IAM service accounts, and generate analytics reports by monitoring logs.",
        "wg": []
      },
      {
        "t": "(B) 實施 Apigee API 管理平台，將預測模型包裝為 API 產品，並利用其內建的開發者門戶（Developer Portal）與分析引擎來管理合作夥伴的接入與配額限制。",
        "en": "(B) Implement the Apigee API Management platform to wrap predictive models as API products, leveraging its built-in Developer Portal and analytics engine to manage partner onboarding and quota limits.",
        "wg": [
          { "t": "API 管理平台", "en": "API Management platform", "ps": "n" },
          { "t": "開發者門戶", "en": "Developer Portal", "ps": "n" }
        ]
      },
      {
        "t": "(C) 使用 Cloud Endpoints 並透過 Firebase Authentication 進行身分驗證，要求合作夥伴直接與 GKE 上的 TensorFlow 容器通訊，並利用 Cloud Monitoring 建立自定義儀表板。",
        "en": "(C) Use Cloud Endpoints with Firebase Authentication for identity verification, requiring partners to communicate directly with TensorFlow containers on GKE, and use Cloud Monitoring to create custom dashboards.",
        "wg": []
      },
      {
        "t": "(D) 建立一組全域外部 HTTP(S) 負載平衡器，並在後端配置 Cloud Run 服務來封裝模型，僅透過 Cloud Armor 的 IP 白名單功能來過濾合作夥伴的請求。",
        "en": "(D) Establish a Global External HTTP(S) Load Balancer and configure Cloud Run services at the backend to encapsulate models, filtering partner requests solely through Cloud Armor IP whitelisting.",
        "wg": []
      }
    ],
    "answer": "(B)",
    "why": {
      "t": "Apigee 是專為企業級 API 管理設計的，能完全滿足 HRL 對於合作夥伴管理、詳細分析報告與安全性（如速率限制、OAuth）的要求，且相較於自管型或基礎工具，它能大幅減少營運複雜性。選項 (A) 手動負擔過重；選項 (C) 雖然可行但缺乏完善的開發者生命週期管理；選項 (D) 則缺乏 API 層級的業務洞察。",
      "en": "Apigee is specifically designed for enterprise-grade API management, fully meeting HRL's requirements for partner management, detailed analytics, and security (e.g., rate limiting, OAuth) while significantly reducing operational complexity compared to self-managed or basic tools. Option (A) has excessive manual overhead; Option (C) lacks full developer lifecycle management; Option (D) lacks business insights at the API level.",
      "wg": [
        { "t": "營運複雜性", "en": "operational complexity", "ps": "n" }
      ]
    }
  },
  {
    "no": "2",
    "level": "hard",
    "keywords": "Media Content Delivery, Latency, Media CDN",
    "question": [
      {
        "t": "HRL 的執行長 S. Hawke 目標是將「高腎上腺素」的競賽體驗帶給全球觀眾，特別是新興市場的粉絲。技術需求明確指出需要減少觀眾延遲並提高同時觀看的人數。目前，HRL 將影片內容存儲在物件儲存服務中，並由賽道的移動數據中心進行初步處理。為了在維持全球可用性的同時提供最高品質的即時與錄製串流內容，哪種 Google Cloud 解決方案最能優化分發路徑？",
        "en": "HRL CEO S. Hawke aims to bring a 'high-adrenaline' racing experience to a global audience, especially fans in emerging markets. Technical requirements explicitly state the need to reduce viewer latency and increase the number of concurrent viewers. Currently, HRL stores video content in object storage and performs initial processing at trackside mobile data centers. To optimize the delivery path for high-quality real-time and recorded streams while maintaining global availability, which Google Cloud solution is most suitable?",
        "wg": [
          { "t": "新興市場", "en": "emerging markets", "ps": "n" },
          { "t": "觀眾延遲", "en": "viewer latency", "ps": "n" },
          { "t": "同時觀看", "en": "concurrent viewers", "ps": "n" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "(A) 將 Cloud Storage 存儲桶設置為多區域（Multi-regional），並要求全球觀眾透過簽署 URL (Signed URLs) 直接從存儲桶下載影片數據，以簡化架構並降低管理成本。",
        "en": "(A) Set the Cloud Storage buckets to Multi-regional and require global viewers to download video data directly from the buckets via Signed URLs to simplify the architecture and reduce management costs.",
        "wg": []
      },
      {
        "t": "(B) 部署全域外部 HTTP(S) 負載平衡器，並在各個目標地理區域（如新興市場）手動部署 GKE 節點池來快取內容，利用 Cloud DNS 的地理分流功能導流。",
        "en": "(B) Deploy a Global External HTTP(S) Load Balancer and manually deploy GKE node pools in target geographic regions (e.g., emerging markets) to cache content, using Cloud DNS geolocation routing.",
        "wg": []
      },
      {
        "t": "(C) 建立一個連接到 Cloud Storage 的 Cloud CDN 實例，並啟用動態內容壓縮與預讀功能，依靠 Google 的邊緣端點（PoPs）處理所有靜態與動態影片流量。",
        "en": "(C) Create a Cloud CDN instance connected to Cloud Storage, enabling dynamic content compression and prefetching, relying on Google's Points of Presence (PoPs) to handle all static and dynamic video traffic.",
        "wg": []
      },
      {
        "t": "(D) 使用 Media CDN，利用 Google 遍布全球的邊緣快取基礎架構，針對影片串流協議（如 HLS 或 DASH）進行優化，並配合 Cloud Armor 進行邊緣安全性防護。",
        "en": "(D) Use Media CDN, leveraging Google's global edge cache infrastructure optimized for video streaming protocols (like HLS or DASH), in conjunction with Cloud Armor for edge security protection.",
        "wg": [
          { "t": "邊緣快取", "en": "edge cache", "ps": "n" },
          { "t": "串流協議", "en": "streaming protocols", "ps": "n" }
        ]
      }
    ],
    "answer": "(D)",
    "why": {
      "t": "對於影片串流業務（特別是高畫質與大規模併發），Media CDN 是 Google Cloud 的最優選，因為它是專門為影音分發設計的，利用了 Google 傳輸 YouTube 流量的相同網路，能顯著降低延遲並支持極高數量的小型併發請求。選項 (C) 雖然可用，但 Media CDN 對於影片串流的專用優化更強；選項 (A) 和 (B) 則無法有效處理大規模併發與地理分散的性能問題。",
      "en": "For video streaming businesses (especially high-definition and large-scale concurrency), Media CDN is the optimal Google Cloud choice because it is specifically designed for video delivery, utilizing the same network Google uses for YouTube traffic to significantly reduce latency and support high concurrency. Option (C) is viable, but Media CDN offers stronger specialized optimizations for video; Options (A) and (B) fail to address performance issues for large-scale, geographically dispersed audiences.",
      "wg": [
        { "t": "影音分發", "en": "video delivery", "ps": "n" }
      ]
    }
  },
  {
    "no": "3",
    "level": "hard",
    "keywords": "Data Ingestion, Real-time Analytics, Dataflow, BigQuery",
    "question": [
      { "t": "HRL 團隊需要在每場比賽期間從移動數據中心收集大量即時遙測數據，並即時產出洞察，例如選手超車的可能性。目前預測模型僅能在賽後處理數據。技術需求要求建立一個能夠處理海量競賽數據的資料集市（Data Mart），並支持實時分析觀眾的消費模式。為了最小化運作複雜性並提供即時分析能力，您應該設計哪種數據流水線？", "en": "The HRL team needs to collect massive real-time telemetry data from mobile data centers during each race and generate instant insights, such as the probability of overtaking. Currently, predictive models can only process data post-race. Technical requirements call for creating a Data Mart capable of processing large volumes of race data and supporting real-time analytics of viewer consumption patterns. To minimize operational complexity and provide real-time analytics, which data pipeline should you design?", "wg": [ { "t": "遙測數據", "en": "telemetry data", "ps": "n" }, { "t": "資料集市", "en": "Data Mart", "ps": "n" }, { "t": "數據流水線", "en": "data pipeline", "ps": "n" } ] }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "(A) 使用 Cloud Pub/Sub 接收即時遙測數據，透過 Cloud Dataflow 進行串流處理與清洗，並將結果寫入 BigQuery 以進行即時查詢與全賽季趨勢分析。",
        "en": "(A) Use Cloud Pub/Sub to receive real-time telemetry data, Cloud Dataflow for stream processing and cleaning, and write results to BigQuery for real-time querying and season-long trend analysis.",
        "wg": [
          { "t": "串流處理", "en": "stream processing", "ps": "n" },
          { "t": "趨勢分析", "en": "trend analysis", "ps": "n" }
        ]
      },
      {
        "t": "(B) 在 Compute Engine 上安裝 Apache Kafka 叢集來緩衝數據，並定期啟動 Cloud Dataproc 工作將數據轉換為 Parquet 格式存儲在 Cloud Storage 中以供後續處理。",
        "en": "(B) Install an Apache Kafka cluster on Compute Engine to buffer data, and periodically launch Cloud Dataproc jobs to convert data into Parquet format stored in Cloud Storage for subsequent processing.",
        "wg": []
      },
      {
        "t": "(C) 使用 Cloud Functions 作為輕量級的資料攝取工具，將每條遙測訊息寫入 Firestore 資料庫，並利用 Looker 直接連接 Firestore 進行報表展示。",
        "en": "(C) Use Cloud Functions as a lightweight data ingestion tool, writing each telemetry message into a Firestore database, and utilize Looker to connect directly to Firestore for reporting.",
        "wg": []
      },
      {
        "t": "(D) 將遙測數據直接透過 SFTP 上傳至 Cloud Storage，並在 Cloud Data Fusion 中設定批次作業，每小時將數據同步至 Bigtable 以支持高吞吐量的讀取。",
        "en": "(D) Upload telemetry data directly to Cloud Storage via SFTP and set up batch jobs in Cloud Data Fusion to sync data to Bigtable every hour to support high-throughput reads.",
        "wg": []
      }
    ],
    "answer": "(A)",
    "why": {
      "t": "Pub/Sub + Dataflow + BigQuery 是 Google Cloud 推薦的標準伺服器端（Serverless）即時分析架構。它能自動縮放以應對賽事期間的數據峰值，完全滿足「極小化運作複雜性」與「即時分析」的要求。選項 (B) 涉及繁瑣的自管叢集管理；選項 (C) 無法處理海量數據的高性能分析；選項 (D) 則是批次處理，無法提供即時洞察。",
      "en": "Pub/Sub + Dataflow + BigQuery is the standard Serverless real-time analytics architecture recommended by Google Cloud. It autoscales to handle data spikes during races, meeting requirements for 'minimizing operational complexity' and 'real-time analytics.' Option (B) involves tedious self-managed cluster management; Option (C) cannot handle high-performance analysis for massive datasets; Option (D) is batch processing and cannot provide real-time insights.",
      "wg": [
        { "t": "伺服器端", "en": "Serverless", "ps": "adj" }
      ]
    }
  },
  {
    "no": "4",
    "level": "hard",
    "keywords": "Video Transcoding, Managed Services, Transcoder API",
    "question": [
      { "t": "HRL 目前的影片編碼與轉碼是在為每個作業建立的虛擬機上執行的。隨著全球業務擴展與觀眾對畫質要求的提升，技術團隊需要提高轉碼效能，同時希望能降低管理這些虛擬機的行政成本。考量到「極小化運作複雜性」的目標，應推薦哪種方案來升級其影片處理流程？", "en": "HRL's current video encoding and transcoding are performed on VMs created for each job. As global business expands and viewer expectations for video quality rise, the technical team needs to increase transcoding performance and reduce the administrative costs of managing these VMs. Considering the 'minimize operational complexity' goal, which solution should be recommended to upgrade their video processing workflow?", "wg": [ { "t": "編碼與轉碼", "en": "encoding and transcoding", "ps": "n" }, { "t": "行政成本", "en": "administrative costs", "ps": "n" } ] }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "(A) 將現有的轉碼腳本封裝成容器，並在 GKE Autopilot 叢集上運行，利用節點自動撥補與 Spot VM 來降低成本並提高擴展性。",
        "en": "(A) Encapsulate existing transcoding scripts into containers and run them on a GKE Autopilot cluster, leveraging node auto-provisioning and Spot VMs to reduce costs and increase scalability.",
        "wg": []
      },
      {
        "t": "(B) 為 Compute Engine 建立受管理執行個體群組 (MIG)，使用搭載 GPU 的自定義映像檔，並利用負載平衡器根據 CPU 使用率自動增減實例數量。",
        "en": "(B) Create a Managed Instance Group (MIG) for Compute Engine using custom images with GPUs and utilize a load balancer to automatically scale instances based on CPU utilization.",
        "wg": []
      },
      {
        "t": "(C) 使用 Transcoder API，這是一個全託管服務，能將 Cloud Storage 中的源影片轉換為多種格式與碼率，並能與 Dataflow 流水線整合以實現自動化觸發。",
        "en": "(C) Use the Transcoder API, a fully managed service that converts source videos in Cloud Storage into multiple formats and bitrates, and can integrate with Dataflow pipelines for automated triggering.",
        "wg": [
          { "t": "全託管服務", "en": "fully managed service", "ps": "n" },
          { "t": "碼率", "en": "bitrates", "ps": "n" }
        ]
      },
      {
        "t": "(D) 將轉碼邏輯遷移至 Cloud Functions (2nd gen)，利用其支持的更長執行時間，在影片上傳至存儲桶時觸發處理，並將結果寫回另一個區域的存儲桶。",
        "en": "(D) Migrate transcoding logic to Cloud Functions (2nd gen), leveraging its support for longer execution times to trigger processing upon video upload to a bucket, writing the results to a bucket in another region.",
        "wg": []
      }
    ],
    "answer": "(C)",
    "why": {
      "t": "Transcoder API 是 Google Cloud 提供的全託管、專用型影片處理工具。它消除了管理伺服器或容器叢集的需要，能自動根據負載縮放，且對於「提高效能」與「減少維運複雜度」而言是最優選。選項 (A) 和 (B) 仍需一定程度的基礎設施管理；選項 (D) 對於大規模或長時長的轉碼任務可能面臨資源限制或超時風險。",
      "en": "The Transcoder API is a fully managed, specialized video processing tool provided by Google Cloud. It eliminates the need to manage servers or container clusters, scales automatically based on load, and is the optimal choice for 'increasing performance' and 'minimizing operational complexity.' Options (A) and (B) still require some infrastructure management; Option (D) may face resource limits or timeout risks for large-scale or long-duration transcoding tasks.",
      "wg": [
        { "t": "基礎設施", "en": "infrastructure", "ps": "n" }
      ]
    }
  },
  {
    "no": "5",
    "level": "hard",
    "keywords": "Vertex AI, Machine Learning, Real-time Predictions",
    "question": [
      { "t": "HRL 欲在比賽期間向粉絲提供關於機械故障與比賽結果的即時預測（In-race Predictions）。目前的 TensorFlow 模型運行在自管型虛擬機上，不僅無法處理實時流數據，也難以滿足高吞吐量的要求。為了實現「維持或增加預測吞吐量與準確度」並支持「即時分析」的技術要求，架構師應採取哪兩項行動？（請選擇兩項）", "en": "HRL wants to provide in-race predictions regarding mechanical failures and race results to fans during competitions. The current TensorFlow models running on self-managed VMs cannot handle real-time streaming data nor meet high-throughput requirements. To meet the technical requirements of 'maintaining or increasing prediction throughput and accuracy' and supporting 'real-time analytics,' which two actions should the architect take? (Choose two)", "wg": [ { "t": "即時預測", "en": "In-race Predictions", "ps": "n" }, { "t": "高吞吐量", "en": "high-throughput", "ps": "n" } ] }
    ],
    "type": "複選題",
    "options": [
      {
        "t": "(A) 在 Compute Engine 上部署一個具有高性能固態硬碟（Local SSD）的強化型虛擬機叢集，手動配置 TensorFlow Serving 來提升現有模型的讀取速度。",
        "en": "(A) Deploy a cluster of hardened VMs with Local SSDs on Compute Engine and manually configure TensorFlow Serving to improve the read speeds of existing models.",
        "wg": []
      },
      {
        "t": "(B) 將 TensorFlow 模型遷移至 Vertex AI Prediction (Online)，並配置自動調整規模功能，以支持賽事高峰期間的高併發預測請求。",
        "en": "(B) Migrate TensorFlow models to Vertex AI Prediction (Online) and configure autoscaling to support high-concurrency prediction requests during peak race times.",
        "wg": [
          { "t": "自動調整規模", "en": "autoscaling", "ps": "n" },
          { "t": "併發", "en": "concurrency", "ps": "n" }
        ]
      },
      {
        "t": "(C) 使用 Cloud Dataproc 將地端的 TensorFlow 作業轉換為 Spark MLlib 任務，以便利用分散式運算優化賽後的全賽季趨勢分析效能。",
        "en": "(C) Use Cloud Dataproc to convert on-premises TensorFlow jobs into Spark MLlib tasks to leverage distributed computing for optimizing post-race season-long trend analysis performance.",
        "wg": []
      },
      {
        "t": "(D) 利用 Vertex AI Feature Store 來集中管理與共享遙測數據中的特徵，確保即時預測與批次訓練時使用一致且低延遲的數據。",
        "en": "(D) Utilize Vertex AI Feature Store to centrally manage and share features from telemetry data, ensuring consistent and low-latency data for both real-time predictions and batch training.",
        "wg": [
          { "t": "特徵商店", "en": "Feature Store", "ps": "n" },
          { "t": "低延遲", "en": "low-latency", "ps": "adj" }
        ]
      },
      {
        "t": "(E) 實施 Vertex AI Pipelines 來自動化處理 TensorFlow 模型的批次訓練與部署，並將生成的靜態文件存儲在 Cloud Storage 以便隨時查閱。",
        "en": "(E) Implement Vertex AI Pipelines to automate the batch training and deployment of TensorFlow models, storing the generated static files in Cloud Storage for easy reference.",
        "wg": []
      }
    ],
    "answer": "(B), (D)",
    "why": {
      "t": "Vertex AI Prediction (Online) 是實現即時、高吞吐量模型服務的最佳工具，能自動縮放以應對流量波動 (B)。同時，Vertex AI Feature Store 能解決即時預測中關鍵的『特徵一致性』問題，確保模型能夠快速獲取最新的遙測數據特徵 (D)。選項 (A) 管理負擔太重；選項 (C) 偏離了 TensorFlow 技術棧；選項 (E) 雖有助於自動化運維，但對於『即時預測效能』的直接改善不如 B 和 D。",
      "en": "Vertex AI Prediction (Online) is the best tool for real-time, high-throughput model serving, capable of autoscaling to handle traffic fluctuations (B). Simultaneously, Vertex AI Feature Store addresses the critical 'feature consistency' issue in real-time predictions, ensuring models can quickly access the latest telemetry features (D). Option (A) has too much management overhead; Option (C) diverges from the TensorFlow stack; Option (E) helps automate operations but offers less direct improvement to 'real-time prediction performance' than B and D.",
      "wg": [
        { "t": "特徵一致性", "en": "feature consistency", "ps": "n" }
      ]
    }
  },
{
 "no": "1",
 "level": "hard",
 "keywords": "Media, Latency, Cloud CDN, Global Distribution",
 "question": [
  {
   "t": "Helicopter Racing League (HRL) 正積極向新興市場擴張，希望能讓當地的粉絲順暢地觀看已錄製的比賽影片內容。",
   "en": "Helicopter Racing League (HRL) is actively expanding into emerging markets and wants to allow fans in those regions to smoothly watch recorded race video content.",
   "wg": [
    {
     "t": "新興市場",
     "en": "emerging markets",
     "ps": "noun"
    },
    {
     "t": "已錄製的比賽影片",
     "en": "recorded race video",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "目前的影片內容儲存在單一區域的 Cloud Storage bucket 中，但新興地區的使用者回報影片載入延遲過高。",
   "en": "Current video content is stored in a Cloud Storage bucket in a single region, but users in emerging regions are reporting excessive video loading latency.",
   "wg": [
    {
     "t": "單一區域",
     "en": "single region",
     "ps": "adjective"
    },
    {
     "t": "載入延遲",
     "en": "loading latency",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "您需要設計一個能顯著降低觀看延遲、提升全球可用性，且能將內容推送到最接近使用者位置的解決方案，同時需盡量降低維運複雜度。",
   "en": "You need to design a solution that significantly reduces viewing latency, enhances global availability, and pushes content as close to users as possible, while minimizing operational complexity.",
   "wg": [
    {
     "t": "全球可用性",
     "en": "global availability",
     "ps": "noun"
    },
    {
     "t": "維運複雜度",
     "en": "operational complexity",
     "ps": "noun"
    }
   ]
  }
 ],
 "type": "單選題",
 "options": [
  {
   "t": "(A) 將 Cloud Storage bucket 遷移至多重區域 (Multi-regional) 類別，並在應用層實作地理位置路由 (Geolocation routing)。",
   "en": "(A) Migrate the Cloud Storage bucket to the Multi-regional class and implement geolocation routing at the application layer.",
   "wg": [
    {
     "t": "多重區域",
     "en": "Multi-regional",
     "ps": "adjective"
    },
    {
     "t": "地理位置路由",
     "en": "Geolocation routing",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(B) 在每個新興市場區域建立 Compute Engine 實例叢集並部署 Nginx 伺服器作為快取代理，手動設定區域負載平衡。",
   "en": "(B) Create Compute Engine instance clusters in each emerging market region and deploy Nginx servers as cache proxies, manually configuring regional load balancing.",
   "wg": [
    {
     "t": "快取代理",
     "en": "cache proxies",
     "ps": "noun"
    },
    {
     "t": "區域負載平衡",
     "en": "regional load balancing",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(C) 設定 Cloud CDN 並將其後端指向現有的 Cloud Storage bucket，啟用快取功能以將內容分發至邊緣節點。",
   "en": "(C) Configure Cloud CDN with the existing Cloud Storage bucket as the backend, enabling caching to distribute content to edge nodes.",
   "wg": [
    {
     "t": "Cloud CDN",
     "en": "Cloud CDN",
     "ps": "product"
    },
    {
     "t": "邊緣節點",
     "en": "edge nodes",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(D) 使用 Storage Transfer Service 將影片內容複製到位於各個新興市場區域的獨立 Cloud Storage buckets 中。",
   "en": "(D) Use Storage Transfer Service to replicate video content into separate Cloud Storage buckets located in each emerging market region.",
   "wg": [
    {
     "t": "複製",
     "en": "replicate",
     "ps": "verb"
    },
    {
     "t": "獨立 Cloud Storage buckets",
     "en": "separate Cloud Storage buckets",
     "ps": "noun"
    }
   ]
  }
 ],
 "answer": "(C)",
 "why": {
  "t": "Cloud CDN 是針對靜態內容（如錄製的影片）進行全球分發與降低延遲的最佳託管服務，符合 HRL 將內容移近使用者的需求且維運成本最低。選項 (A) 多重區域 Bucket 僅涵蓋大洲級別（如全美、全歐），無法像 CDN 一樣深入邊緣網路。選項 (B) 需要管理 VM，違反最小化維運原則。選項 (D) 雖然可行，但需要維護多個 Bucket 的資料一致性與複雜的路由邏輯，不如 CDN 高效。",
  "en": "Cloud CDN is the optimal managed service for global distribution and latency reduction of static content (like recorded video), meeting HRL's need to move content closer to users with minimal operational cost. Option (A) Multi-regional buckets only cover continental levels (e.g., US, EU) and cannot reach edge networks like a CDN. Option (B) requires managing VMs, violating the principle of minimizing operations. Option (D) is feasible but requires maintaining data consistency across multiple buckets and complex routing logic, making it less efficient than a CDN.",
  "wg": [
   {
    "t": "靜態內容",
    "en": "static content",
    "ps": "noun"
   },
   {
    "t": "邊緣網路",
    "en": "edge networks",
    "ps": "noun"
   }
  ]
 }
},
{
 "no": "2",
 "level": "hard",
 "keywords": "AI/ML, Vertex AI, Managed Services, Migration",
 "question": [
  {
   "t": "HRL 決定將現有在 VM 上運行的 TensorFlow 比賽預測模型遷移至全託管的環境，以減少維護底層基礎架構的負擔。",
   "en": "HRL has decided to migrate their existing TensorFlow race prediction models running on VMs to a fully managed environment to reduce the burden of maintaining underlying infrastructure.",
   "wg": [
    {
     "t": "全託管的環境",
     "en": "fully managed environment",
     "ps": "noun"
    },
    {
     "t": "基礎架構",
     "en": "infrastructure",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "新的解決方案必須支援自動擴展以應對比賽期間的高流量，並能無縫整合模型訓練與部署的 pipeline。",
   "en": "The new solution must support autoscaling to handle high traffic during races and seamlessly integrate model training and deployment pipelines.",
   "wg": [
    {
     "t": "自動擴展",
     "en": "autoscaling",
     "ps": "verb"
    },
    {
     "t": "模型訓練",
     "en": "model training",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "此外，HRL 希望利用 Google Cloud 的原生 MLOps 功能來持續監控模型效能。您應該建議採用哪項服務？",
   "en": "Additionally, HRL wants to leverage Google Cloud's native MLOps capabilities to continuously monitor model performance. Which service should you recommend?",
   "wg": [
    {
     "t": "原生 MLOps 功能",
     "en": "native MLOps capabilities",
     "ps": "noun"
    },
    {
     "t": "監控模型效能",
     "en": "monitor model performance",
     "ps": "verb"
    }
   ]
  }
 ],
 "type": "單選題",
 "options": [
  {
   "t": "(A) Vertex AI Prediction，並使用 Vertex AI Pipelines 進行模型的持續訓練與部署。",
   "en": "(A) Vertex AI Prediction, and use Vertex AI Pipelines for continuous training and deployment of models.",
   "wg": [
    {
     "t": "Vertex AI Prediction",
     "en": "Vertex AI Prediction",
     "ps": "product"
    },
    {
     "t": "持續訓練",
     "en": "continuous training",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(B) 將 TensorFlow 模型容器化，並部署到 Google Kubernetes Engine (GKE) Autopilot 叢集上。",
   "en": "(B) Containerize the TensorFlow models and deploy them on a Google Kubernetes Engine (GKE) Autopilot cluster.",
   "wg": [
    {
     "t": "容器化",
     "en": "Containerize",
     "ps": "verb"
    },
    {
     "t": "GKE Autopilot",
     "en": "GKE Autopilot",
     "ps": "product"
    }
   ]
  },
  {
   "t": "(C) 使用 Dataflow 串流處理作業來執行即時預測，並將結果寫入 Bigtable。",
   "en": "(C) Use Dataflow streaming jobs to execute real-time predictions and write the results to Bigtable.",
   "wg": [
    {
     "t": "串流處理作業",
     "en": "streaming jobs",
     "ps": "noun"
    },
    {
     "t": "即時預測",
     "en": "real-time predictions",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(D) 將模型部署到 App Engine Flexible 環境，並使用 Cloud Monitoring 監控延遲。",
   "en": "(D) Deploy the models to the App Engine Flexible environment and use Cloud Monitoring to monitor latency.",
   "wg": [
    {
     "t": "App Engine Flexible",
     "en": "App Engine Flexible",
     "ps": "product"
    },
    {
     "t": "監控延遲",
     "en": "monitor latency",
     "ps": "verb"
    }
   ]
  }
 ],
 "answer": "(A)",
 "why": {
  "t": "Vertex AI 是 Google Cloud 針對機器學習工作負載的整合式託管平台。Vertex AI Prediction 提供無伺服器、自動擴展的模型服務，完美符合 HRL 減少維運與支援高流量的需求。同時，Vertex AI Pipelines 滿足了 MLOps 的整合需求。選項 (B) GKE 雖然可行，但相比 Vertex AI 仍需較多叢集管理工作，且缺乏原生的 MLOps 監控整合。選項 (C) Dataflow 主要用於資料處理而非模型服務 API。選項 (D) App Engine 缺乏針對 ML 模型的優化功能（如 GPU 加速或模型監控）。",
  "en": "Vertex AI is Google Cloud's integrated managed platform for machine learning workloads. Vertex AI Prediction offers serverless, autoscaling model serving, perfectly matching HRL's needs to reduce ops and support high traffic. Meanwhile, Vertex AI Pipelines meets the MLOps integration requirements. Option (B) GKE is feasible but requires more cluster management than Vertex AI and lacks native MLOps monitoring integration. Option (C) Dataflow is primarily for data processing, not model serving APIs. Option (D) App Engine lacks features optimized for ML models (like GPU acceleration or model monitoring).",
  "wg": [
   {
    "t": "整合式託管平台",
    "en": "integrated managed platform",
    "ps": "noun"
   },
   {
    "t": "無伺服器",
    "en": "serverless",
    "ps": "adjective"
   }
  ]
 }
},
{
 "no": "3",
 "level": "hard",
 "keywords": "Data Analytics, BigQuery, Data Mart, Scalability",
 "question": [
  {
   "t": "HRL 需要建立一個新的 Data Mart 來儲存並處理來自全球賽事的巨量遙測數據，以支援長期的賽季結果分析 (Season-long results)。",
   "en": "HRL needs to create a new Data Mart to store and process massive telemetry data from global races to support season-long results analysis.",
   "wg": [
    {
     "t": "Data Mart",
     "en": "Data Mart",
     "ps": "noun"
    },
    {
     "t": "巨量遙測數據",
     "en": "massive telemetry data",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "業務分析師將會頻繁地對這些數據執行複雜的 SQL 查詢以產出洞察報告。目前的解決方案無法有效擴展以應對資料量的增長。",
   "en": "Business analysts will frequently run complex SQL queries on this data to generate insight reports. The current solution cannot scale effectively to handle data growth.",
   "wg": [
    {
     "t": "業務分析師",
     "en": "Business analysts",
     "ps": "noun"
    },
    {
     "t": "複雜的 SQL 查詢",
     "en": "complex SQL queries",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "您需要選擇一個符合全託管 (Serverless)、支援標準 SQL 介面，並能處理 PB 級資料的資料倉儲解決方案。",
   "en": "You need to select a data warehouse solution that is fully managed (Serverless), supports a standard SQL interface, and can handle petabyte-scale data.",
   "wg": [
    {
     "t": "全託管",
     "en": "fully managed",
     "ps": "adjective"
    },
    {
     "t": "資料倉儲",
     "en": "data warehouse",
     "ps": "noun"
    }
   ]
  }
 ],
 "type": "單選題",
 "options": [
  {
   "t": "(A) 使用 Cloud Bigtable 儲存遙測數據，並安裝 HBase shell 用戶端供分析師進行查詢。",
   "en": "(A) Use Cloud Bigtable to store telemetry data and install HBase shell clients for analysts to perform queries.",
   "wg": [
    {
     "t": "Cloud Bigtable",
     "en": "Cloud Bigtable",
     "ps": "product"
    },
    {
     "t": "HBase shell 用戶端",
     "en": "HBase shell clients",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(B) 在 Compute Engine 上建立 MySQL 叢集，並使用唯讀複本 (Read Replicas) 來分擔分析查詢的流量。",
   "en": "(B) Create a MySQL cluster on Compute Engine and use Read Replicas to offload analytics query traffic.",
   "wg": [
    {
     "t": "唯讀複本",
     "en": "Read Replicas",
     "ps": "noun"
    },
    {
     "t": "分擔",
     "en": "offload",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "(C) 使用 Firestore 來儲存賽事數據，因為它支援即時更新並提供靈活的文件查詢語法。",
   "en": "(C) Use Firestore to store race data because it supports real-time updates and provides flexible document query syntax.",
   "wg": [
    {
     "t": "Firestore",
     "en": "Firestore",
     "ps": "product"
    },
    {
     "t": "靈活的文件查詢語法",
     "en": "flexible document query syntax",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(D) 將遙測數據匯入 BigQuery，並使用其內建的分析功能與標準 SQL 介面供分析師使用。",
   "en": "(D) Import telemetry data into BigQuery and use its built-in analytics capabilities and standard SQL interface for analysts.",
   "wg": [
    {
     "t": "BigQuery",
     "en": "BigQuery",
     "ps": "product"
    },
    {
     "t": "標準 SQL 介面",
     "en": "standard SQL interface",
     "ps": "noun"
    }
   ]
  }
 ],
 "answer": "(D)",
 "why": {
  "t": "BigQuery 是 Google Cloud 的全託管、無伺服器企業資料倉儲，專為分析大規模資料（PB 級）而設計，且原生支援標準 SQL，完全符合建立 Data Mart 與業務分析師的需求。選項 (A) Bigtable 雖然適合高吞吐寫入，但不支援標準 SQL 且不適合複雜的分析查詢 (OLAP)。選項 (B) MySQL 難以擴展至處理 PB 級的長期歷史數據分析。選項 (C) Firestore 是 NoSQL document DB，適合行動應用後端，而非大規模資料分析。",
  "en": "BigQuery is Google Cloud's fully managed, serverless enterprise data warehouse, designed for analyzing large-scale data (petabyte-scale) and natively supporting standard SQL, perfectly fitting the needs for building a Data Mart and supporting business analysts. Option (A) Bigtable is suitable for high-throughput writes but does not support standard SQL and is not fit for complex analytical queries (OLAP). Option (B) MySQL struggles to scale for processing PB-level long-term historical data analytics. Option (C) Firestore is a NoSQL document DB suitable for mobile app backends, not large-scale data analytics.",
  "wg": [
   {
    "t": "企業資料倉儲",
    "en": "enterprise data warehouse",
    "ps": "noun"
   },
   {
    "t": "分析查詢",
    "en": "analytical queries",
    "ps": "noun"
   }
  ]
 }
},
{
 "no": "4",
 "level": "hard",
 "keywords": "Transcoding, Compute Engine, Pub/Sub, Cost Optimization",
 "question": [
  {
   "t": "HRL 需要設計一個可擴展的影片轉碼 (Transcoding) 流程。比賽結束後會有大量的原始影片上傳，系統必須能在短時間內處理完畢。",
   "en": "HRL needs to design a scalable video transcoding workflow. A large volume of raw video is uploaded after races, and the system must process it within a short timeframe.",
   "wg": [
    {
     "t": "影片轉碼",
     "en": "video transcoding",
     "ps": "noun"
    },
    {
     "t": "可擴展",
     "en": "scalable",
     "ps": "adjective"
    }
   ]
  },
  {
   "t": "轉碼工作屬於批次處理性質，且允許在處理過程中偶爾中斷重試。您希望能將營運成本降至最低，同時保持自動化。",
   "en": "The transcoding work is batch-oriented and allows for occasional interruptions and retries during processing. You want to minimize operational costs while maintaining automation.",
   "wg": [
    {
     "t": "批次處理",
     "en": "batch-oriented",
     "ps": "adjective"
    },
    {
     "t": "中斷重試",
     "en": "interruptions and retries",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "請選擇兩個步驟來構建此架構。",
   "en": "Choose two steps to build this architecture.",
   "wg": [
    {
     "t": "營運成本",
     "en": "operational costs",
     "ps": "noun"
    },
    {
     "t": "架構",
     "en": "architecture",
     "ps": "noun"
    }
   ]
  }
 ],
 "type": "複選題",
 "options": [
  {
   "t": "(A) 使用 Pub/Sub 接收影片上傳通知，以解耦上傳與處理流程。",
   "en": "(A) Use Pub/Sub to receive video upload notifications to decouple the upload and processing workflows.",
   "wg": [
    {
     "t": "解耦",
     "en": "decouple",
     "ps": "verb"
    },
    {
     "t": "上傳通知",
     "en": "upload notifications",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(B) 在 Compute Engine 上配置擁有 GPU 的專用執行個體 (Sole-tenant nodes) 進行轉碼。",
   "en": "(B) Configure Sole-tenant nodes with GPUs on Compute Engine for transcoding.",
   "wg": [
    {
     "t": "專用執行個體",
     "en": "Sole-tenant nodes",
     "ps": "noun"
    },
    {
     "t": "GPU",
     "en": "GPU",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(C) 使用 App Engine Standard 環境來執行 FFMPEG 轉碼腳本。",
   "en": "(C) Use the App Engine Standard environment to run FFMPEG transcoding scripts.",
   "wg": [
    {
     "t": "App Engine Standard",
     "en": "App Engine Standard",
     "ps": "product"
    },
    {
     "t": "腳本",
     "en": "scripts",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(D) 使用 Google Kubernetes Engine (GKE) 並配置搶占式節點 (Spot VMs) 來執行轉碼 Job。",
   "en": "(D) Use Google Kubernetes Engine (GKE) configured with Spot VMs to execute transcoding Jobs.",
   "wg": [
    {
     "t": "搶占式節點",
     "en": "Spot VMs",
     "ps": "noun"
    },
    {
     "t": "轉碼 Job",
     "en": "transcoding Jobs",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(E) 將處理完成的影片直接存入 Cloud SQL 以供前端應用程式讀取。",
   "en": "(E) Store the processed videos directly into Cloud SQL for frontend applications to read.",
   "wg": [
    {
     "t": "Cloud SQL",
     "en": "Cloud SQL",
     "ps": "product"
    },
    {
     "t": "前端應用程式",
     "en": "frontend applications",
     "ps": "noun"
    }
   ]
  }
 ],
 "answer": "(A), (D)",
 "why": {
  "t": "此題考查成本優化與架構解耦。選項 (A) 使用 Pub/Sub 是標準的非同步解耦模式，能緩衝大量上傳請求。選項 (D) 使用 GKE 搭配 Spot VMs (搶占式實例) 是處理可容錯批次作業最省錢的方式，Spot VM 價格遠低於標準 VM。選項 (B) 專用節點成本極高，不符合成本效益。選項 (C) App Engine Standard 有請求逾時限制，不適合長時間的轉碼作業。選項 (E) 資料庫不適合儲存大型二進位檔案 (BLOBs)，應存於 Cloud Storage。",
  "en": "This question tests cost optimization and architectural decoupling. Option (A) using Pub/Sub is a standard asynchronous decoupling pattern that buffers massive upload requests. Option (D) using GKE with Spot VMs (preemptible instances) is the most cost-effective way to handle fault-tolerant batch jobs, as Spot VM prices are much lower than standard VMs. Option (B) Sole-tenant nodes are extremely expensive and not cost-effective. Option (C) App Engine Standard has request timeout limits, making it unsuitable for long-running transcoding tasks. Option (E) Databases are not suitable for storing large binary files (BLOBs); Cloud Storage should be used.",
  "wg": [
   {
    "t": "成本效益",
    "en": "cost-effective",
    "ps": "adjective"
   },
   {
    "t": "非同步解耦模式",
    "en": "asynchronous decoupling pattern",
    "ps": "noun"
   }
  ]
 }
},
{
 "no": "5",
 "level": "hard",
 "keywords": "API Management, Apigee, Security, Monetization",
 "question": [
  {
   "t": "HRL 計劃將其比賽預測模型的存取權限開放給合作夥伴，並以此建立新的商品化營收來源 (Merchandising revenue stream)。",
   "en": "HRL plans to expose access to their race prediction models to partners and create a new merchandising revenue stream from it.",
   "wg": [
    {
     "t": "合作夥伴",
     "en": "partners",
     "ps": "noun"
    },
    {
     "t": "商品化營收來源",
     "en": "merchandising revenue stream",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "您需要設計一個安全的 API 介面，不僅要能驗證合作夥伴身份，還需要具備流量配額管理 (Quotas) 與計費分析 (Analytics) 的能力。",
   "en": "You need to design a secure API interface that not only authenticates partners but also possesses capabilities for traffic quota management and billing analytics.",
   "wg": [
    {
     "t": "驗證",
     "en": "authenticates",
     "ps": "verb"
    },
    {
     "t": "流量配額管理",
     "en": "traffic quota management",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "解決方案必須能夠輕鬆地管理不同等級的合作夥伴服務協議 (SLA)。您應該採用哪項 Google Cloud 服務？",
   "en": "The solution must be able to easily manage different levels of Service Level Agreements (SLAs) for partners. Which Google Cloud service should you adopt?",
   "wg": [
    {
     "t": "服務協議",
     "en": "Service Level Agreements",
     "ps": "noun"
    },
    {
     "t": "管理",
     "en": "manage",
     "ps": "verb"
    }
   ]
  }
 ],
 "type": "單選題",
 "options": [
  {
   "t": "(A) 使用 Cloud Load Balancing 搭配 Cloud Armor，設定安全規則以限制特定 IP 的存取。",
   "en": "(A) Use Cloud Load Balancing with Cloud Armor, configuring security rules to restrict access to specific IPs.",
   "wg": [
    {
     "t": "Cloud Armor",
     "en": "Cloud Armor",
     "ps": "product"
    },
    {
     "t": "安全規則",
     "en": "security rules",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(B) 部署 Apigee API 管理平台，利用其 Developer Portal、貨幣化 (Monetization) 功能與流量政策來管理合作夥伴。",
   "en": "(B) Deploy the Apigee API management platform, leveraging its Developer Portal, Monetization features, and traffic policies to manage partners.",
   "wg": [
    {
     "t": "Apigee",
     "en": "Apigee",
     "ps": "product"
    },
    {
     "t": "貨幣化",
     "en": "Monetization",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(C) 使用 Identity-Aware Proxy (IAP) 來保護後端服務，並要求合作夥伴使用 Google 帳戶登入。",
   "en": "(C) Use Identity-Aware Proxy (IAP) to protect backend services and require partners to log in using Google accounts.",
   "wg": [
    {
     "t": "Identity-Aware Proxy",
     "en": "Identity-Aware Proxy",
     "ps": "product"
    },
    {
     "t": "保護後端服務",
     "en": "protect backend services",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "(D) 在 Compute Engine 上架設開源的 Kong API Gateway，並自行開發計費模組。",
   "en": "(D) Set up an open-source Kong API Gateway on Compute Engine and develop a custom billing module.",
   "wg": [
    {
     "t": "自行開發",
     "en": "develop a custom",
     "ps": "verb"
    },
    {
     "t": "計費模組",
     "en": "billing module",
     "ps": "noun"
    }
   ]
  }
 ],
 "answer": "(B)",
 "why": {
  "t": "Apigee 是 Google Cloud 的企業級 API 管理平台，專門設計用於管理 API 產品生命週期、安全性、分析以及『貨幣化』(Monetization)。題目明確提到『商品化營收』與『合作夥伴管理』，Apigee 是最完整且符合業務需求的解決方案。選項 (A) Cloud Armor 僅提供網路層安全防護 (DDoS/WAF)，無法管理 API 配額或計費。選項 (C) IAP 僅處理身分驗證，缺乏流量管理與營收分析功能。選項 (D) 自行架設 Gateway 徒增維運與開發成本，不符合最小化維運原則。",
  "en": "Apigee is Google Cloud's enterprise-grade API management platform, specifically designed for managing API product lifecycles, security, analytics, and 'monetization'. The question explicitly mentions 'merchandising revenue' and 'partner management', making Apigee the most complete solution aligning with business needs. Option (A) Cloud Armor only provides network-layer security (DDoS/WAF) and cannot manage API quotas or billing. Option (C) IAP handles authentication only, lacking traffic management and revenue analytics. Option (D) Self-hosting a Gateway adds unnecessary operational and development costs, contradicting the principle of minimizing ops.",
  "wg": [
   {
    "t": "企業級 API 管理平台",
    "en": "enterprise-grade API management platform",
    "ps": "noun"
   },
   {
    "t": "產品生命週期",
    "en": "product lifecycles",
    "ps": "noun"
   }
  ]
 }
},
{
 "no": "6",
 "level": "hard",
 "keywords": "Networking, VPN, Hybrid Connectivity, Mobility",
 "question": [
  {
   "t": "HRL 的比賽每週會在全球不同的偏遠地區舉行，現場部署的卡車行動資料中心負責收集遙測數據並傳送回 Google Cloud 進行即時分析。",
   "en": "HRL races are held weekly in different remote locations around the world, where deployed truck-mounted mobile data centers collect telemetry data and send it back to Google Cloud for real-time analysis.",
   "wg": [
    {
     "t": "行動資料中心",
     "en": "mobile data centers",
     "ps": "noun"
    },
    {
     "t": "偏遠地區",
     "en": "remote locations",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "雖然卡車配備了企業級的網際網路連線，但由於地點頻繁變動，無法佈建固定的實體線路。您需要建立一個安全、高可用且能快速部署的連接方案。",
   "en": "Although the trucks are equipped with enterprise-grade internet connectivity, physical fixed lines cannot be provisioned due to frequent location changes. You need to establish a secure, highly available, and rapidly deployable connectivity solution.",
   "wg": [
    {
     "t": "實體線路",
     "en": "physical fixed lines",
     "ps": "noun"
    },
    {
     "t": "快速部署",
     "en": "rapidly deployable",
     "ps": "adjective"
    }
   ]
  },
  {
   "t": "該方案必須確保資料傳輸過程中的加密，並且盡量減少每次賽事更換地點時的重新設定工作。",
   "en": "The solution must ensure encryption during data transmission and minimize reconfiguration work each time the race location changes.",
   "wg": [
    {
     "t": "加密",
     "en": "encryption",
     "ps": "noun"
    },
    {
     "t": "重新設定工作",
     "en": "reconfiguration work",
     "ps": "noun"
    }
   ]
  }
 ],
 "type": "單選題",
 "options": [
  {
   "t": "(A) 為每輛卡車配置 Cloud Interconnect (Partner)，並與當地的 ISP 合作以確保 SLA 連線品質。",
   "en": "(A) Provision Cloud Interconnect (Partner) for each truck and partner with local ISPs to ensure SLA connection quality.",
   "wg": [
    {
     "t": "Cloud Interconnect",
     "en": "Cloud Interconnect",
     "ps": "product"
    },
    {
     "t": "ISP",
     "en": "ISP",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(B) 在 Google Cloud 端設定高可用性 VPN (HA VPN)閘道器，並在卡車的路由器上設定動態路由 (BGP) 以連接至 VPC。",
   "en": "(B) Configure a High Availability VPN (HA VPN) gateway on Google Cloud and set up dynamic routing (BGP) on the truck routers to connect to the VPC.",
   "wg": [
    {
     "t": "高可用性 VPN",
     "en": "High Availability VPN",
     "ps": "product"
    },
    {
     "t": "動態路由",
     "en": "dynamic routing",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(C) 使用 Carrier Peering 服務直接連接 Google 的邊緣網路，以獲得比公用網際網路更佳的傳輸效能。",
   "en": "(C) Use Carrier Peering services to connect directly to Google's edge network for better transmission performance than the public internet.",
   "wg": [
    {
     "t": "Carrier Peering",
     "en": "Carrier Peering",
     "ps": "product"
    },
    {
     "t": "邊緣網路",
     "en": "edge network",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(D) 在每個比賽地點部署 Transfer Appliance，將資料存入後快遞回 Google 資料中心進行上傳。",
   "en": "(D) Deploy Transfer Appliances at each race location, store the data, and courier them back to Google data centers for upload.",
   "wg": [
    {
     "t": "Transfer Appliance",
     "en": "Transfer Appliance",
     "ps": "product"
    },
    {
     "t": "快遞",
     "en": "courier",
     "ps": "verb"
    }
   ]
  }
 ],
 "answer": "(B)",
 "why": {
  "t": "考量到 HRL 比賽地點每週變動且位於偏遠地區，實體線路 (Interconnect) 的佈建時間（通常需數週至數月）完全不可行。選項 (B) HA VPN 透過公共網際網路提供加密通道，且支援動態路由 (BGP)，當卡車移動到新地點只要有上網能力即可自動建立連線，符合快速部署與安全性需求。選項 (A) Interconnect 缺乏靈活性。選項 (C) Peering 雖快但未提供加密 (IPSec)，不符合安全要求。選項 (D) Transfer Appliance 是離線遷移方案，無法滿足『即時分析』的需求。",
  "en": "Considering HRL race locations change weekly and are in remote areas, the provisioning time for physical lines (Interconnect) (usually weeks to months) is completely unfeasible. Option (B) HA VPN provides an encrypted tunnel over the public internet and supports dynamic routing (BGP), allowing automatic connection establishment as long as there is internet access when trucks move to new locations, meeting rapid deployment and security needs. Option (A) Interconnect lacks flexibility. Option (C) Peering is fast but does not provide encryption (IPSec), failing security requirements. Option (D) Transfer Appliance is an offline migration solution and cannot meet the 'real-time analysis' requirement.",
  "wg": [
   {
    "t": "實體線路",
    "en": "physical lines",
    "ps": "noun"
   },
   {
    "t": "離線遷移方案",
    "en": "offline migration solution",
    "ps": "noun"
   }
  ]
 }
},
{
 "no": "7",
 "level": "hard",
 "keywords": "Data Processing, Dataflow, Pub/Sub, Real-time Analytics",
 "question": [
  {
   "t": "為了提升粉絲參與度，HRL 希望能即時分析『群眾情緒』(Crowd Sentiment)，包含社群媒體串流與現場麥克風收集到的音訊資料。",
   "en": "To increase fan engagement, HRL wants to analyze 'Crowd Sentiment' in real-time, including social media streams and audio data collected from on-site microphones.",
   "wg": [
    {
     "t": "群眾情緒",
     "en": "Crowd Sentiment",
     "ps": "noun"
    },
    {
     "t": "即時分析",
     "en": "real-time analysis",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "資料量會在比賽期間暴增，系統必須具備自動擴展能力，且需能處理亂序到達 (Out-of-order) 的事件數據。",
   "en": "Data volume will spike during races, so the system must be capable of autoscaling and handling out-of-order event data.",
   "wg": [
    {
     "t": "自動擴展",
     "en": "autoscaling",
     "ps": "adjective"
    },
    {
     "t": "亂序到達",
     "en": "Out-of-order",
     "ps": "adjective"
    }
   ]
  },
  {
   "t": "處理後的結構化數據需存入 BigQuery 供儀表板使用。請選擇最符合維運最小化與技術需求的架構組合。",
   "en": "Processed structured data needs to be stored in BigQuery for dashboard use. Select the architecture combination that best fits minimal operations and technical requirements.",
   "wg": [
    {
     "t": "維運最小化",
     "en": "minimal operations",
     "ps": "noun"
    },
    {
     "t": "架構組合",
     "en": "architecture combination",
     "ps": "noun"
    }
   ]
  }
 ],
 "type": "單選題",
 "options": [
  {
   "t": "(A) 使用 Cloud Functions 接收資料並直接寫入 BigQuery，利用 BigQuery 的重複資料刪除功能處理亂序問題。",
   "en": "(A) Use Cloud Functions to receive data and write directly to BigQuery, leveraging BigQuery's deduplication features to handle out-of-order issues.",
   "wg": [
    {
     "t": "Cloud Functions",
     "en": "Cloud Functions",
     "ps": "product"
    },
    {
     "t": "重複資料刪除",
     "en": "deduplication",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(B) 透過 Pub/Sub 擷取資料，並使用 Dataflow 搭配視窗 (Windowing) 與浮水印 (Watermarks) 機制進行處理，最後寫入 BigQuery。",
   "en": "(B) Ingest data via Pub/Sub, process it using Dataflow with Windowing and Watermarks mechanisms, and finally write to BigQuery.",
   "wg": [
    {
     "t": "Dataflow",
     "en": "Dataflow",
     "ps": "product"
    },
    {
     "t": "視窗與浮水印",
     "en": "Windowing and Watermarks",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(C) 部署 Kafka 叢集於 Compute Engine 上進行訊息緩衝，並撰寫 Spark Streaming 應用程式處理數據後存入 Cloud Storage。",
   "en": "(C) Deploy a Kafka cluster on Compute Engine for message buffering, and write Spark Streaming applications to process data before storing it in Cloud Storage.",
   "wg": [
    {
     "t": "Kafka 叢集",
     "en": "Kafka cluster",
     "ps": "noun"
    },
    {
     "t": "Spark Streaming",
     "en": "Spark Streaming",
     "ps": "product"
    }
   ]
  },
  {
   "t": "(D) 使用 Cloud Run 部署自定義的 Python 容器來輪詢 Pub/Sub 訂閱，處理完畢後透過 InsertAll API 寫入 BigQuery。",
   "en": "(D) Use Cloud Run to deploy custom Python containers to poll Pub/Sub subscriptions, and write to BigQuery via the InsertAll API after processing.",
   "wg": [
    {
     "t": "Cloud Run",
     "en": "Cloud Run",
     "ps": "product"
    },
    {
     "t": "輪詢",
     "en": "poll",
     "ps": "verb"
    }
   ]
  }
 ],
 "answer": "(B)",
 "why": {
  "t": "Dataflow 是 Google Cloud 處理即時串流數據的首選全託管服務，特別擅長處理『亂序到達』(Out-of-order) 與延遲數據，這依賴於其強大的 Watermark 與 Windowing 功能。搭配 Pub/Sub 作為緩衝區，能完美應對比賽期間的流量暴增。選項 (A) Cloud Functions 在高併發寫入 BigQuery 時可能會遇到連接數限制，且處理複雜的時間視窗邏輯相當困難。選項 (C) 自行維護 Kafka/Spark 違反『最小化維運複雜度』原則。選項 (D) 雖然可行，但自行實作亂序處理邏輯極為複雜且易錯，不如 Dataflow 原生支援。",
  "en": "Dataflow is Google Cloud's premier fully managed service for processing real-time streaming data, excelling at handling 'out-of-order' and late data through its powerful Watermark and Windowing capabilities. Paired with Pub/Sub as a buffer, it perfectly handles traffic spikes during races. Option (A) Cloud Functions may face connection limits when writing to BigQuery at high concurrency, and implementing complex time window logic is difficult. Option (C) Maintaining Kafka/Spark violates the 'minimize operational complexity' principle. Option (D) is feasible but implementing out-of-order logic manually is complex and error-prone compared to Dataflow's native support.",
  "wg": [
   {
    "t": "亂序到達",
    "en": "out-of-order",
    "ps": "adjective"
   },
   {
    "t": "全託管服務",
    "en": "fully managed service",
    "ps": "noun"
   }
  ]
 }
},
{
 "no": "8",
 "level": "hard",
 "keywords": "Security, Compliance, DLP, PII",
 "question": [
  {
   "t": "HRL 必須遵守各國的資料隱私法規 (如 GDPR)。在分析『群眾情緒』時，您發現收集到的音訊轉錄文字中可能包含粉絲的姓名或電話號碼等 PII (個人識別資訊)。",
   "en": "HRL must comply with data privacy regulations (like GDPR). While analyzing 'Crowd Sentiment', you discover that the transcribed audio text may contain PII (Personally Identifiable Information) such as fans' names or phone numbers.",
   "wg": [
    {
     "t": "資料隱私法規",
     "en": "data privacy regulations",
     "ps": "noun"
    },
    {
     "t": "個人識別資訊",
     "en": "Personally Identifiable Information",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "您需要確保在將這些資料寫入 Data Warehouse 進行長期分析之前，這些敏感資訊已被適當處理。",
   "en": "You need to ensure that this sensitive information is appropriately processed before writing the data to the Data Warehouse for long-term analysis.",
   "wg": [
    {
     "t": "敏感資訊",
     "en": "sensitive information",
     "ps": "noun"
    },
    {
     "t": "適當處理",
     "en": "appropriately processed",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "解決方案應自動化且無需修改既有的分析模型程式碼。您應該採取哪兩個步驟？",
   "en": "The solution should be automated and require no changes to existing analysis model code. Which two steps should you take?",
   "wg": [
    {
     "t": "自動化",
     "en": "automated",
     "ps": "adjective"
    },
    {
     "t": "分析模型",
     "en": "analysis model",
     "ps": "noun"
    }
   ]
  }
 ],
 "type": "複選題",
 "options": [
  {
   "t": "(A) 使用 Cloud Key Management Service (KMS) 為每個區域建立加密金鑰，並對儲存桶中的所有資料進行加密。",
   "en": "(A) Use Cloud Key Management Service (KMS) to create encryption keys for each region and encrypt all data in the storage buckets.",
   "wg": [
    {
     "t": "Cloud KMS",
     "en": "Cloud KMS",
     "ps": "product"
    },
    {
     "t": "加密金鑰",
     "en": "encryption keys",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(B) 在資料處理 Pipeline 中整合 Cloud Data Loss Prevention (DLP) API，設定檢測並遮蔽 (Masking) 常見的 PII 類型。",
   "en": "(B) Integrate the Cloud Data Loss Prevention (DLP) API into the data processing pipeline, configuring it to detect and mask common PII types.",
   "wg": [
    {
     "t": "Cloud DLP API",
     "en": "Cloud DLP API",
     "ps": "product"
    },
    {
     "t": "遮蔽",
     "en": "masking",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "(C) 設定 VPC Service Controls 以圍繞 BigQuery 和 Cloud Storage 建立安全邊界，防止資料外洩。",
   "en": "(C) Configure VPC Service Controls to create a security perimeter around BigQuery and Cloud Storage to prevent data exfiltration.",
   "wg": [
    {
     "t": "VPC Service Controls",
     "en": "VPC Service Controls",
     "ps": "product"
    },
    {
     "t": "安全邊界",
     "en": "security perimeter",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(D) 在將資料寫入 BigQuery 之前，呼叫 DLP API 的 de-identify 方法對文字欄位進行轉換。",
   "en": "(D) Call the DLP API's de-identify method to transform text fields before writing data to BigQuery.",
   "wg": [
    {
     "t": "去識別化方法",
     "en": "de-identify method",
     "ps": "noun"
    },
    {
     "t": "轉換",
     "en": "transform",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "(E) 修改 BigQuery 的 IAM 權限，僅允許『資料法規遵循官』角色的使用者存取包含 PII 的原始資料表。",
   "en": "(E) Modify BigQuery IAM permissions to only allow users with the 'Data Compliance Officer' role to access raw tables containing PII.",
   "wg": [
    {
     "t": "IAM 權限",
     "en": "IAM permissions",
     "ps": "noun"
    },
    {
     "t": "原始資料表",
     "en": "raw tables",
     "ps": "noun"
    }
   ]
  }
 ],
 "answer": "(B), (D)",
 "why": {
  "t": "題目要求針對 PII 進行處理以符合隱私法規（去識別化），而非單純的存取控制或靜態加密。選項 (B) 和 (D) 正確地指出了使用 Cloud DLP API 的核心功能：檢測 (Inspection) 與去識別化 (De-identification/Masking)。這能確保資料在進入分析資料倉儲前已移除敏感特徵。選項 (A) 加密僅防止未授權存取，解密後 PII 依然存在。選項 (C) VPC SC 防止資料外洩，但無法處理資料內容中的隱私問題。選項 (E) IAM 僅限制誰能看，但對於被授權的分析師或模型來說，PII 仍然暴露，不符去識別化要求。",
  "en": "The question requires processing PII for privacy compliance (de-identification), not just access control or encryption at rest. Options (B) and (D) correctly point to using the core functions of Cloud DLP API: Inspection and De-identification/Masking. This ensures sensitive features are removed before data enters the analytics warehouse. Option (A) Encryption only prevents unauthorized access; PII remains after decryption. Option (C) VPC SC prevents data exfiltration but doesn't address privacy issues within the data content. Option (E) IAM only limits who can view, but for authorized analysts or models, PII is still exposed, failing de-identification requirements.",
  "wg": [
   {
    "t": "去識別化",
    "en": "de-identification",
    "ps": "noun"
   },
   {
    "t": "靜態加密",
    "en": "encryption at rest",
    "ps": "noun"
   }
  ]
 }
},
{
 "no": "9",
 "level": "hard",
 "keywords": "Global Load Balancing, Scalability, Web Application, Latency",
 "question": [
  {
   "t": "HRL 的付費賽事串流服務目前部署在單一區域，為了增加同時在線觀看人數並改善全球使用者的延遲，您計劃將前端應用程式擴展至多個區域。",
   "en": "HRL's paid race streaming service is currently deployed in a single region. To increase concurrent viewers and improve latency for global users, you plan to expand the frontend application to multiple regions.",
   "wg": [
    {
     "t": "付費賽事串流服務",
     "en": "paid race streaming service",
     "ps": "noun"
    },
    {
     "t": "前端應用程式",
     "en": "frontend application",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "您需要一個能將流量導向最近的健康區域，並在區域故障時自動切換流量的負載平衡解決方案。同時，該方案需支援 SSL 卸載 (Offloading) 以減輕後端負擔。",
   "en": "You need a load balancing solution that routes traffic to the nearest healthy region and automatically fails over during regional outages. Also, it must support SSL offloading to reduce backend load.",
   "wg": [
    {
     "t": "導向",
     "en": "routes",
     "ps": "verb"
    },
    {
     "t": "SSL 卸載",
     "en": "SSL offloading",
     "ps": "noun"
    }
   ]
  }
 ],
 "type": "單選題",
 "options": [
  {
   "t": "(A) 使用外部 HTTP(S) 全球負載平衡器 (Global External Application Load Balancer)，並將多個區域的 Managed Instance Groups (MIGs) 設定為後端服務。",
   "en": "(A) Use a Global External Application Load Balancer and configure Managed Instance Groups (MIGs) in multiple regions as backend services.",
   "wg": [
    {
     "t": "全球負載平衡器",
     "en": "Global Load Balancer",
     "ps": "product"
    },
    {
     "t": "後端服務",
     "en": "backend services",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(B) 在每個區域部署網路負載平衡器 (Network Load Balancer)，並使用 Cloud DNS 的地理位置路由策略將使用者導向最近的區域 IP。",
   "en": "(B) Deploy a Network Load Balancer in each region and use Cloud DNS geolocation routing policies to direct users to the nearest regional IP.",
   "wg": [
    {
     "t": "網路負載平衡器",
     "en": "Network Load Balancer",
     "ps": "product"
    },
    {
     "t": "地理位置路由",
     "en": "geolocation routing",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(C) 使用 Cloud CDN 作為主要的入口點，若快取未命中 (Cache Miss)，則回源至位於總部的單一主要負載平衡器。",
   "en": "(C) Use Cloud CDN as the primary entry point, falling back to a single primary load balancer at headquarters if there is a Cache Miss.",
   "wg": [
    {
     "t": "快取未命中",
     "en": "Cache Miss",
     "ps": "noun"
    },
    {
     "t": "入口點",
     "en": "entry point",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(D) 部署 Traffic Director 來管理跨區域的流量，並將應用程式容器化部署在每個區域的 GKE 叢集中。",
   "en": "(D) Deploy Traffic Director to manage cross-region traffic and containerize the application for deployment in GKE clusters in each region.",
   "wg": [
    {
     "t": "Traffic Director",
     "en": "Traffic Director",
     "ps": "product"
    },
    {
     "t": "跨區域",
     "en": "cross-region",
     "ps": "adjective"
    }
   ]
  }
 ],
 "answer": "(A)",
 "why": {
  "t": "題目需求為『全球流量導向』、『低延遲』(最近區域)、『自動容錯移轉』以及『SSL 卸載』，這些正是 HTTP(S) 全球負載平衡器 (Global External ALB) 的核心功能。它使用單一 Anycast IP，能自動將用戶導向最近的後端 MIG。選項 (B) DNS 路由雖然可以做地理分流，但缺乏即時的健康檢查與流量滿載外溢 (Spillover) 能力，且 DNS 快取會導致容錯切換延遲。選項 (C) Cloud CDN 僅適用於靜態內容，付費入口網站包含動態邏輯。選項 (D) Traffic Director 主要用於內網微服務 (Service Mesh) 的流量管理，而非面向公開用戶的全球入口。",
  "en": "The requirements 'global traffic routing', 'low latency' (nearest region), 'automatic failover', and 'SSL offloading' are core features of the Global External HTTP(S) Load Balancer. It uses a single Anycast IP to automatically route users to the nearest backend MIG. Option (B) DNS routing can do geo-distribution but lacks real-time health checks and spillover capabilities, and DNS caching causes failover latency. Option (C) Cloud CDN is for static content, while a payment portal involves dynamic logic. Option (D) Traffic Director is primarily for internal microservices (Service Mesh) traffic management, not for public-facing global entry.",
  "wg": [
   {
    "t": "自動容錯移轉",
    "en": "automatic failover",
    "ps": "noun"
   },
   {
    "t": "單一 Anycast IP",
    "en": "single Anycast IP",
    "ps": "noun"
   }
  ]
 }
},
{
 "no": "10",
 "level": "hard",
 "keywords": "Storage, Cost Optimization, Lifecycle Management, Operations",
 "question": [
  {
   "t": "HRL 累積了多年的原始比賽影片檔案，儲存在 Cloud Storage 的 Standard 儲存級別中。維運團隊發現儲存成本逐年攀升，但舊比賽的影片在賽季結束 90 天後極少被存取。",
   "en": "HRL has accumulated years of raw race video files stored in the Cloud Storage Standard class. The operations team noticed storage costs rising annually, but videos from old races are rarely accessed 90 days after the season ends.",
   "wg": [
    {
     "t": "儲存成本",
     "en": "storage costs",
     "ps": "noun"
    },
    {
     "t": "極少被存取",
     "en": "rarely accessed",
     "ps": "adverb"
    }
   ]
  },
  {
   "t": "然而，為了符合法規要求，這些資料必須保留至少 5 年。您需要實作一個自動化的解決方案來降低成本，同時不增加管理負擔。",
   "en": "However, to meet regulatory requirements, this data must be retained for at least 5 years. You need to implement an automated solution to reduce costs without adding management overhead.",
   "wg": [
    {
     "t": "法規要求",
     "en": "regulatory requirements",
     "ps": "noun"
    },
    {
     "t": "管理負擔",
     "en": "management overhead",
     "ps": "noun"
    }
   ]
  }
 ],
 "type": "單選題",
 "options": [
  {
   "t": "(A) 撰寫一個 Cloud Functions 腳本，每天掃描 Bucket 中的物件，將超過 90 天的檔案移動到另一個設定為 Archive 級別的 Bucket。",
   "en": "(A) Write a Cloud Functions script to scan objects in the bucket daily and move files older than 90 days to another bucket configured with the Archive class.",
   "wg": [
    {
     "t": "掃描",
     "en": "scan",
     "ps": "verb"
    },
    {
     "t": "Archive 級別",
     "en": "Archive class",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(B) 在 Cloud Storage Bucket 上設定物件生命週期管理 (Lifecycle Management) 規則，將建立超過 90 天的物件自動降級至 Coldline 或 Archive 級別。",
   "en": "(B) Configure Object Lifecycle Management rules on the Cloud Storage bucket to automatically downgrade objects older than 90 days to Coldline or Archive class.",
   "wg": [
    {
     "t": "物件生命週期管理",
     "en": "Object Lifecycle Management",
     "ps": "feature"
    },
    {
     "t": "降級",
     "en": "downgrade",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "(C) 使用 gsutil rsync 指令定期將舊資料同步到地端的磁帶備份系統，並從雲端刪除這些檔案。",
   "en": "(C) Use the gsutil rsync command to regularly sync old data to an on-premises tape backup system and delete the files from the cloud.",
   "wg": [
    {
     "t": "磁帶備份系統",
     "en": "tape backup system",
     "ps": "noun"
    },
    {
     "t": "同步",
     "en": "sync",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "(D) 將所有現有資料轉換為 Nearline 儲存級別，因為 Nearline 提供了存取頻率與儲存成本之間的最佳平衡。",
   "en": "(D) Convert all existing data to the Nearline storage class, as Nearline offers the best balance between access frequency and storage cost.",
   "wg": [
    {
     "t": "Nearline",
     "en": "Nearline",
     "ps": "storage class"
    },
    {
     "t": "最佳平衡",
     "en": "best balance",
     "ps": "noun"
    }
   ]
  }
 ],
 "answer": "(B)",
 "why": {
  "t": "Cloud Storage 的『物件生命週期管理』(Lifecycle Management) 是處理此類需求的標準全託管功能，能根據物件年齡自動轉換儲存級別 (如 Standard -> Archive)，完全無需編寫程式碼或維護伺服器，符合『降低成本』與『不增加管理負擔』的目標。選項 (A) 使用 Cloud Functions 需要維護程式碼且會產生額外的列出/操作 API 費用。選項 (C) 遷回地端磁帶極大增加了維運複雜度。選項 (D) Nearline 的成本高於 Archive，且對於『極少存取』(90天後) 的資料來說，Archive 才是成本效益最高的選擇。",
  "en": "Cloud Storage's 'Object Lifecycle Management' is the standard fully managed feature for this requirement, automatically transitioning storage classes based on object age (e.g., Standard -> Archive) without writing code or maintaining servers, meeting the goals of 'reducing costs' and 'no added management overhead'. Option (A) Cloud Functions requires maintaining code and incurs extra API listing/operation costs. Option (C) Moving back to on-prem tape drastically increases operational complexity. Option (D) Nearline is more expensive than Archive, and for data that is 'rarely accessed' (after 90 days), Archive is the most cost-effective choice.",
  "wg": [
   {
    "t": "全託管功能",
    "en": "fully managed feature",
    "ps": "noun"
   },
   {
    "t": "成本效益最高",
    "en": "most cost-effective",
    "ps": "adjective"
   }
  ]
 }
},
{
 "no": "11",
 "level": "hard",
 "keywords": "Deployment, DevOps, Canary, GKE",
 "question": [
  {
   "t": "HRL 的工程團隊每週都會更新比賽預測演算法，希望能將新版本部署到生產環境中，但必須將風險降至最低。",
   "en": "HRL's engineering team updates the race prediction algorithm weekly and wants to deploy new versions to the production environment while minimizing risk.",
   "wg": [
    {
     "t": "比賽預測演算法",
     "en": "race prediction algorithm",
     "ps": "noun"
    },
    {
     "t": "部署",
     "en": "deploy",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "業務需求規定，新版本必須先針對 5% 的真實即時流量進行測試，經由自動化系統驗證預測準確度未下降後，才逐步推廣至剩餘的使用者。",
   "en": "Business requirements dictate that the new version must first be tested against 5% of real live traffic. Only after an automated system verifies that prediction accuracy has not degraded should it be gradually rolled out to the remaining users.",
   "wg": [
    {
     "t": "真實即時流量",
     "en": "real live traffic",
     "ps": "noun"
    },
    {
     "t": "逐步推廣",
     "en": "gradually rolled out",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "目前的應用程式運行在 Google Kubernetes Engine (GKE) 上，並使用 Istio Service Mesh。您應採取哪種部署策略？",
   "en": "The current application runs on Google Kubernetes Engine (GKE) using Istio Service Mesh. Which deployment strategy should you adopt?",
   "wg": [
    {
     "t": "Service Mesh",
     "en": "Service Mesh",
     "ps": "technology"
    },
    {
     "t": "部署策略",
     "en": "deployment strategy",
     "ps": "noun"
    }
   ]
  }
 ],
 "type": "單選題",
 "options": [
  {
   "t": "(A) 執行藍/綠部署 (Blue/Green Deployment)，將新版本部署在獨立的環境中，確認健康狀態後切換負載平衡器以轉移 100% 流量。",
   "en": "(A) Perform a Blue/Green Deployment by deploying the new version in a separate environment, and after verifying health, switch the load balancer to transfer 100% of the traffic.",
   "wg": [
    {
     "t": "藍/綠部署",
     "en": "Blue/Green Deployment",
     "ps": "strategy"
    },
    {
     "t": "轉移 100% 流量",
     "en": "transfer 100% of the traffic",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "(B) 採用金絲雀部署 (Canary Deployment)，利用 Istio 的 VirtualService 設定權重路由，將 5% 流量導向新版本。",
   "en": "(B) Adopt a Canary Deployment, using Istio's VirtualService to configure weighted routing to direct 5% of traffic to the new version.",
   "wg": [
    {
     "t": "金絲雀部署",
     "en": "Canary Deployment",
     "ps": "strategy"
    },
    {
     "t": "權重路由",
     "en": "weighted routing",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(C) 使用滾動更新 (Rolling Update) 策略，設定 maxSurge 為 5%，讓 Kubernetes 逐一替換舊的 Pods。",
   "en": "(C) Use a Rolling Update strategy, setting maxSurge to 5%, allowing Kubernetes to replace old Pods one by one.",
   "wg": [
    {
     "t": "滾動更新",
     "en": "Rolling Update",
     "ps": "strategy"
    },
    {
     "t": "maxSurge",
     "en": "maxSurge",
     "ps": "parameter"
    }
   ]
  },
  {
   "t": "(D) 實作重新建立 (Recreate) 策略，先停止所有舊版本容器以釋放資源，再啟動新版本容器以確保一致性。",
   "en": "(D) Implement a Recreate strategy, stopping all old version containers first to free up resources, then starting new version containers to ensure consistency.",
   "wg": [
    {
     "t": "重新建立",
     "en": "Recreate",
     "ps": "strategy"
    },
    {
     "t": "一致性",
     "en": "consistency",
     "ps": "noun"
    }
   ]
  }
 ],
 "answer": "(B)",
 "why": {
  "t": "題目明確要求『針對 5% 流量進行測試』與『驗證後逐步推廣』，這是金絲雀部署 (Canary Deployment) 的典型定義。由於 HRL 已使用 Istio，利用 Istio 的流量分割功能 (Traffic Splitting) 是最精確且符合雲端原生實踐的做法。選項 (A) 藍/綠部署通常是一次性切換所有流量，不符 5% 測試需求。選項 (C) 滾動更新雖然是漸進的，但很難精確控制在『5%』並暫停進行業務驗證（Kubernetes 原生滾動更新主要看健康檢查 probes，而非業務指標）。選項 (D) Recreate 會導致停機時間，不符合生產環境需求。",
  "en": "The question explicitly requires 'testing against 5% of traffic' and 'gradual rollout after verification', which is the classic definition of a Canary Deployment. Since HRL is already using Istio, leveraging Istio's Traffic Splitting capabilities is the most precise and cloud-native practice. Option (A) Blue/Green deployment typically switches all traffic at once, failing the 5% requirement. Option (C) Rolling Update is progressive but hard to pause exactly at '5%' for business verification (Kubernetes native rolling updates rely on health check probes, not business metrics). Option (D) Recreate causes downtime, which is unacceptable for a production environment.",
  "wg": [
   {
    "t": "流量分割",
    "en": "Traffic Splitting",
    "ps": "noun"
   },
   {
    "t": "雲端原生實踐",
    "en": "cloud-native practice",
    "ps": "noun"
   }
  ]
 }
},
{
 "no": "12",
 "level": "hard",
 "keywords": "Security, Cloud KMS, Encryption, Compliance",
 "question": [
  {
   "t": "HRL 正計劃推出一個即時博弈平台，允許粉絲對賽事結果進行下注。該平台的交易資料庫 (Cloud SQL for PostgreSQL) 將儲存敏感的財務數據。",
   "en": "HRL is planning to launch a real-time betting platform allowing fans to bet on race results. The platform's transaction database (Cloud SQL for PostgreSQL) will store sensitive financial data.",
   "wg": [
    {
     "t": "即時博弈平台",
     "en": "real-time betting platform",
     "ps": "noun"
    },
    {
     "t": "交易資料庫",
     "en": "transaction database",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "為了符合嚴格的金融法規，HRL 的安全團隊要求必須擁有加密金鑰的完全控制權，包括金鑰的輪替 (Rotation) 與銷毀，且不能使用 Google 管理的預設金鑰。",
   "en": "To comply with strict financial regulations, HRL's security team requires full control over encryption keys, including key rotation and destruction, and cannot use Google-managed default keys.",
   "wg": [
    {
     "t": "加密金鑰",
     "en": "encryption keys",
     "ps": "noun"
    },
    {
     "t": "金鑰的輪替",
     "en": "key rotation",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "您需要設計一個既符合合規要求又能與 Cloud SQL 無縫整合的加密方案。",
   "en": "You need to design an encryption solution that meets compliance requirements and integrates seamlessly with Cloud SQL.",
   "wg": [
    {
     "t": "合規要求",
     "en": "compliance requirements",
     "ps": "noun"
    },
    {
     "t": "無縫整合",
     "en": "integrates seamlessly",
     "ps": "verb"
    }
   ]
  }
 ],
 "type": "單選題",
 "options": [
  {
   "t": "(A) 使用 Cloud SQL 的預設加密設定，因為它已符合 FIPS 140-2 Level 1 標準。",
   "en": "(A) Use Cloud SQL's default encryption settings, as they already meet FIPS 140-2 Level 1 standards.",
   "wg": [
    {
     "t": "預設加密設定",
     "en": "default encryption settings",
     "ps": "noun"
    },
    {
     "t": "FIPS 140-2 Level 1",
     "en": "FIPS 140-2 Level 1",
     "ps": "standard"
    }
   ]
  },
  {
   "t": "(B) 在應用層實作客戶端加密 (Client-side encryption)，將資料加密後再寫入資料庫。",
   "en": "(B) Implement client-side encryption at the application layer, encrypting data before writing it to the database.",
   "wg": [
    {
     "t": "客戶端加密",
     "en": "Client-side encryption",
     "ps": "technology"
    },
    {
     "t": "應用層",
     "en": "application layer",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(C) 使用 Cloud Key Management Service (Cloud KMS) 建立客戶管理加密金鑰 (CMEK)，並設定 Cloud SQL 使用該金鑰加密資料。",
   "en": "(C) Use Cloud Key Management Service (Cloud KMS) to create Customer-Managed Encryption Keys (CMEK) and configure Cloud SQL to use that key for data encryption.",
   "wg": [
    {
     "t": "客戶管理加密金鑰",
     "en": "Customer-Managed Encryption Keys",
     "ps": "feature"
    },
    {
     "t": "Cloud KMS",
     "en": "Cloud KMS",
     "ps": "product"
    }
   ]
  },
  {
   "t": "(D) 使用 Cloud HSM (Hardware Security Module) 產生金鑰，並手動將金鑰檔案掛載到 Cloud SQL 實例中。",
   "en": "(D) Use Cloud HSM (Hardware Security Module) to generate keys and manually mount the key file into the Cloud SQL instance.",
   "wg": [
    {
     "t": "Cloud HSM",
     "en": "Cloud HSM",
     "ps": "product"
    },
    {
     "t": "掛載",
     "en": "mount",
     "ps": "verb"
    }
   ]
  }
 ],
 "answer": "(C)",
 "why": {
  "t": "題目要求『擁有金鑰的完全控制權』(如輪替與銷毀) 且不使用 Google 預設金鑰，同時要求『與 Cloud SQL 無縫整合』。CMEK (Customer-Managed Encryption Keys) 是 Google Cloud 提供的標準解決方案，允許用戶透過 Cloud KMS 管理金鑰生命週期，而 Cloud SQL 原生支援 CMEK 進行靜態資料加密。選項 (A) 使用 Google 預設金鑰，不符需求。選項 (B) 客戶端加密雖然安全，但會導致資料庫失去索引與搜尋功能 (如 SQL 查詢無法對加密欄位排序)，且實作複雜。選項 (D) Cloud SQL 是託管服務，不允許用戶手動掛載檔案系統或金鑰檔案。",
  "en": "The question requires 'full control over keys' (rotation, destruction) without using Google default keys, and 'seamless integration with Cloud SQL'. CMEK (Customer-Managed Encryption Keys) is the standard Google Cloud solution allowing users to manage key lifecycles via Cloud KMS, and Cloud SQL natively supports CMEK for encryption at rest. Option (A) uses Google default keys, failing requirements. Option (B) Client-side encryption is secure but causes the database to lose indexing and search capabilities (e.g., SQL queries cannot sort encrypted fields) and is complex to implement. Option (D) Cloud SQL is a managed service and does not allow manual mounting of filesystems or key files.",
  "wg": [
   {
    "t": "金鑰生命週期",
    "en": "key lifecycles",
    "ps": "noun"
   },
   {
    "t": "靜態資料加密",
    "en": "encryption at rest",
    "ps": "noun"
   }
  ]
 }
},
{
 "no": "13",
 "level": "hard",
 "keywords": "Reliability, Cloud SQL, High Availability, Disaster Recovery",
 "question": [
  {
   "t": "HRL 的核心比賽資料庫儲存了即時的賽事結果與車手排名，該系統對延遲極為敏感且不允許資料遺失。",
   "en": "HRL's core race database stores real-time race results and driver rankings; the system is extremely latency-sensitive and allows for zero data loss.",
   "wg": [
    {
     "t": "核心比賽資料庫",
     "en": "core race database",
     "ps": "noun"
    },
    {
     "t": "資料遺失",
     "en": "data loss",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "您將資料庫託管於 Cloud SQL for PostgreSQL。為了確保在單一區域 (Zone) 發生故障時系統能自動恢復且不會遺失任何已提交的交易 (RPO=0)，您應該如何配置？",
   "en": "You host the database on Cloud SQL for PostgreSQL. To ensure the system automatically recovers without losing any committed transactions (RPO=0) in the event of a single Zone failure, how should you configure it?",
   "wg": [
    {
     "t": "單一區域",
     "en": "single Zone",
     "ps": "noun"
    },
    {
     "t": "已提交的交易",
     "en": "committed transactions",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "此外，應用程式需要能夠在故障移轉 (Failover) 發生後無需更改連線字串。",
   "en": "Additionally, the application needs to be able to continue without changing the connection string after a failover occurs.",
   "wg": [
    {
     "t": "故障移轉",
     "en": "Failover",
     "ps": "noun"
    },
    {
     "t": "連線字串",
     "en": "connection string",
     "ps": "noun"
    }
   ]
  }
 ],
 "type": "單選題",
 "options": [
  {
   "t": "(A) 啟用 Cloud SQL 的高可用性 (High Availability) 設定，這將在同一個 Region 的不同 Zone 建立備用實例，並使用同步複製 (Synchronous Replication)。",
   "en": "(A) Enable Cloud SQL High Availability configuration, which creates a standby instance in a different Zone within the same Region and uses Synchronous Replication.",
   "wg": [
    {
     "t": "高可用性",
     "en": "High Availability",
     "ps": "feature"
    },
    {
     "t": "同步複製",
     "en": "Synchronous Replication",
     "ps": "technology"
    }
   ]
  },
  {
   "t": "(B) 在另一個 Region 建立唯讀複本 (Read Replica)，並在主實例故障時手動將其晉升 (Promote) 為主實例。",
   "en": "(B) Create a Read Replica in another Region and manually promote it to the primary instance if the primary fails.",
   "wg": [
    {
     "t": "唯讀複本",
     "en": "Read Replica",
     "ps": "noun"
    },
    {
     "t": "晉升",
     "en": "Promote",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "(C) 設定 Cloud SQL 的自動備份功能，將備份檔案儲存於多重區域 (Multi-regional) 的 Cloud Storage Bucket 中。",
   "en": "(C) Configure Cloud SQL automated backups to store backup files in a Multi-regional Cloud Storage Bucket.",
   "wg": [
    {
     "t": "自動備份",
     "en": "automated backups",
     "ps": "noun"
    },
    {
     "t": "多重區域",
     "en": "Multi-regional",
     "ps": "adjective"
    }
   ]
  },
  {
   "t": "(D) 使用 Compute Engine 部署 PostgreSQL 叢集，並設定 pgpool-II 進行負載平衡與讀寫分離。",
   "en": "(D) Deploy a PostgreSQL cluster on Compute Engine and configure pgpool-II for load balancing and read-write splitting.",
   "wg": [
    {
     "t": "pgpool-II",
     "en": "pgpool-II",
     "ps": "software"
    },
    {
     "t": "讀寫分離",
     "en": "read-write splitting",
     "ps": "noun"
    }
   ]
  }
 ],
 "answer": "(A)",
 "why": {
  "t": "Cloud SQL 的高可用性 (HA) 模式是針對區域級故障 (Zonal failure) 設計的標準方案。它使用同步複製 (Synchronous Replication) 將資料寫入主節點與備用節點，確保 RPO=0。當主節點故障時，系統會自動切換至備用節點，且 IP 位址保持不變，應用程式無需更改連線字串。選項 (B) 跨地區唯讀複本主要用於災難復原 (DR) 與讀取擴展，使用的是非同步複製，會有資料延遲 (RPO > 0) 且需手動介入或更改連線設定。選項 (C) 備份是冷復原方案，RPO 和 RTO 都很高。選項 (D) 自行架設增加了維運複雜度，不符雲端優先原則。",
  "en": "Cloud SQL's High Availability (HA) mode is the standard solution for Zonal failures. It uses Synchronous Replication to write data to both primary and standby nodes, ensuring RPO=0. When the primary fails, the system automatically fails over to the standby, and the IP address remains the same, so the application requires no connection string changes. Option (B) Cross-region Read Replicas are for Disaster Recovery (DR) and read scaling, using asynchronous replication which entails data lag (RPO > 0) and requires manual intervention or connection changes. Option (C) Backups are a cold recovery solution with high RPO and RTO. Option (D) Self-hosting increases operational complexity, contradicting cloud-first principles.",
  "wg": [
   {
    "t": "同步複製",
    "en": "Synchronous Replication",
    "ps": "noun"
   },
   {
    "t": "區域級故障",
    "en": "Zonal failure",
    "ps": "noun"
   }
  ]
 }
},
{
 "no": "14",
 "level": "hard",
 "keywords": "Security, GKE, Kubernetes, Network Policy",
 "question": [
  {
   "t": "HRL 的預測服務運行在 Google Kubernetes Engine (GKE) 上。為了防止潛在的惡意攻擊，安全團隊要求對叢集網路進行嚴格加固。",
   "en": "HRL's prediction service runs on Google Kubernetes Engine (GKE). To prevent potential malicious attacks, the security team requires strict hardening of the cluster network.",
   "wg": [
    {
     "t": "嚴格加固",
     "en": "strict hardening",
     "ps": "verb"
    },
    {
     "t": "潛在的惡意攻擊",
     "en": "potential malicious attacks",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "具體需求有兩點：1. 叢集的控制平面 (Control Plane) 不得透過公用網際網路存取；2. 叢集內部的 Pods 之間預設應禁止通訊，僅允許白名單定義的服務互相存取。",
   "en": "There are two specific requirements: 1. The cluster's Control Plane must not be accessible via the public internet; 2. Communication between Pods within the cluster should be denied by default, allowing only whitelisted services to access each other.",
   "wg": [
    {
     "t": "控制平面",
     "en": "Control Plane",
     "ps": "noun"
    },
    {
     "t": "預設禁止通訊",
     "en": "denied by default",
     "ps": "phrase"
    }
   ]
  },
  {
   "t": "您應該在 GKE 叢集中啟用哪兩項設定？",
   "en": "Which two settings should you enable in the GKE cluster?",
   "wg": [
    {
     "t": "啟用",
     "en": "enable",
     "ps": "verb"
    },
    {
     "t": "設定",
     "en": "settings",
     "ps": "noun"
    }
   ]
  }
 ],
 "type": "複選題",
 "options": [
  {
   "t": "(A) 啟用私有叢集 (Private Cluster)，並設定授權網路 (Authorized Networks) 以限制對主節點的存取。",
   "en": "(A) Enable Private Cluster and configure Authorized Networks to restrict access to the master node.",
   "wg": [
    {
     "t": "私有叢集",
     "en": "Private Cluster",
     "ps": "feature"
    },
    {
     "t": "授權網路",
     "en": "Authorized Networks",
     "ps": "feature"
    }
   ]
  },
  {
   "t": "(B) 設定網路政策 (Network Policies)，並建立一個『拒絕所有』(Deny-All) 的 Ingress 規則作為預設值。",
   "en": "(B) Configure Network Policies and create a 'Deny-All' Ingress rule as the default.",
   "wg": [
    {
     "t": "網路政策",
     "en": "Network Policies",
     "ps": "feature"
    },
    {
     "t": "拒絕所有",
     "en": "Deny-All",
     "ps": "rule"
    }
   ]
  },
  {
   "t": "(C) 使用 VPC 防火牆規則來封鎖所有來自 0.0.0.0/0 的流量，並針對每個 Pod IP 建立允許規則。",
   "en": "(C) Use VPC firewall rules to block all traffic from 0.0.0.0/0 and create allow rules for each Pod IP.",
   "wg": [
    {
     "t": "VPC 防火牆規則",
     "en": "VPC firewall rules",
     "ps": "feature"
    },
    {
     "t": "封鎖",
     "en": "block",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "(D) 啟用二進位授權 (Binary Authorization) 以確保只有受信任的容器映像檔能夠被部署。",
   "en": "(D) Enable Binary Authorization to ensure only trusted container images can be deployed.",
   "wg": [
    {
     "t": "二進位授權",
     "en": "Binary Authorization",
     "ps": "feature"
    },
    {
     "t": "受信任的容器映像檔",
     "en": "trusted container images",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(E) 實作 Cloud Armor 安全政策來過濾 Pod 之間的 HTTP 流量。",
   "en": "(E) Implement Cloud Armor security policies to filter HTTP traffic between Pods.",
   "wg": [
    {
     "t": "Cloud Armor 安全政策",
     "en": "Cloud Armor security policies",
     "ps": "feature"
    },
    {
     "t": "過濾",
     "en": "filter",
     "ps": "verb"
    }
   ]
  }
 ],
 "answer": "(A), (B)",
 "why": {
  "t": "針對需求 1 (隱藏控制平面)，啟用 GKE『私有叢集』(Private Cluster) 是標準做法，它會讓 Master Node 只有內網 IP，從而無法從公網存取。針對需求 2 (Pod 間流量控制)，Kubernetes 的『網路政策』(Network Policy) 才是正確的工具，它能在 Pod 層級定義 L3/L4 流量規則。選項 (C) VPC 防火牆作用於 VM 層級，無法有效管理 Pod 動態 IP 之間的通訊。選項 (D) Binary Authorization 是針對映像檔安全，與網路隔離無關。選項 (E) Cloud Armor 是邊緣安全服務 (WAF)，不適用於叢集內部東西向流量 (East-West traffic) 的隔離。",
  "en": "For requirement 1 (hiding control plane), enabling GKE 'Private Cluster' is the standard practice, assigning only private IPs to Master Nodes, making them inaccessible from the public internet. For requirement 2 (inter-Pod traffic control), Kubernetes 'Network Policy' is the correct tool, defining L3/L4 traffic rules at the Pod level. Option (C) VPC firewalls operate at the VM level and cannot effectively manage communication between dynamic Pod IPs. Option (D) Binary Authorization is for image security, unrelated to network isolation. Option (E) Cloud Armor is an edge security service (WAF), not applicable for internal East-West traffic isolation within the cluster.",
  "wg": [
   {
    "t": "標準做法",
    "en": "standard practice",
    "ps": "noun"
   },
   {
    "t": "東西向流量",
    "en": "East-West traffic",
    "ps": "noun"
   }
  ]
 }
},
{
 "no": "15",
 "level": "hard",
 "keywords": "Migration, Legacy Apps, Compute Engine, Auto-healing",
 "question": [
  {
   "t": "HRL 總部有一個舊有的『比賽模擬器』應用程式，運行在 Windows Server 上。該應用程式是用舊版 .NET 開發的，難以在短期內進行容器化改造。",
   "en": "HRL headquarters has a legacy 'Race Simulator' application running on Windows Server. The application was developed with an old version of .NET and is difficult to containerize in the short term.",
   "wg": [
    {
     "t": "比賽模擬器",
     "en": "Race Simulator",
     "ps": "noun"
    },
    {
     "t": "容器化改造",
     "en": "containerize",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "為了將資料中心撤收，您需要將此應用程式遷移至 Google Cloud。該應用程式雖然無狀態 (Stateless)，但偶爾會當機需要重啟。",
   "en": "To decommission the data center, you need to migrate this application to Google Cloud. Although the application is Stateless, it occasionally crashes and needs a restart.",
   "wg": [
    {
     "t": "無狀態",
     "en": "Stateless",
     "ps": "adjective"
    },
    {
     "t": "當機需要重啟",
     "en": "crashes and needs a restart",
     "ps": "phrase"
    }
   ]
  },
  {
   "t": "您需要選擇一個遷移路徑，能在不修改程式碼的前提下確保高可用性，並能自動處理當機恢復。",
   "en": "You need to select a migration path that ensures high availability without modifying the code and can automatically handle crash recovery.",
   "wg": [
    {
     "t": "遷移路徑",
     "en": "migration path",
     "ps": "noun"
    },
    {
     "t": "當機恢復",
     "en": "crash recovery",
     "ps": "noun"
    }
   ]
  }
 ],
 "type": "單選題",
 "options": [
  {
   "t": "(A) 使用 Migrate for Anthos 將 Windows 應用程式容器化，並部署到 GKE Windows Node Pool 上。",
   "en": "(A) Use Migrate for Anthos to containerize the Windows application and deploy it to a GKE Windows Node Pool.",
   "wg": [
    {
     "t": "Migrate for Anthos",
     "en": "Migrate for Anthos",
     "ps": "product"
    },
    {
     "t": "容器化",
     "en": "containerize",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "(B) 建立一個 Compute Engine 的託管執行個體群組 (MIG)，使用 Windows Server 映像檔，並設定應用程式健康檢查 (Health Check) 與自動修復 (Auto-healing) 策略。",
   "en": "(B) Create a Compute Engine Managed Instance Group (MIG) using a Windows Server image, and configure application Health Checks and Auto-healing policies.",
   "wg": [
    {
     "t": "託管執行個體群組",
     "en": "Managed Instance Group",
     "ps": "product"
    },
    {
     "t": "自動修復",
     "en": "Auto-healing",
     "ps": "feature"
    }
   ]
  },
  {
   "t": "(C) 將應用程式部署到 App Engine Flexible 環境，因為它支援自定義的運行環境。",
   "en": "(C) Deploy the application to the App Engine Flexible environment because it supports custom runtimes.",
   "wg": [
    {
     "t": "App Engine Flexible",
     "en": "App Engine Flexible",
     "ps": "product"
    },
    {
     "t": "自定義的運行環境",
     "en": "custom runtimes",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(D) 使用 VMware Engine 將現有的虛擬機器直接搬移上雲，並依賴 VMware HA 功能。",
   "en": "(D) Use VMware Engine to lift and shift the existing virtual machines directly to the cloud, relying on VMware HA features.",
   "wg": [
    {
     "t": "VMware Engine",
     "en": "VMware Engine",
     "ps": "product"
    },
    {
     "t": "直接搬移",
     "en": "lift and shift",
     "ps": "verb"
    }
   ]
  }
 ],
 "answer": "(B)",
 "why": {
  "t": "題目強調『難以容器化』且『無需修改程式碼』，排除選項 (A) 和 (C)，因為這兩者都涉及容器化過程 (App Engine Flex 底層也是容器)。選項 (B) 使用 Compute Engine MIG 是最佳解，因為它允許直接使用 VM 映像檔 (Lift & Shift)，同時提供了『自動修復』(Auto-healing) 功能：當健康檢查失敗（如 App 當機）時，MIG 會自動重啟 VM，完美解決應用程式不穩定的問題。選項 (D) VMware Engine 雖然可行但成本較高且維運模式改變較小，不如原生 MIG 具備彈性擴展與自動修復的雲端優勢。",
  "en": "The question emphasizes 'difficult to containerize' and 'no code modification', ruling out options (A) and (C), as both involve containerization (App Engine Flex is container-based under the hood). Option (B) using Compute Engine MIG is the optimal solution because it allows direct use of VM images (Lift & Shift) while providing 'Auto-healing': when health checks fail (e.g., app crash), the MIG automatically restarts the VM, perfectly solving the instability issue. Option (D) VMware Engine is feasible but more expensive and offers less cloud-native elasticity and auto-healing benefits compared to native MIG.",
  "wg": [
   {
    "t": "Lift & Shift",
    "en": "Lift & Shift",
    "ps": "strategy"
   },
   {
    "t": "雲端優勢",
    "en": "cloud advantages",
    "ps": "noun"
   }
  ]
 }
},
{
 "no": "16",
 "level": "hard",
 "keywords": "Governance, Organization Policy, Security, Public IP",
 "question": [
  {
   "t": "HRL 的雲端環境發展迅速，許多開發團隊開始在不同的專案中自行建立虛擬機器 (VM)。安全團隊發現有些開發用 VM 被錯誤地配置了外部 IP (Public IP)，暴露在網際網路上。",
   "en": "HRL's cloud environment is growing rapidly, with many development teams spinning up Virtual Machines (VMs) in various projects. The security team discovered that some development VMs were incorrectly configured with external IPs (Public IPs), exposing them to the internet.",
   "wg": [
    {
     "t": "自行建立",
     "en": "spinning up",
     "ps": "verb"
    },
    {
     "t": "外部 IP",
     "en": "external IPs",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "為了加強治理，您需要在組織層級實施一項強制性控制，禁止任何新建立的 VM 擁有外部 IP，除非該 VM 位於特定的白名單專案中。",
   "en": "To strengthen governance, you need to implement a mandatory control at the organization level that prohibits any newly created VMs from having external IPs, unless the VM is in a specific allowlisted project.",
   "wg": [
    {
     "t": "組織層級",
     "en": "organization level",
     "ps": "noun"
    },
    {
     "t": "強制性控制",
     "en": "mandatory control",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "此限制必須在資源建立時即刻生效，而非建立後才偵測。您應該怎麼做？",
   "en": "This restriction must be effective immediately upon resource creation, rather than being detected after creation. What should you do?",
   "wg": [
    {
     "t": "建立時即刻生效",
     "en": "effective immediately upon resource creation",
     "ps": "phrase"
    },
    {
     "t": "建立後才偵測",
     "en": "detected after creation",
     "ps": "phrase"
    }
   ]
  }
 ],
 "type": "單選題",
 "options": [
  {
   "t": "(A) 設定 VPC 防火牆規則，預設拒絕所有來自網際網路的入站流量 (Ingress Traffic)，並僅開放特定專案的連接埠。",
   "en": "(A) Configure VPC Firewall rules to deny all ingress traffic from the internet by default, and only open ports for specific projects.",
   "wg": [
    {
     "t": "VPC 防火牆規則",
     "en": "VPC Firewall rules",
     "ps": "feature"
    },
    {
     "t": "入站流量",
     "en": "Ingress Traffic",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(B) 定義一個組織政策 (Organization Policy)，設定約束條件 `compute.vmExternalIpAccess`，並將允許的專案加入例外清單。",
   "en": "(B) Define an Organization Policy with the constraint `compute.vmExternalIpAccess`, and add allowed projects to the exception list.",
   "wg": [
    {
     "t": "組織政策",
     "en": "Organization Policy",
     "ps": "product"
    },
    {
     "t": "約束條件",
     "en": "constraint",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(C) 使用 Cloud Asset Inventory 建立一個 Feed，監控資源變更通知。當偵測到有 Public IP 時，觸發 Cloud Functions 自動將其移除。",
   "en": "(C) Use Cloud Asset Inventory to create a Feed monitoring resource change notifications. Trigger a Cloud Function to automatically remove Public IPs when detected.",
   "wg": [
    {
     "t": "Cloud Asset Inventory",
     "en": "Cloud Asset Inventory",
     "ps": "product"
    },
    {
     "t": "觸發",
     "en": "trigger",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "(D) 在 IAM 中建立一個自訂角色，移除 `compute.instances.create` 權限，並將其套用至所有開發人員群組。",
   "en": "(D) Create a custom role in IAM, removing the `compute.instances.create` permission, and apply it to all developer groups.",
   "wg": [
    {
     "t": "自訂角色",
     "en": "custom role",
     "ps": "noun"
    },
    {
     "t": "開發人員群組",
     "en": "developer groups",
     "ps": "noun"
    }
   ]
  }
 ],
 "answer": "(B)",
 "why": {
  "t": "組織政策 (Organization Policy) 是 Google Cloud 用於集中式管控資源配置的工具。透過 `compute.vmExternalIpAccess` 約束條件，可以從根本上『禁止』VM 被指派外部 IP，這是在 API 層級進行阻擋，符合『建立時生效』的需求。選項 (A) 防火牆僅阻擋流量，無法阻止 IP 的指派與暴露風險。選項 (C) 屬於事後補救 (Reactive)，資源建立與移除之間存在時間差，不符合『即刻生效』要求。選項 (D) 移除建立實例的權限會導致開發者完全無法工作，而非僅限制 IP 配置。",
  "en": "Organization Policy is Google Cloud's tool for centralized resource configuration governance. The `compute.vmExternalIpAccess` constraint fundamentally 'prohibits' VMs from being assigned external IPs at the API level, meeting the 'effective upon creation' requirement. Option (A) Firewall rules only block traffic, not the assignment and exposure risk of the IP itself. Option (C) is reactive; there is a time gap between resource creation and removal, failing the 'immediate' requirement. Option (D) Removing instance creation permissions would stop developers from working entirely, rather than just restricting IP configuration.",
  "wg": [
   {
    "t": "集中式管控",
    "en": "centralized governance",
    "ps": "noun"
   },
   {
    "t": "事後補救",
    "en": "reactive",
    "ps": "adjective"
   }
  ]
 }
},
{
 "no": "17",
 "level": "hard",
 "keywords": "Hybrid Connectivity, Dedicated Interconnect, SLA, High Availability",
 "question": [
  {
   "t": "HRL 總部透過 Dedicated Interconnect 連線至 Google Cloud，用於傳輸關鍵的比賽即時轉播訊號。業務部門要求連線必須達到 99.99% 的可用性 SLA。",
   "en": "HRL headquarters connects to Google Cloud via Dedicated Interconnect for transmitting critical live race broadcast signals. The business unit requires the connection to meet a 99.99% availability SLA.",
   "wg": [
    {
     "t": "關鍵的",
     "en": "critical",
     "ps": "adjective"
    },
    {
     "t": "可用性 SLA",
     "en": "availability SLA",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "目前您已在一個大都會區域 (Metro) 建立了一條 Interconnect 連線。為了滿足 SLA 需求，您需要如何擴展網路拓樸？",
   "en": "Currently, you have established one Interconnect connection in a single metropolitan area (Metro). To meet the SLA requirement, how should you expand the network topology?",
   "wg": [
    {
     "t": "大都會區域",
     "en": "metropolitan area",
     "ps": "noun"
    },
    {
     "t": "網路拓樸",
     "en": "network topology",
     "ps": "noun"
    }
   ]
  }
 ],
 "type": "單選題",
 "options": [
  {
   "t": "(A) 在另一個不同的大都會區域 (Metro) 建立第二條 Interconnect 連線，並確保每個都會區都有兩條獨立的電路 (Circuit)。",
   "en": "(A) Establish a second Interconnect connection in a different metropolitan area (Metro), ensuring each metro has two independent circuits.",
   "wg": [
    {
     "t": "獨立的電路",
     "en": "independent circuits",
     "ps": "noun"
    },
    {
     "t": "大都會區域",
     "en": "metropolitan area",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(B) 在同一個大都會區域增加第二條 Interconnect 連線，以提供備援。",
   "en": "(B) Add a second Interconnect connection in the same metropolitan area to provide redundancy.",
   "wg": [
    {
     "t": "備援",
     "en": "redundancy",
     "ps": "noun"
    },
    {
     "t": "增加",
     "en": "add",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "(C) 設定 Cloud VPN 作為現有 Interconnect 的備援路徑，並啟用 ECMP 路由。",
   "en": "(C) Configure Cloud VPN as a backup path for the existing Interconnect and enable ECMP routing.",
   "wg": [
    {
     "t": "Cloud VPN",
     "en": "Cloud VPN",
     "ps": "product"
    },
    {
     "t": "ECMP 路由",
     "en": "ECMP routing",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(D) 升級現有的連線頻寬至 100 Gbps，以減少擁塞並提高可靠性。",
   "en": "(D) Upgrade the existing connection bandwidth to 100 Gbps to reduce congestion and improve reliability.",
   "wg": [
    {
     "t": "連線頻寬",
     "en": "connection bandwidth",
     "ps": "noun"
    },
    {
     "t": "可靠性",
     "en": "reliability",
     "ps": "noun"
    }
   ]
  }
 ],
 "answer": "(A)",
 "why": {
  "t": "Google Cloud 對於 Dedicated Interconnect 的 99.99% SLA 有嚴格的拓樸要求：必須在『兩個不同的都會區』(Metros)，且每個都會區需有『兩個不同的邊緣可用性網域』(Edge Availability Domains)，總共 4 條電路。選項 (B) 僅在同一都會區備援只能達到 99.9% SLA。選項 (C) VPN 作為備援雖然常見，但 Google 不保證 VPN + Interconnect 的組合能達到 99.99% 的企業級 SLA (通常 VPN 頻寬與穩定性較低)。選項 (D) 增加頻寬與 SLA 無關。",
  "en": "Google Cloud has strict topology requirements for a 99.99% SLA on Dedicated Interconnect: it requires connections in 'two different metropolitan areas' (Metros), with 'two different edge availability domains' in each metro, totaling 4 circuits. Option (B) redundancy in the same metro only achieves 99.9% SLA. Option (C) while using VPN as backup is common, Google does not guarantee a 99.99% enterprise-grade SLA for the VPN + Interconnect combination (VPNs typically have lower bandwidth and stability). Option (D) increasing bandwidth is unrelated to SLA.",
  "wg": [
   {
    "t": "嚴格的拓樸要求",
    "en": "strict topology requirements",
    "ps": "noun"
   },
   {
    "t": "邊緣可用性網域",
    "en": "edge availability domains",
    "ps": "noun"
   }
  ]
 }
},
{
 "no": "18",
 "level": "hard",
 "keywords": "Serverless, Cloud Run, Scalability, Cost Optimization",
 "question": [
  {
   "t": "HRL 正在重構其『賽事結果查詢 API』。該 API 的流量特性是：在非賽季期間幾乎沒有流量，但在比賽週末會有極高的並發請求 (Concurrent Requests)。",
   "en": "HRL is refactoring its 'Race Results Query API'. The traffic pattern is: almost no traffic during the off-season, but extremely high concurrent requests during race weekends.",
   "wg": [
    {
     "t": "賽事結果查詢 API",
     "en": "Race Results Query API",
     "ps": "noun"
    },
    {
     "t": "並發請求",
     "en": "concurrent requests",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "您希望將此 API 容器化，以確保開發與生產環境的一致性，並要求平台能在無流量時自動縮減至零 (Scale to Zero) 以節省成本，且在流量進來時能快速啟動。",
   "en": "You want to containerize this API to ensure consistency between development and production, and require the platform to automatically scale to zero when there is no traffic to save costs, while starting up quickly when traffic arrives.",
   "wg": [
    {
     "t": "容器化",
     "en": "containerize",
     "ps": "verb"
    },
    {
     "t": "縮減至零",
     "en": "scale to zero",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "您應該選擇哪項運算服務？",
   "en": "Which compute service should you choose?",
   "wg": [
    {
     "t": "運算服務",
     "en": "compute service",
     "ps": "noun"
    },
    {
     "t": "選擇",
     "en": "choose",
     "ps": "verb"
    }
   ]
  }
 ],
 "type": "單選題",
 "options": [
  {
   "t": "(A) Google Kubernetes Engine (GKE) Standard，並設定 Horizontal Pod Autoscaler (HPA)。",
   "en": "(A) Google Kubernetes Engine (GKE) Standard, configured with Horizontal Pod Autoscaler (HPA).",
   "wg": [
    {
     "t": "GKE Standard",
     "en": "GKE Standard",
     "ps": "product"
    },
    {
     "t": "HPA",
     "en": "HPA",
     "ps": "feature"
    }
   ]
  },
  {
   "t": "(B) Compute Engine 託管執行個體群組 (MIG)，並設定基於 CPU 使用率的自動擴展。",
   "en": "(B) Compute Engine Managed Instance Groups (MIG), configured with CPU utilization-based autoscaling.",
   "wg": [
    {
     "t": "託管執行個體群組",
     "en": "Managed Instance Groups",
     "ps": "product"
    },
    {
     "t": "自動擴展",
     "en": "autoscaling",
     "ps": "feature"
    }
   ]
  },
  {
   "t": "(C) Cloud Functions (第 2 代)，因為它是事件驅動的。",
   "en": "(C) Cloud Functions (2nd gen), because it is event-driven.",
   "wg": [
    {
     "t": "Cloud Functions",
     "en": "Cloud Functions",
     "ps": "product"
    },
    {
     "t": "事件驅動",
     "en": "event-driven",
     "ps": "adjective"
    }
   ]
  },
  {
   "t": "(D) Cloud Run，因為它支援容器化應用程式、處理 HTTP 並發請求，且能自動擴展至零。",
   "en": "(D) Cloud Run, because it supports containerized applications, handles HTTP concurrent requests, and can automatically scale to zero.",
   "wg": [
    {
     "t": "Cloud Run",
     "en": "Cloud Run",
     "ps": "product"
    },
    {
     "t": "並發請求",
     "en": "concurrent requests",
     "ps": "noun"
    }
   ]
  }
 ],
 "answer": "(D)",
 "why": {
  "t": "Cloud Run 是 Google Cloud 執行『容器化』、『HTTP 請求』且需要『縮減至零』(Scale to Zero) 的最佳無伺服器平台。它特別擅長處理 API 類型的並發請求 (Concurrency)。選項 (A) GKE Standard 無法縮減至零 (至少需一個節點)，且維運成本較高。選項 (B) Compute Engine MIG 擴展速度慢且無法縮減至零 (至少需一台 VM)。選項 (C) Cloud Functions 雖然也能縮減至零，但 Cloud Run 在處理高並發 HTTP API 與容器開發體驗 (Dockerfile) 上更具優勢，且題目強調了『容器化』的需求。",
  "en": "Cloud Run is the optimal serverless platform on Google Cloud for running 'containerized', 'HTTP request' workloads that need to 'scale to zero'. It excels at handling concurrency for API-type workloads. Option (A) GKE Standard cannot scale to zero (requires at least one node) and has higher operational costs. Option (B) Compute Engine MIG scales slowly and cannot scale to zero (requires at least one VM). Option (C) Cloud Functions can scale to zero, but Cloud Run is superior for high-concurrency HTTP APIs and container development experience (Dockerfile), and the question emphasizes the 'containerization' requirement.",
  "wg": [
   {
    "t": "無伺服器平台",
    "en": "serverless platform",
    "ps": "noun"
   },
   {
    "t": "並發",
    "en": "concurrency",
    "ps": "noun"
   }
  ]
 }
},
{
 "no": "19",
 "level": "hard",
 "keywords": "Cost Management, BigQuery, FinOps, Attribution",
 "question": [
  {
   "t": "HRL 的財務長對 BigQuery 分析費用的快速增長表示擔憂。目前的費用以『隨用隨付』(On-demand) 模式計費，難以預測且無法區分是哪個部門（行銷、工程或營運）產生了費用。",
   "en": "HRL's CFO is concerned about the rapid growth of BigQuery analytics costs. Current costs are billed under the 'On-demand' model, making them unpredictable and impossible to attribute to specific departments (Marketing, Engineering, or Operations).",
   "wg": [
    {
     "t": "隨用隨付",
     "en": "On-demand",
     "ps": "model"
    },
    {
     "t": "區分",
     "en": "attribute",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "您需要建立一個機制來實現兩個目標：1. 確保費用可預測並設定上限；2. 能夠精確地將成本分攤給各個部門。",
   "en": "You need to establish a mechanism to achieve two goals: 1. Ensure costs are predictable and capped; 2. Accurately allocate costs to each department.",
   "wg": [
    {
     "t": "設定上限",
     "en": "capped",
     "ps": "adjective"
    },
    {
     "t": "成本分攤",
     "en": "allocate costs",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "您應該採取哪兩個行動？",
   "en": "Which two actions should you take?",
   "wg": [
    {
     "t": "行動",
     "en": "actions",
     "ps": "noun"
    },
    {
     "t": "採取",
     "en": "take",
     "ps": "verb"
    }
   ]
  }
 ],
 "type": "複選題",
 "options": [
  {
   "t": "(A) 為每個部門的 Dataset 與 Job 套用不同的標籤 (Labels)，並利用 Billing Reports 進行篩選。",
   "en": "(A) Apply different Labels to Datasets and Jobs for each department, and use Billing Reports for filtering.",
   "wg": [
    {
     "t": "標籤",
     "en": "Labels",
     "ps": "feature"
    },
    {
     "t": "Billing Reports",
     "en": "Billing Reports",
     "ps": "tool"
    }
   ]
  },
  {
   "t": "(B) 限制每個使用者每天可以執行的查詢位元組數上限 (Custom Quota)。",
   "en": "(B) Limit the maximum number of query bytes each user can execute per day (Custom Quota).",
   "wg": [
    {
     "t": "自訂配額",
     "en": "Custom Quota",
     "ps": "feature"
    },
    {
     "t": "限制",
     "en": "Limit",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "(C) 購買 BigQuery Editions (如 Standard 或 Enterprise) 並使用預留容量 (Reservations) 來取代隨用隨付模式。",
   "en": "(C) Purchase BigQuery Editions (e.g., Standard or Enterprise) and use Reservations to replace the On-demand model.",
   "wg": [
    {
     "t": "BigQuery Editions",
     "en": "BigQuery Editions",
     "ps": "product"
    },
    {
     "t": "預留容量",
     "en": "Reservations",
     "ps": "feature"
    }
   ]
  },
  {
   "t": "(D) 將每個部門的數據遷移到不同的 Google Cloud 專案 (Project) 中，以便於帳單層級進行區隔。",
   "en": "(D) Migrate each department's data to separate Google Cloud Projects to facilitate segregation at the billing level.",
   "wg": [
    {
     "t": "專案",
     "en": "Projects",
     "ps": "noun"
    },
    {
     "t": "區隔",
     "en": "segregation",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(E) 使用 Cloud Monitoring 建立成本儀表板，並在費用超支時發送 Slack 通知。",
   "en": "(E) Use Cloud Monitoring to create a cost dashboard and send Slack notifications when costs overrun.",
   "wg": [
    {
     "t": "Cloud Monitoring",
     "en": "Cloud Monitoring",
     "ps": "product"
    },
    {
     "t": "超支",
     "en": "overrun",
     "ps": "verb"
    }
   ]
  }
 ],
 "answer": "(A), (C)",
 "why": {
  "t": "要達成『費用可預測』與『設定上限』，從隨用隨付 (On-demand) 切換至 BigQuery Editions 的容量計價 (Capacity computing/Reservations) 是最有效的方法，因為您是購買固定的 Slot 容量，成本不會隨查詢量無限增加。要達成『精確分攤』，標籤 (Labels) 是 FinOps 的標準做法，可以標記 Job 或 Dataset 的歸屬部門。選項 (B) 雖然可以限制用量，但會導致查詢失敗 (Hard limit)，影響業務運作，且不是『預測』費用的最佳解。選項 (D) 遷移專案工程浩大。選項 (E) 僅是監控，無法主動控制費用上限。",
  "en": "To achieve 'predictable costs' and 'capped costs', switching from On-demand to BigQuery Editions capacity pricing (Reservations) is the most effective method, as you purchase fixed Slot capacity, so costs don't scale infinitely with query volume. To achieve 'accurate allocation', Labels are the FinOps standard for tagging Jobs or Datasets to owning departments. Option (B) can limit usage but causes query failures (Hard limit), impacting business operations, and isn't the best solution for 'predicting' costs. Option (D) migrating projects is a massive engineering effort. Option (E) is monitoring only and does not proactively cap costs.",
  "wg": [
   {
    "t": "容量計價",
    "en": "Capacity computing",
    "ps": "noun"
   },
   {
    "t": "標準做法",
    "en": "standard practice",
    "ps": "noun"
   }
  ]
 }
},
{
 "no": "20",
 "level": "hard",
 "keywords": "Observability, Distributed Tracing, Latency, Microservices",
 "question": [
  {
   "t": "HRL 的賽事直播平台採用微服務架構，由前端、API Gateway、使用者服務與資料庫等多個元件組成。使用者回報在登入或載入影片時偶爾會遇到高延遲或 500 錯誤。",
   "en": "HRL's race streaming platform uses a microservices architecture, consisting of frontend, API Gateway, user service, and database components. Users report occasional high latency or 500 errors when logging in or loading videos.",
   "wg": [
    {
     "t": "微服務架構",
     "en": "microservices architecture",
     "ps": "noun"
    },
    {
     "t": "高延遲",
     "en": "high latency",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "Cloud Logging 中的錯誤日誌分散在各個服務中，難以定位問題的根源 (Root Cause) 到底是在哪一個環節。您需要一個工具來視覺化單一請求跨越多個微服務的完整路徑與耗時。",
   "en": "Error logs in Cloud Logging are scattered across services, making it difficult to pinpoint the Root Cause. You need a tool to visualize the complete path and duration of a single request across multiple microservices.",
   "wg": [
    {
     "t": "分散",
     "en": "scattered",
     "ps": "adjective"
    },
    {
     "t": "視覺化",
     "en": "visualize",
     "ps": "verb"
    }
   ]
  }
 ],
 "type": "單選題",
 "options": [
  {
   "t": "(A) 使用 Cloud Profiler 來分析每個服務的 CPU 與記憶體使用狀況，找出效能瓶頸。",
   "en": "(A) Use Cloud Profiler to analyze CPU and memory usage of each service to identify performance bottlenecks.",
   "wg": [
    {
     "t": "Cloud Profiler",
     "en": "Cloud Profiler",
     "ps": "product"
    },
    {
     "t": "效能瓶頸",
     "en": "performance bottlenecks",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(B) 在所有微服務中設定 Error Reporting，並彙整所有例外狀況 (Exceptions) 到單一儀表板。",
   "en": "(B) Configure Error Reporting in all microservices and aggregate all exceptions into a single dashboard.",
   "wg": [
    {
     "t": "Error Reporting",
     "en": "Error Reporting",
     "ps": "product"
    },
    {
     "t": "例外狀況",
     "en": "exceptions",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(C) 實作 Cloud Trace (分散式追蹤)，並確保應用程式在服務呼叫間傳遞 Trace Context。",
   "en": "(C) Implement Cloud Trace (Distributed Tracing) and ensure the application propagates Trace Context between service calls.",
   "wg": [
    {
     "t": "Cloud Trace",
     "en": "Cloud Trace",
     "ps": "product"
    },
    {
     "t": "分散式追蹤",
     "en": "Distributed Tracing",
     "ps": "technology"
    }
   ]
  },
  {
   "t": "(D) 使用 VPC Flow Logs 監控服務之間的網路流量延遲與封包遺失率。",
   "en": "(D) Use VPC Flow Logs to monitor network traffic latency and packet loss rates between services.",
   "wg": [
    {
     "t": "VPC Flow Logs",
     "en": "VPC Flow Logs",
     "ps": "product"
    },
    {
     "t": "封包遺失率",
     "en": "packet loss rates",
     "ps": "noun"
    }
   ]
  }
 ],
 "answer": "(C)",
 "why": {
  "t": "題目描述的是典型的微服務除錯場景，需要追蹤『單一請求跨越多個服務』的路徑與時間 (Waterfall view)。Cloud Trace 是 Google Cloud 提供的分散式追蹤解決方案，能精確顯示請求在每個微服務停留的時間，協助定位是哪個服務導致了延遲或錯誤。選項 (A) Profiler 用於程式碼級別的資源消耗優化 (如某個函數佔用太多 CPU)。選項 (B) Error Reporting 僅彙整錯誤堆疊 (Stack Trace)，無法顯示服務間的調用關係與延遲。選項 (D) VPC Flow Logs 是網路層級監控，無法深入應用程式邏輯。",
  "en": "The scenario describes a typical microservice debugging case requiring tracking the path and duration of a 'single request across multiple services' (Waterfall view). Cloud Trace is Google Cloud's distributed tracing solution, precisely showing the time spent in each microservice, helping pinpoint which service caused latency or error. Option (A) Profiler is for code-level resource optimization (e.g., a function using too much CPU). Option (B) Error Reporting aggregates stack traces but cannot show inter-service call relationships or latency. Option (D) VPC Flow Logs are network-level monitoring and cannot provide insight into application logic.",
  "wg": [
   {
    "t": "分散式追蹤",
    "en": "Distributed Tracing",
    "ps": "noun"
   },
   {
    "t": "程式碼級別",
    "en": "code-level",
    "ps": "adjective"
   }
  ]
 }
},
{
 "no": "21",
 "level": "hard",
 "keywords": "Database, Global Consistency, Cloud Spanner, Gaming",
 "question": [
  {
   "t": "Mountkirk Games 準備推出一款全球多人連線遊戲。遊戲的核心功能之一是『全球玩家物品欄』(Global Player Inventory)，用於儲存玩家購買的虛擬寶物。",
   "en": "Mountkirk Games is preparing to launch a global multiplayer online game. A core feature is the 'Global Player Inventory', used to store virtual items purchased by players.",
   "wg": [
    {
     "t": "全球玩家物品欄",
     "en": "Global Player Inventory",
     "ps": "noun"
    },
    {
     "t": "虛擬寶物",
     "en": "virtual items",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "為了防止物品複製或雙重消費 (Double Spending)，資料庫必須支援跨區域的強一致性 (Strong Consistency) 交易，同時能夠水平擴展以應對數百萬併發玩家。",
   "en": "To prevent item duplication or double spending, the database must support cross-region Strong Consistency transactions while being able to horizontally scale to handle millions of concurrent players.",
   "wg": [
    {
     "t": "強一致性",
     "en": "Strong Consistency",
     "ps": "term"
    },
    {
     "t": "水平擴展",
     "en": "horizontally scale",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "現有的 MySQL 分片架構維護困難，您需要建議一個全託管的 Google Cloud 資料庫服務來取代它。",
   "en": "The existing MySQL sharding architecture is difficult to maintain. You need to recommend a fully managed Google Cloud database service to replace it.",
   "wg": [
    {
     "t": "分片架構",
     "en": "sharding architecture",
     "ps": "noun"
    },
    {
     "t": "全託管",
     "en": "fully managed",
     "ps": "adjective"
    }
   ]
  }
 ],
 "type": "單選題",
 "options": [
  {
   "t": "(A) Cloud Spanner，配置為多重區域 (Multi-region) 實例，利用其 TrueTime 技術實現全球 ACID 交易。",
   "en": "(A) Cloud Spanner, configured as a Multi-region instance, leveraging its TrueTime technology to achieve global ACID transactions.",
   "wg": [
    {
     "t": "Cloud Spanner",
     "en": "Cloud Spanner",
     "ps": "product"
    },
    {
     "t": "ACID 交易",
     "en": "ACID transactions",
     "ps": "term"
    }
   ]
  },
  {
   "t": "(B) Cloud Bigtable，因為它專為高吞吐量寫入設計，且能透過 Row Keys 的設計來達成最終一致性。",
   "en": "(B) Cloud Bigtable, because it is designed for high-throughput writes and can achieve eventual consistency through Row Key design.",
   "wg": [
    {
     "t": "Cloud Bigtable",
     "en": "Cloud Bigtable",
     "ps": "product"
    },
    {
     "t": "最終一致性",
     "en": "eventual consistency",
     "ps": "term"
    }
   ]
  },
  {
   "t": "(C) Cloud SQL for PostgreSQL，並設定跨區域的高可用性 (HA) 與唯讀複本 (Read Replicas) 以支援全球讀取。",
   "en": "(C) Cloud SQL for PostgreSQL, configuring cross-region High Availability (HA) and Read Replicas to support global reads.",
   "wg": [
    {
     "t": "唯讀複本",
     "en": "Read Replicas",
     "ps": "feature"
    },
    {
     "t": "全球讀取",
     "en": "global reads",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(D) Firestore (Datastore 模式)，因為它是一個全球分布的 NoSQL 資料庫，且支援 ACID 交易。",
   "en": "(D) Firestore (Datastore mode), because it is a globally distributed NoSQL database and supports ACID transactions.",
   "wg": [
    {
     "t": "NoSQL 資料庫",
     "en": "NoSQL database",
     "ps": "noun"
    },
    {
     "t": "全球分布",
     "en": "globally distributed",
     "ps": "adjective"
    }
   ]
  }
 ],
 "answer": "(A)",
 "why": {
  "t": "Cloud Spanner 是唯一能同時提供『全球水平擴展』與『關聯式資料庫強一致性 (ACID)』的服務。對於金融或遊戲物品欄這類絕對不允許資料不一致 (如雙重消費) 的場景，Spanner 是標準答案。選項 (B) Bigtable 僅提供單一列原子性，不支援跨列/跨表的 ACID 交易。選項 (C) Cloud SQL 無法水平擴展寫入能力 (Vertical Scaling Only)，且跨區域複本是最終一致性。選項 (D) Firestore 雖然支援 ACID，但在處理極大規模的高頻寫入 (High write throughput) 時，不如 Spanner 適合關聯性強的庫存系統。",
  "en": "Cloud Spanner is the only service that provides both 'global horizontal scaling' and 'relational database strong consistency (ACID)'. For scenarios like finance or game inventory where data inconsistency (e.g., double spending) is strictly prohibited, Spanner is the standard answer. Option (B) Bigtable only provides single-row atomicity and does not support cross-row/cross-table ACID transactions. Option (C) Cloud SQL cannot scale writes horizontally (Vertical Scaling Only), and cross-region replicas are eventually consistent. Option (D) Firestore supports ACID, but for massive-scale high-frequency writes, it is less suitable for strongly relational inventory systems than Spanner.",
  "wg": [
   {
    "t": "強一致性",
    "en": "strong consistency",
    "ps": "noun"
   },
   {
    "t": "標準答案",
    "en": "standard answer",
    "ps": "noun"
   }
  ]
 }
},
{
 "no": "22",
 "level": "hard",
 "keywords": "Data Engineering, Dataflow, Batch and Streaming, Unification",
 "question": [
  {
   "t": "TerramEarth 從其重型機具收集了兩種類型的數據：透過行動網路傳送的『即時遙測數據』，以及維修時手動上傳的『歷史日誌檔案』。",
   "en": "TerramEarth collects two types of data from its heavy machinery: 'real-time telemetry' sent via cellular networks, and 'historical log files' manually uploaded during maintenance.",
   "wg": [
    {
     "t": "即時遙測數據",
     "en": "real-time telemetry",
     "ps": "noun"
    },
    {
     "t": "歷史日誌檔案",
     "en": "historical log files",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "工程團隊希望使用同一套業務邏輯程式碼來處理這兩種數據來源，進行清洗、轉換與聚合，並將結果存入 BigQuery。您需要設計一個統一的資料處理 Pipeline。",
   "en": "The engineering team wants to use the same business logic code to process both data sources for cleaning, transformation, and aggregation, storing the results in BigQuery. You need to design a unified data processing pipeline.",
   "wg": [
    {
     "t": "同一套業務邏輯",
     "en": "same business logic",
     "ps": "noun"
    },
    {
     "t": "統一的",
     "en": "unified",
     "ps": "adjective"
    }
   ]
  },
  {
   "t": "該解決方案必須是全託管的，且能自動處理延遲到達的數據 (Late Data)。",
   "en": "The solution must be fully managed and capable of automatically handling late-arriving data (Late Data).",
   "wg": [
    {
     "t": "延遲到達的數據",
     "en": "Late Data",
     "ps": "noun"
    },
    {
     "t": "自動處理",
     "en": "automatically handling",
     "ps": "verb"
    }
   ]
  }
 ],
 "type": "單選題",
 "options": [
  {
   "t": "(A) 使用 Cloud Dataproc 執行 Spark Structured Streaming 作業，因為它支援批次與串流的統一 API。",
   "en": "(A) Use Cloud Dataproc to run Spark Structured Streaming jobs, as it supports a unified API for batch and streaming.",
   "wg": [
    {
     "t": "Cloud Dataproc",
     "en": "Cloud Dataproc",
     "ps": "product"
    },
    {
     "t": "統一 API",
     "en": "unified API",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(B) 撰寫 Cloud Functions 來觸發處理邏輯，即時數據由 Pub/Sub 觸發，批次檔案由 Cloud Storage 事件觸發。",
   "en": "(B) Write Cloud Functions to trigger processing logic; real-time data is triggered by Pub/Sub, and batch files are triggered by Cloud Storage events.",
   "wg": [
    {
     "t": "Cloud Functions",
     "en": "Cloud Functions",
     "ps": "product"
    },
    {
     "t": "觸發",
     "en": "trigger",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "(C) 使用 BigQuery 的排程查詢 (Scheduled Queries) 來處理批次資料，並使用 BigQuery 串流插入 API 處理即時資料。",
   "en": "(C) Use BigQuery Scheduled Queries to process batch data and the BigQuery Streaming Insert API for real-time data.",
   "wg": [
    {
     "t": "排程查詢",
     "en": "Scheduled Queries",
     "ps": "feature"
    },
    {
     "t": "串流插入",
     "en": "Streaming Insert",
     "ps": "feature"
    }
   ]
  },
  {
   "t": "(D) 使用 Dataflow 搭配 Apache Beam SDK 開發 Pipeline，因為它專為統一批次與串流處理模式而設計。",
   "en": "(D) Use Dataflow with the Apache Beam SDK to develop the pipeline, as it is specifically designed for a unified batch and streaming processing model.",
   "wg": [
    {
     "t": "Dataflow",
     "en": "Dataflow",
     "ps": "product"
    },
    {
     "t": "統一批次與串流",
     "en": "unified batch and streaming",
     "ps": "phrase"
    }
   ]
  }
 ],
 "answer": "(D)",
 "why": {
  "t": "Dataflow (基於 Apache Beam) 是 Google Cloud 核心的資料處理服務，其最大特色就是『統一的程式設計模型』(Unified Programming Model)，同一段程式碼可以同時運行在 Batch (讀取檔案) 與 Streaming (讀取 Pub/Sub) 模式下，並具備強大的 Windowing 與 Watermark 機制處理延遲數據。選項 (A) Dataproc 是託管的 Hadoop/Spark，雖可行但管理複雜度高於 Serverless 的 Dataflow。選項 (B) Cloud Functions 不適合大規模資料聚合 (Aggregation) 與長時間運行的 ETL。選項 (C) 雖然都用 BigQuery，但處理邏輯分散在 SQL 與應用程式中，不符合『同一套代碼』的要求。",
  "en": "Dataflow (based on Apache Beam) is Google Cloud's core data processing service, featuring a 'Unified Programming Model'. The same code can run in both Batch (reading files) and Streaming (reading Pub/Sub) modes, with powerful Windowing and Watermark mechanisms for handling late data. Option (A) Dataproc is managed Hadoop/Spark; while feasible, it has higher management complexity than serverless Dataflow. Option (B) Cloud Functions are not suitable for large-scale aggregation and long-running ETL. Option (C) splits logic between SQL and application code, failing the 'same code' requirement.",
  "wg": [
   {
    "t": "統一的程式設計模型",
    "en": "Unified Programming Model",
    "ps": "term"
   },
   {
    "t": "聚合",
    "en": "Aggregation",
    "ps": "noun"
   }
  ]
 }
},
{
 "no": "23",
 "level": "hard",
 "keywords": "Networking, Dedicated Interconnect, Hybrid, EHR",
 "question": [
  {
   "t": "EHR Healthcare 需要將其地端資料中心內約 500 TB 的醫學影像封存資料遷移至 Google Cloud。由於資料極為敏感，安全長禁止透過公共網際網路傳輸。",
   "en": "EHR Healthcare needs to migrate approximately 500 TB of archived medical imaging data from their on-premises data center to Google Cloud. Due to the extreme sensitivity of the data, the CISO prohibits transmission over the public internet.",
   "wg": [
    {
     "t": "醫學影像封存資料",
     "en": "archived medical imaging data",
     "ps": "noun"
    },
    {
     "t": "公共網際網路",
     "en": "public internet",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "EHR 的資料中心位於一個與 Google Cloud 共用設施的託管中心 (Colocation Facility)。您需要建立一個至少 20 Gbps 頻寬的專用連線以滿足遷移時限。",
   "en": "EHR's data center is located in a colocation facility shared with Google Cloud. You need to establish a dedicated connection with at least 20 Gbps bandwidth to meet the migration deadline.",
   "wg": [
    {
     "t": "託管中心",
     "en": "Colocation Facility",
     "ps": "noun"
    },
    {
     "t": "專用連線",
     "en": "dedicated connection",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "您應該選擇哪種連接方式？",
   "en": "Which connectivity option should you choose?",
   "wg": [
    {
     "t": "連接方式",
     "en": "connectivity option",
     "ps": "noun"
    },
    {
     "t": "選擇",
     "en": "choose",
     "ps": "verb"
    }
   ]
  }
 ],
 "type": "單選題",
 "options": [
  {
   "t": "(A) 設定 Partner Interconnect，透過服務供應商連接，因為這樣可以更快完成開通。",
   "en": "(A) Configure Partner Interconnect via a service provider, as this allows for faster provisioning.",
   "wg": [
    {
     "t": "Partner Interconnect",
     "en": "Partner Interconnect",
     "ps": "product"
    },
    {
     "t": "服務供應商",
     "en": "service provider",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(B) 部署 Transfer Appliance 硬體裝置，將資料複製後寄送回 Google。",
   "en": "(B) Deploy Transfer Appliance hardware to copy the data and ship it back to Google.",
   "wg": [
    {
     "t": "Transfer Appliance",
     "en": "Transfer Appliance",
     "ps": "product"
    },
    {
     "t": "寄送",
     "en": "ship",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "(C) 設定多條 Cloud VPN 通道並啟用 ECMP，以聚合頻寬並透過網際網路傳輸。",
   "en": "(C) Configure multiple Cloud VPN tunnels and enable ECMP to aggregate bandwidth and transmit over the internet.",
   "wg": [
    {
     "t": "Cloud VPN",
     "en": "Cloud VPN",
     "ps": "product"
    },
    {
     "t": "聚合頻寬",
     "en": "aggregate bandwidth",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "(D) 設定 Dedicated Interconnect，因為客戶位於 Google 的網路接入點 (PoP) 位置，且需要高頻寬私有連線。",
   "en": "(D) Configure Dedicated Interconnect, as the customer is located at a Google Point of Presence (PoP) and requires high-bandwidth private connectivity.",
   "wg": [
    {
     "t": "Dedicated Interconnect",
     "en": "Dedicated Interconnect",
     "ps": "product"
    },
    {
     "t": "網路接入點",
     "en": "Point of Presence",
     "ps": "noun"
    }
   ]
  }
 ],
 "answer": "(D)",
 "why": {
  "t": "題目有關鍵提示：『位於 Colocation Facility』(代表在 Google PoP 內)、『20 Gbps 頻寬』以及『禁止公共網際網路』。Dedicated Interconnect 是在客戶設備與 Google 邊緣路由器之間拉實體光纖，提供 10 Gbps 或 100 Gbps 的專用頻寬，完全不經過網際網路，最適合此情境。選項 (A) Partner Interconnect 通常用於客戶不在 Google PoP 的情況。選項 (B) 雖然安全，但對於擁有 20 Gbps 連線能力的場景，線上傳輸可能比實體寄送更具長期效益且即時。選項 (C) 違反了『禁止透過公共網際網路』的規定。",
  "en": "The key hints are 'located in a Colocation Facility' (implies being in a Google PoP), '20 Gbps bandwidth', and 'prohibits public internet'. Dedicated Interconnect involves running physical fiber between customer equipment and Google edge routers, providing 10 Gbps or 100 Gbps dedicated bandwidth completely bypassing the internet, making it ideal. Option (A) Partner Interconnect is usually for when customers are not in a Google PoP. Option (B) is secure, but with 20 Gbps connectivity, online transfer is likely more efficient and real-time than shipping hardware. Option (C) violates the 'prohibit public internet' rule.",
  "wg": [
   {
    "t": "實體光纖",
    "en": "physical fiber",
    "ps": "noun"
   },
   {
    "t": "長期效益",
    "en": "long-term efficiency",
    "ps": "noun"
   }
  ]
 }
},
{
 "no": "24",
 "level": "hard",
 "keywords": "Security, IAM, Service Accounts, Least Privilege",
 "question": [
  {
   "t": "在稽核 HRL 的專案時，您發現 Compute Engine 預設服務帳戶 (Default Service Account) 被賦予了 Editor 角色，且被多個不同用途的 VM (Web Server, Worker, Database Proxy) 共用。",
   "en": "While auditing HRL's project, you discovered that the Compute Engine Default Service Account has the Editor role and is shared by multiple VMs with different purposes (Web Server, Worker, Database Proxy).",
   "wg": [
    {
     "t": "預設服務帳戶",
     "en": "Default Service Account",
     "ps": "noun"
    },
    {
     "t": "共用",
     "en": "shared",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "這違反了最小權限原則 (Least Privilege)。您需要修復此安全漏洞，確保每個工作負載僅擁有其執行所需的最低權限。",
   "en": "This violates the Principle of Least Privilege. You need to remediate this security vulnerability ensuring each workload has only the minimum permissions required for its execution.",
   "wg": [
    {
     "t": "最小權限原則",
     "en": "Least Privilege",
     "ps": "principle"
    },
    {
     "t": "修復",
     "en": "remediate",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "您應該採取哪兩個步驟？",
   "en": "Which two steps should you take?",
   "wg": [
    {
     "t": "步驟",
     "en": "steps",
     "ps": "noun"
    },
    {
     "t": "採取",
     "en": "take",
     "ps": "verb"
    }
   ]
  }
 ],
 "type": "複選題",
 "options": [
  {
   "t": "(A) 針對 Web、Worker 與 Proxy 分別建立新的使用者管理服務帳戶 (User-managed Service Accounts)。",
   "en": "(A) Create new User-managed Service Accounts specifically for Web, Worker, and Proxy respectively.",
   "wg": [
    {
     "t": "使用者管理服務帳戶",
     "en": "User-managed Service Accounts",
     "ps": "noun"
    },
    {
     "t": "分別建立",
     "en": "Create ... respectively",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "(B) 修改預設服務帳戶的權限，移除 Editor 角色，改為僅賦予 Viewer 角色。",
   "en": "(B) Modify the Default Service Account permissions by removing the Editor role and assigning only the Viewer role.",
   "wg": [
    {
     "t": "修改",
     "en": "Modify",
     "ps": "verb"
    },
    {
     "t": "Viewer 角色",
     "en": "Viewer role",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(C) 為新建立的服務帳戶指派特定的細粒度 IAM 角色 (如 BigQuery Data Viewer, Cloud SQL Client)，並更新 VM 使用這些帳戶。",
   "en": "(C) Assign specific granular IAM roles (e.g., BigQuery Data Viewer, Cloud SQL Client) to the new Service Accounts and update the VMs to use these accounts.",
   "wg": [
    {
     "t": "細粒度 IAM 角色",
     "en": "granular IAM roles",
     "ps": "noun"
    },
    {
     "t": "指派",
     "en": "Assign",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "(D) 在每個 VM 實例上設定存取範圍 (Access Scopes)，限制其只能存取 Cloud Storage 與 Logging。",
   "en": "(D) Configure Access Scopes on each VM instance to restrict access to only Cloud Storage and Logging.",
   "wg": [
    {
     "t": "存取範圍",
     "en": "Access Scopes",
     "ps": "feature"
    },
    {
     "t": "限制",
     "en": "restrict",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "(E) 刪除預設服務帳戶，以強制所有應用程式停止運作，直到開發者重新配置安全設定。",
   "en": "(E) Delete the Default Service Account to force all applications to stop working until developers reconfigure security settings.",
   "wg": [
    {
     "t": "刪除",
     "en": "Delete",
     "ps": "verb"
    },
    {
     "t": "強制",
     "en": "force",
     "ps": "verb"
    }
   ]
  }
 ],
 "answer": "(A), (C)",
 "why": {
  "t": "要落實最小權限原則，必須將『身分』(Identity) 與『權限』(Role) 解耦。步驟一 (A) 是為不同的工作負載建立獨立的身分 (Service Accounts)，這樣才能區分誰需要什麼。步驟二 (C) 是賦予這些新身分精確的權限。選項 (B) 仍然讓所有 VM 共用同一個帳戶，無法區分權限需求（例如 Web 需要讀 DB，Worker 需要寫 Storage）。選項 (D) Access Scopes 是舊一代的機制，Google 建議使用 IAM 作為主要管控手段。選項 (E) 具破壞性，不是專業的遷移方式。",
  "en": "To implement Least Privilege, you must decouple 'Identity' from 'Permissions'. Step one (A) is creating separate identities (Service Accounts) for different workloads to distinguish needs. Step two (C) is assigning precise permissions to these new identities. Option (B) still has all VMs sharing the same account, failing to separate needs (e.g., Web needs DB read, Worker needs Storage write). Option (D) Access Scopes are a legacy mechanism; Google recommends IAM as the primary control. Option (E) is destructive and not a professional migration method.",
  "wg": [
   {
    "t": "身分與權限解耦",
    "en": "decouple Identity from Permissions",
    "ps": "concept"
   },
   {
    "t": "具破壞性",
    "en": "destructive",
    "ps": "adjective"
   }
  ]
 }
},
{
 "no": "25",
 "level": "hard",
 "keywords": "Database, Caching, Memorystore, Latency",
 "question": [
  {
   "t": "HRL 正在開發即時排行榜功能，需要在比賽期間處理來自全球數百萬粉絲的投票與排名更新。該系統需要支援極高的寫入吞吐量 (Throughput) 與亞毫秒級 (Sub-millisecond) 的讀取延遲。",
   "en": "HRL is developing a real-time leaderboard feature that needs to handle millions of votes and ranking updates from global fans during races. The system requires extremely high write throughput and sub-millisecond read latency.",
   "wg": [
    {
     "t": "即時排行榜",
     "en": "real-time leaderboard",
     "ps": "noun"
    },
    {
     "t": "亞毫秒級",
     "en": "Sub-millisecond",
     "ps": "adjective"
    }
   ]
  },
  {
   "t": "排行榜資料在比賽結束後即不再重要，無需長期持久化儲存。您應該選擇哪項資料儲存服務？",
   "en": "Leaderboard data is not important after the race ends and does not require long-term persistent storage. Which data storage service should you choose?",
   "wg": [
    {
     "t": "持久化儲存",
     "en": "persistent storage",
     "ps": "noun"
    },
    {
     "t": "不再重要",
     "en": "not important",
     "ps": "phrase"
    }
   ]
  }
 ],
 "type": "單選題",
 "options": [
  {
   "t": "(A) Cloud Bigtable，因為它支援高吞吐量寫入，適合時間序列資料。",
   "en": "(A) Cloud Bigtable, because it supports high-throughput writes and is suitable for time-series data.",
   "wg": [
    {
     "t": "Cloud Bigtable",
     "en": "Cloud Bigtable",
     "ps": "product"
    },
    {
     "t": "時間序列資料",
     "en": "time-series data",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(B) Memorystore for Redis，因為它是記憶體內資料庫，提供最低延遲且原生支援 Sorted Sets 用於排行榜。",
   "en": "(B) Memorystore for Redis, because it is an in-memory database providing the lowest latency and natively supports Sorted Sets for leaderboards.",
   "wg": [
    {
     "t": "Memorystore for Redis",
     "en": "Memorystore for Redis",
     "ps": "product"
    },
    {
     "t": "記憶體內資料庫",
     "en": "in-memory database",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(C) Cloud SQL，使用記憶體優化的機器類型 (High-memory machine types)。",
   "en": "(C) Cloud SQL, using High-memory machine types.",
   "wg": [
    {
     "t": "Cloud SQL",
     "en": "Cloud SQL",
     "ps": "product"
    },
    {
     "t": "記憶體優化",
     "en": "High-memory",
     "ps": "adjective"
    }
   ]
  },
  {
   "t": "(D) Firestore，因為它支援即時監聽 (Real-time listeners) 並能自動推播更新給前端客戶端。",
   "en": "(D) Firestore, because it supports Real-time listeners and can automatically push updates to frontend clients.",
   "wg": [
    {
     "t": "Firestore",
     "en": "Firestore",
     "ps": "product"
    },
    {
     "t": "即時監聽",
     "en": "Real-time listeners",
     "ps": "feature"
    }
   ]
  }
 ],
 "answer": "(B)",
 "why": {
  "t": "關鍵字『亞毫秒級延遲』(Sub-millisecond) 與『排行榜』(Leaderboard) 強烈指向 Redis。Redis 的 Sorted Sets 資料結構是實作排行榜的業界標準。Memorystore for Redis 是 Google Cloud 的全託管 Redis 服務，完全符合這些需求。選項 (A) Bigtable 雖然吞吐量高，但延遲通常在毫秒級 (個位數 ms)，且不具備原生的排行榜排序邏輯。選項 (D) Firestore 雖然即時，但寫入頻率有限制 (單一文件每秒 1 次寫入)，難以應對數百萬高頻更新的熱點問題。",
  "en": "The keywords 'Sub-millisecond latency' and 'Leaderboard' strongly point to Redis. Redis Sorted Sets are the industry standard for implementing leaderboards. Memorystore for Redis is Google Cloud's fully managed Redis service, perfectly meeting these needs. Option (A) Bigtable has high throughput but latency is typically in the single-digit millisecond range, and it lacks native leaderboard sorting logic. Option (D) Firestore is real-time but has write frequency limits (1 write per second per document), making it difficult to handle millions of high-frequency updates on hotspots.",
  "wg": [
   {
    "t": "全託管 Redis 服務",
    "en": "fully managed Redis service",
    "ps": "noun"
   },
   {
    "t": "業界標準",
    "en": "industry standard",
    "ps": "noun"
   }
  ]
 }
},
{
 "no": "26",
 "level": "hard",
 "keywords": "Networking, Cloud DNS, Hybrid, Resolution",
 "question": [
  {
   "t": "EHR Healthcare 已透過 Dedicated Interconnect 建立了混合雲連線。現在面臨一個名稱解析 (DNS Resolution) 的挑戰：地端的傳統應用程式需要能解析 Google Cloud 上的私有 GKE 服務 (如 `app.internal`)。",
   "en": "EHR Healthcare has established hybrid cloud connectivity via Dedicated Interconnect. Now facing a DNS Resolution challenge: legacy on-premises applications need to resolve private GKE services on Google Cloud (e.g., `app.internal`).",
   "wg": [
    {
     "t": "名稱解析",
     "en": "DNS Resolution",
     "ps": "noun"
    },
    {
     "t": "私有 GKE 服務",
     "en": "private GKE services",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "同時，Google Cloud 上的新微服務也需要透過主機名稱 (Hostname) 呼叫地端的大型主機資料庫。您需要設計一個雙向解析方案，且不希望維護額外的 DNS 伺服器叢集。",
   "en": "Simultaneously, new microservices on Google Cloud need to call on-premises mainframe databases via hostname. You need to design a bidirectional resolution solution without maintaining additional DNS server clusters.",
   "wg": [
    {
     "t": "雙向解析",
     "en": "bidirectional resolution",
     "ps": "noun"
    },
    {
     "t": "DNS 伺服器叢集",
     "en": "DNS server clusters",
     "ps": "noun"
    }
   ]
  }
 ],
 "type": "單選題",
 "options": [
  {
   "t": "(A) 在 Compute Engine 上部署 BIND DNS 伺服器，並設定條件轉發器 (Conditional Forwarders) 指向地端 DNS。",
   "en": "(A) Deploy BIND DNS servers on Compute Engine and configure Conditional Forwarders pointing to on-premises DNS.",
   "wg": [
    {
     "t": "BIND DNS 伺服器",
     "en": "BIND DNS servers",
     "ps": "noun"
    },
    {
     "t": "條件轉發器",
     "en": "Conditional Forwarders",
     "ps": "feature"
    }
   ]
  },
  {
   "t": "(B) 設定 Cloud DNS 轉發區域 (Forwarding Zones)：建立『入站轉發』(Inbound) 供地端查詢雲端，以及『出站轉發』(Outbound) 供雲端查詢地端。",
   "en": "(B) Configure Cloud DNS Forwarding Zones: create 'Inbound' forwarding for on-prem to query cloud, and 'Outbound' forwarding for cloud to query on-prem.",
   "wg": [
    {
     "t": "轉發區域",
     "en": "Forwarding Zones",
     "ps": "feature"
    },
    {
     "t": "入站/出站",
     "en": "Inbound/Outbound",
     "ps": "adjective"
    }
   ]
  },
  {
   "t": "(C) 使用私有 Google 存取權 (Private Google Access) 讓地端直接查詢 metadata server (169.254.169.254) 來解析名稱。",
   "en": "(C) Use Private Google Access to allow on-premises systems to directly query the metadata server (169.254.169.254) for name resolution.",
   "wg": [
    {
     "t": "私有 Google 存取權",
     "en": "Private Google Access",
     "ps": "feature"
    },
    {
     "t": "metadata server",
     "en": "metadata server",
     "ps": "component"
    }
   ]
  },
  {
   "t": "(D) 將地端的 DNS 區域檔案 (Zone files) 匯出並匯入至 Cloud DNS，並設定排程腳本每小時同步一次。",
   "en": "(D) Export on-premises DNS Zone files and import them into Cloud DNS, setting up a scheduled script to sync hourly.",
   "wg": [
    {
     "t": "區域檔案",
     "en": "Zone files",
     "ps": "noun"
    },
    {
     "t": "同步",
     "en": "sync",
     "ps": "verb"
    }
   ]
  }
 ],
 "answer": "(B)",
 "why": {
  "t": "Cloud DNS 的『轉發區域』(Forwarding Zones) 是 Google Cloud 原生的混合雲 DNS 解決方案。設定『入站轉發』(Inbound Policy) 會提供一組內部 IP，讓地端 DNS 伺服器將查詢轉發過來；設定『出站轉發』(Outbound Zone) 則允許 VPC 內的 VM 將特定網域的查詢轉發給地端 DNS。這是全託管服務，符合『不維護額外伺服器』的需求。選項 (A) 需要管理 VM (IaaS)，增加維運負擔。選項 (C) PGA 用於存取 Google API，metadata server 無法從地端直接存取。選項 (D) 透過檔案同步不僅延遲高，且管理複雜，容易出錯。",
  "en": "Cloud DNS 'Forwarding Zones' is Google Cloud's native hybrid DNS solution. Configuring an 'Inbound Policy' provides internal IPs for on-prem DNS servers to forward queries to; configuring an 'Outbound Zone' allows VMs in the VPC to forward queries for specific domains to on-prem DNS. This is a fully managed service, meeting the 'no extra servers' requirement. Option (A) requires managing VMs (IaaS), adding operational overhead. Option (C) PGA is for accessing Google APIs, and the metadata server cannot be accessed directly from on-prem. Option (D) File syncing has high latency, is complex to manage, and prone to errors.",
  "wg": [
   {
    "t": "原生的混合雲 DNS",
    "en": "native hybrid DNS",
    "ps": "solution"
   },
   {
    "t": "全託管服務",
    "en": "fully managed service",
    "ps": "noun"
   }
  ]
 }
},
{
 "no": "27",
 "level": "hard",
 "keywords": "BigQuery, Performance, Cost Optimization, Partitioning",
 "question": [
  {
   "t": "TerramEarth 的資料分析師每天需處理高達 50 TB 的 IoT 遙測數據。查詢模式通常是針對特定日期範圍內的資料，並根據『設備 ID』(DeviceID) 進行篩選或聚合。",
   "en": "TerramEarth's data analysts process up to 50 TB of IoT telemetry data daily. Query patterns typically target data within a specific date range and filter or aggregate based on 'DeviceID'.",
   "wg": [
    {
     "t": "特定日期範圍",
     "en": "specific date range",
     "ps": "phrase"
    },
    {
     "t": "篩選或聚合",
     "en": "filter or aggregate",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "目前的 BigQuery 成本過高且查詢速度不如預期。您需要優化資料表結構以減少掃描的資料量 (Data Scanned) 並提升效能。",
   "en": "Current BigQuery costs are excessive, and query speed is suboptimal. You need to optimize the table structure to reduce the amount of Data Scanned and improve performance.",
   "wg": [
    {
     "t": "資料表結構",
     "en": "table structure",
     "ps": "noun"
    },
    {
     "t": "掃描的資料量",
     "en": "Data Scanned",
     "ps": "metric"
    }
   ]
  },
  {
   "t": "您應該如何定義 BigQuery 的 Table Schema？",
   "en": "How should you define the BigQuery Table Schema?",
   "wg": [
    {
     "t": "定義",
     "en": "define",
     "ps": "verb"
    },
    {
     "t": "Table Schema",
     "en": "Table Schema",
     "ps": "noun"
    }
   ]
  }
 ],
 "type": "單選題",
 "options": [
  {
   "t": "(A) 根據『設備 ID』進行分區 (Partition)，並根據『交易時間』(Transaction Time) 進行叢集 (Cluster)。",
   "en": "(A) Partition by 'DeviceID' and Cluster by 'Transaction Time'.",
   "wg": [
    {
     "t": "分區",
     "en": "Partition",
     "ps": "feature"
    },
    {
     "t": "叢集",
     "en": "Cluster",
     "ps": "feature"
    }
   ]
  },
  {
   "t": "(B) 根據『交易時間』(Transaction Time) 進行分區 (Partition)，並根據『設備 ID』進行叢集 (Cluster)。",
   "en": "(B) Partition by 'Transaction Time' and Cluster by 'DeviceID'.",
   "wg": [
    {
     "t": "交易時間",
     "en": "Transaction Time",
     "ps": "noun"
    },
    {
     "t": "設備 ID",
     "en": "DeviceID",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(C) 為『設備 ID』欄位建立搜尋索引 (Search Index)，並將資料表設定為外部資料表 (External Table) 連結至 Cloud Storage。",
   "en": "(C) Create a Search Index for the 'DeviceID' field and configure the table as an External Table linked to Cloud Storage.",
   "wg": [
    {
     "t": "搜尋索引",
     "en": "Search Index",
     "ps": "feature"
    },
    {
     "t": "外部資料表",
     "en": "External Table",
     "ps": "feature"
    }
   ]
  },
  {
   "t": "(D) 依據日期手動建立分片資料表 (例如 `telemetry_20240101`)，並使用萬用字元 (Wildcard) 查詢。",
   "en": "(D) Manually create sharded tables based on date (e.g., `telemetry_20240101`) and use wildcard queries.",
   "wg": [
    {
     "t": "分片資料表",
     "en": "sharded tables",
     "ps": "noun"
    },
    {
     "t": "萬用字元",
     "en": "wildcard",
     "ps": "noun"
    }
   ]
  }
 ],
 "answer": "(B)",
 "why": {
  "t": "BigQuery 的最佳實踐是根據查詢模式中最常使用的篩選條件來設計分區與叢集。題目指出查詢『針對特定日期範圍』，因此應優先使用『時間』(Transaction Time) 進行分區 (Partitioning)，這能直接修剪 (Prune) 不在時間範圍內的資料，大幅降低成本。其次，查詢會根據『設備 ID』篩選，因此將 DeviceID 設為叢集鍵 (Clustering Key) 能將相同設備的資料物理上排在一起，進一步減少掃描量。選項 (A) 分區數量有限制 (4000)，DeviceID 數量過多 (2000 萬) 不適合做分區鍵。選項 (D) 分片表是舊做法，效能與管理性皆不如分區表。",
  "en": "BigQuery best practice dictates designing partitioning and clustering based on the most frequent query filters. The question states queries target a 'specific date range', so partitioning by 'Transaction Time' should be the priority, as it directly prunes data outside the range, drastically reducing costs. Secondly, since queries filter by 'DeviceID', setting DeviceID as the Clustering Key physically colocates data for the same device, further reducing scanning. Option (A) Partition limits (4000) make DeviceID (20 million) unsuitable as a partition key. Option (D) Sharded tables are a legacy practice, inferior in performance and manageability to partitioned tables.",
  "wg": [
   {
    "t": "最佳實踐",
    "en": "best practice",
    "ps": "noun"
   },
   {
    "t": "修剪",
    "en": "Prune",
    "ps": "verb"
   }
  ]
 }
},
{
 "no": "28",
 "level": "hard",
 "keywords": "Security, Supply Chain, Binary Authorization, GKE",
 "question": [
  {
   "t": "Mountkirk Games 的 DevOps 團隊已建立了自動化的 CI/CD 流程。為了防止被入侵的供應鏈攻擊，安全長要求：GKE 叢集上『絕對禁止』部署任何未經 CI Pipeline 掃描與簽署的容器映像檔。",
   "en": "Mountkirk Games' DevOps team has established an automated CI/CD pipeline. To prevent compromised supply chain attacks, the CISO requires that: deploying any container images not scanned and signed by the CI pipeline to the GKE cluster must be 'strictly prohibited'.",
   "wg": [
    {
     "t": "供應鏈攻擊",
     "en": "supply chain attacks",
     "ps": "noun"
    },
    {
     "t": "絕對禁止",
     "en": "strictly prohibited",
     "ps": "phrase"
    }
   ]
  },
  {
   "t": "您需要實作一個強制性的控制措施，確保只有通過 Attestor 驗證的映像檔才能被啟動。若部署不合規的映像檔，Kubernetes API 應直接拒絕請求。",
   "en": "You need to implement a mandatory control ensuring only images verified by an Attestor can be launched. If a non-compliant image is deployed, the Kubernetes API should reject the request directly.",
   "wg": [
    {
     "t": "Attestor 驗證",
     "en": "verified by an Attestor",
     "ps": "process"
    },
    {
     "t": "直接拒絕",
     "en": "reject ... directly",
     "ps": "verb"
    }
   ]
  }
 ],
 "type": "單選題",
 "options": [
  {
   "t": "(A) 在 Artifact Registry 中啟用弱點掃描，並設定 IAM 策略，禁止開發者讀取有高風險漏洞的映像檔。",
   "en": "(A) Enable vulnerability scanning in Artifact Registry and configure IAM policies to prevent developers from reading images with high-risk vulnerabilities.",
   "wg": [
    {
     "t": "弱點掃描",
     "en": "vulnerability scanning",
     "ps": "feature"
    },
    {
     "t": "禁止讀取",
     "en": "prevent ... reading",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "(B) 設定 Binary Authorization 策略，要求所有映像檔必須具備特定的 Attestation 簽章，並將 GKE 叢集設定為強制執行 (Enforce) 模式。",
   "en": "(B) Configure a Binary Authorization policy requiring all images to have a specific Attestation signature, and set the GKE cluster to Enforce mode.",
   "wg": [
    {
     "t": "Binary Authorization",
     "en": "Binary Authorization",
     "ps": "product"
    },
    {
     "t": "強制執行模式",
     "en": "Enforce mode",
     "ps": "setting"
    }
   ]
  },
  {
   "t": "(C) 部署 Open Policy Agent (OPA) Gatekeeper，並撰寫 Rego 規則來檢查映像檔的來源 Repository 是否在白名單中。",
   "en": "(C) Deploy Open Policy Agent (OPA) Gatekeeper and write Rego rules to check if the image source Repository is in the allowlist.",
   "wg": [
    {
     "t": "Gatekeeper",
     "en": "Gatekeeper",
     "ps": "tool"
    },
    {
     "t": "白名單",
     "en": "allowlist",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(D) 使用 Cloud Build 的建置觸發條件 (Build Triggers)，在偵測到 Dockerfile 修改時自動執行測試，失敗則不推送到 Registry。",
   "en": "(D) Use Cloud Build Triggers to automatically run tests when Dockerfile changes are detected, and do not push to Registry on failure.",
   "wg": [
    {
     "t": "建置觸發條件",
     "en": "Build Triggers",
     "ps": "feature"
    },
    {
     "t": "自動執行",
     "en": "automatically run",
     "ps": "verb"
    }
   ]
  }
 ],
 "answer": "(B)",
 "why": {
  "t": "Binary Authorization 是 Google Cloud 原生的軟體供應鏈安全服務，專門用於 GKE。它透過『Attestation』(簽署證明) 機制，確保映像檔在部署前已通過 CI/CD 的測試與掃描。設定為『強制執行』(Enforce) 模式後，任何缺乏有效簽章的映像檔部署請求都會被 API Server 拒絕，完全符合題目的嚴格要求。選項 (A) 僅限制讀取，無法防止已被下載或快取的映像檔運行。選項 (C) Gatekeeper 雖然強大，但 Binary Authorization 是針對 Google Cloud 生態系整合更深的原生解決方案。選項 (D) 僅是 CI 階段的控制，無法防止有人繞過 CI 直接部署惡意映像檔。",
  "en": "Binary Authorization is Google Cloud's native software supply chain security service for GKE. It uses the 'Attestation' mechanism to ensure images have passed CI/CD tests and scans before deployment. When set to 'Enforce' mode, any deployment request for an image lacking a valid signature is rejected by the API Server, meeting the strict requirement. Option (A) only restricts reading and cannot stop cached images from running. Option (C) Gatekeeper is powerful, but Binary Authorization is the native solution deeply integrated into the Google Cloud ecosystem. Option (D) controls only the CI stage and cannot prevent someone from bypassing CI and deploying malicious images directly.",
  "wg": [
   {
    "t": "軟體供應鏈安全",
    "en": "software supply chain security",
    "ps": "noun"
   },
   {
    "t": "簽署證明",
    "en": "Attestation",
    "ps": "term"
   }
  ]
 }
},
{
 "no": "29",
 "level": "hard",
 "keywords": "Disaster Recovery, Database, SQL Server, Compute Engine",
 "question": [
  {
   "t": "EHR Healthcare 有一個關鍵的病患管理系統，運行在 Compute Engine 的 Windows Server VM 上，使用 Microsoft SQL Server 作為資料庫。",
   "en": "EHR Healthcare has a critical patient management system running on Windows Server VMs on Compute Engine, using Microsoft SQL Server as the database.",
   "wg": [
    {
     "t": "關鍵的",
     "en": "critical",
     "ps": "adjective"
    },
    {
     "t": "Microsoft SQL Server",
     "en": "Microsoft SQL Server",
     "ps": "product"
    }
   ]
  },
  {
   "t": "業務持續性計畫 (BCP) 要求當主要區域發生災難時，系統必須在 4 小時內於次要區域恢復運作 (RTO < 4 小時)，且資料遺失不得超過 15 分鐘 (RPO < 15 分鐘)。",
   "en": "The Business Continuity Plan (BCP) requires that in the event of a primary region disaster, the system must recover in a secondary region within 4 hours (RTO < 4 hours), with data loss not exceeding 15 minutes (RPO < 15 minutes).",
   "wg": [
    {
     "t": "業務持續性計畫",
     "en": "Business Continuity Plan",
     "ps": "term"
    },
    {
     "t": "資料遺失",
     "en": "data loss",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "您希望選擇一個最具成本效益且符合 RPO 的基礎架構層級解決方案，盡量減少資料庫層級的複雜配置。",
   "en": "You want to choose the most cost-effective infrastructure-level solution that meets the RPO, minimizing complex configuration at the database level.",
   "wg": [
    {
     "t": "最具成本效益",
     "en": "most cost-effective",
     "ps": "adjective"
    },
    {
     "t": "基礎架構層級",
     "en": "infrastructure-level",
     "ps": "adjective"
    }
   ]
  }
 ],
 "type": "單選題",
 "options": [
  {
   "t": "(A) 使用 Persistent Disk (PD) 的非同步複製 (Asynchronous Replication) 功能，將磁碟資料持續複製到次要區域。",
   "en": "(A) Use Persistent Disk (PD) Asynchronous Replication to continuously replicate disk data to the secondary region.",
   "wg": [
    {
     "t": "Persistent Disk",
     "en": "Persistent Disk",
     "ps": "product"
    },
    {
     "t": "非同步複製",
     "en": "Asynchronous Replication",
     "ps": "feature"
    }
   ]
  },
  {
   "t": "(B) 設定 SQL Server Always On Availability Groups，並在次要區域部署一個非同步的複本節點 (Async Replica)。",
   "en": "(B) Configure SQL Server Always On Availability Groups with an asynchronous replica node deployed in the secondary region.",
   "wg": [
    {
     "t": "Always On Availability Groups",
     "en": "Always On Availability Groups",
     "ps": "feature"
    },
    {
     "t": "複本節點",
     "en": "replica node",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(C) 每 24 小時建立一次機器映像檔 (Machine Image)，並將其複製到次要區域進行備存。",
   "en": "(C) Create a Machine Image every 24 hours and copy it to the secondary region for backup.",
   "wg": [
    {
     "t": "機器映像檔",
     "en": "Machine Image",
     "ps": "feature"
    },
    {
     "t": "備存",
     "en": "backup",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(D) 使用 Cloud Storage Fuse 將資料庫檔案直接寫入 Multi-regional Bucket，以確保資料同時存在於多個區域。",
   "en": "(D) Use Cloud Storage Fuse to write database files directly to a Multi-regional Bucket, ensuring data exists in multiple regions simultaneously.",
   "wg": [
    {
     "t": "Cloud Storage Fuse",
     "en": "Cloud Storage Fuse",
     "ps": "tool"
    },
    {
     "t": "同時存在",
     "en": "simultaneously",
     "ps": "adverb"
    }
   ]
  }
 ],
 "answer": "(A)",
 "why": {
  "t": "題目要求『基礎架構層級』解決方案並『減少資料庫複雜配置』，同時滿足 RPO < 15 分鐘。Compute Engine Persistent Disk (PD) 的『非同步複製』(Async Replication) 是專為跨區域災難復原設計的功能，它能達到接近 0 到 1 分鐘的 RPO，遠優於題目要求的 15 分鐘，且無需在 SQL Server 內部設定複雜的 Always On AG (這需要 Enterprise 版授權，成本較高)。選項 (B) 雖然技術可行，但屬於資料庫層級配置且成本較高。選項 (C) 24 小時備份無法滿足 RPO 15 分鐘。選項 (D) GCS Fuse 效能極差且不支援資料庫所需的鎖定機制，會導致資料毀損。",
  "en": "The question asks for an 'infrastructure-level' solution minimizing 'complex database configuration' while meeting RPO < 15 mins. Compute Engine Persistent Disk (PD) 'Asynchronous Replication' is a feature designed for cross-region DR, achieving an RPO of near 0 to 1 minute, far better than the 15-minute requirement, without configuring complex Always On AG inside SQL Server (which requires Enterprise licensing and higher cost). Option (B) is technically feasible but is a database-level config with higher costs. Option (C) 24-hour backups fail the 15-minute RPO. Option (D) GCS Fuse has poor performance and lacks locking mechanisms required by databases, leading to corruption.",
  "wg": [
   {
    "t": "跨區域災難復原",
    "en": "cross-region disaster recovery",
    "ps": "noun"
   },
   {
    "t": "基礎架構層級",
    "en": "infrastructure-level",
    "ps": "adjective"
   }
  ]
 }
},
{
 "no": "30",
 "level": "hard",
 "keywords": "Governance, Org Policy, IAM, Security",
 "question": [
  {
   "t": "HRL 正準備進行 IPO，合規團隊要求對 Google Cloud 環境實施更嚴格的管控。目前存在兩個主要風險：1. 開發者可能會在非合規區域 (如亞洲以外) 建立資源；2. 開發者可能會下載服務帳戶金鑰 (Service Account Keys) 到個人筆電，導致憑證洩漏。",
   "en": "HRL is preparing for an IPO, and the compliance team requires stricter controls on the Google Cloud environment. Two major risks exist: 1. Developers might create resources in non-compliant regions (e.g., outside Asia); 2. Developers might download Service Account Keys to personal laptops, leading to credential leakage.",
   "wg": [
    {
     "t": "合規區域",
     "en": "compliant regions",
     "ps": "noun"
    },
    {
     "t": "憑證洩漏",
     "en": "credential leakage",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "您需要在組織層級實施預防性控制 (Preventative Controls) 來消除這兩個風險。請選擇兩項設定。",
   "en": "You need to implement Preventative Controls at the organization level to eliminate these two risks. Choose two settings.",
   "wg": [
    {
     "t": "預防性控制",
     "en": "Preventative Controls",
     "ps": "term"
    },
    {
     "t": "消除",
     "en": "eliminate",
     "ps": "verb"
    }
   ]
  }
 ],
 "type": "複選題",
 "options": [
  {
   "t": "(A) 設定組織政策約束條件 `gcp.resourceLocations`，並將允許的地區限制在亞洲特定區域。",
   "en": "(A) Configure Organization Policy constraint `gcp.resourceLocations` and restrict allowed locations to specific Asian regions.",
   "wg": [
    {
     "t": "組織政策約束條件",
     "en": "Organization Policy constraint",
     "ps": "feature"
    },
    {
     "t": "限制",
     "en": "restrict",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "(B) 設定組織政策約束條件 `iam.disableServiceAccountKeyCreation` 為 True，以禁止建立新的金鑰。",
   "en": "(B) Configure Organization Policy constraint `iam.disableServiceAccountKeyCreation` to True to prohibit creating new keys.",
   "wg": [
    {
     "t": "禁止",
     "en": "prohibit",
     "ps": "verb"
    },
    {
     "t": "建立新的金鑰",
     "en": "creating new keys",
     "ps": "phrase"
    }
   ]
  },
  {
   "t": "(C) 使用 VPC Service Controls 建立服務邊界，將所有開發者的 IP 加入拒絕清單。",
   "en": "(C) Use VPC Service Controls to create a service perimeter and add all developer IPs to the deny list.",
   "wg": [
    {
     "t": "服務邊界",
     "en": "service perimeter",
     "ps": "noun"
    },
    {
     "t": "拒絕清單",
     "en": "deny list",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(D) 在 IAM 權限中移除所有使用者的 `resourcemanager.projects.create` 權限。",
   "en": "(D) Remove the `resourcemanager.projects.create` permission from all users in IAM.",
   "wg": [
    {
     "t": "移除",
     "en": "Remove",
     "ps": "verb"
    },
    {
     "t": "權限",
     "en": "permission",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(E) 部署 Cloud DLP 來掃描 GitHub Repositories，確保沒有金鑰被提交到程式碼庫中。",
   "en": "(E) Deploy Cloud DLP to scan GitHub Repositories to ensure no keys are committed to the codebase.",
   "wg": [
    {
     "t": "掃描",
     "en": "scan",
     "ps": "verb"
    },
    {
     "t": "提交",
     "en": "committed",
     "ps": "verb"
    }
   ]
  }
 ],
 "answer": "(A), (B)",
 "why": {
  "t": "組織政策 (Organization Policy) 是實施全域『預防性控制』的最佳工具。針對風險 1，`gcp.resourceLocations` 約束條件可以強制限定資源只能建立在白名單允許的區域 (如亞洲)，任何嘗試在其他區域建立資源的請求都會失敗。針對風險 2，`iam.disableServiceAccountKeyCreation` 可以完全禁止建立外部的 JSON 金鑰檔，迫使開發者使用更安全的方法 (如 Workload Identity)。選項 (C) VPC SC 主要防資料外洩，不防資源建立位置。選項 (D) 限制專案建立並不能解決資源位置與金鑰問題。選項 (E) DLP 掃描屬於偵測性 (Detective) 控制，而非預防性 (Preventative)。",
  "en": "Organization Policy is the best tool for implementing global 'Preventative Controls'. For Risk 1, the `gcp.resourceLocations` constraint enforces that resources can only be created in allowlisted regions (e.g., Asia); attempts to create resources elsewhere will fail. For Risk 2, `iam.disableServiceAccountKeyCreation` completely prohibits the creation of external JSON key files, forcing developers to use safer methods (like Workload Identity). Option (C) VPC SC prevents data exfiltration, not resource location. Option (D) restricting project creation doesn't solve location or key issues. Option (E) DLP scanning is a Detective control, not Preventative.",
  "wg": [
   {
    "t": "預防性控制",
    "en": "Preventative Controls",
    "ps": "term"
   },
   {
    "t": "偵測性控制",
    "en": "Detective control",
    "ps": "term"
   }
  ]
 }
},
{
 "no": "31",
 "level": "hard",
 "keywords": "Security, GKE, Workload Identity, IAM",
 "question": [
  {
   "t": "Mountkirk Games 的新遊戲後端運行在 Google Kubernetes Engine (GKE) 上，需要存取 BigQuery 進行玩家行為數據的寫入。安全稽核發現，目前的 Pods 是掛載長效的 Service Account JSON 金鑰檔案來通過驗證。",
   "en": "Mountkirk Games' new game backend runs on Google Kubernetes Engine (GKE) and needs to access BigQuery to write player behavior data. Security audits revealed that current Pods are mounting long-lived Service Account JSON key files for authentication.",
   "wg": [
    {
     "t": "長效的",
     "en": "long-lived",
     "ps": "adjective"
    },
    {
     "t": "掛載",
     "en": "mounting",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "這種做法存在金鑰洩漏與輪替困難的風險。您需要重新設計驗證機制，消除對靜態金鑰檔案的依賴，並確保每個微服務只能存取其所需的特定資源。",
   "en": "This practice poses risks of key leakage and rotation difficulties. You need to redesign the authentication mechanism to eliminate reliance on static key files and ensure each microservice can only access the specific resources it needs.",
   "wg": [
    {
     "t": "靜態金鑰檔案",
     "en": "static key files",
     "ps": "noun"
    },
    {
     "t": "消除",
     "en": "eliminate",
     "ps": "verb"
    }
   ]
  }
 ],
 "type": "單選題",
 "options": [
  {
   "t": "(A) 將 Service Account 金鑰儲存在 Kubernetes Secrets 中，並在 Pod 啟動時透過環境變數注入。",
   "en": "(A) Store Service Account keys in Kubernetes Secrets and inject them via environment variables when the Pod starts.",
   "wg": [
    {
     "t": "Kubernetes Secrets",
     "en": "Kubernetes Secrets",
     "ps": "feature"
    },
    {
     "t": "環境變數",
     "en": "environment variables",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(B) 為 GKE 節點 (Nodes) 的服務帳戶賦予 BigQuery 寫入權限，並讓所有 Pods 繼承節點的權限。",
   "en": "(B) Grant BigQuery write permissions to the GKE Nodes' service account, allowing all Pods to inherit permissions from the node.",
   "wg": [
    {
     "t": "GKE 節點",
     "en": "GKE Nodes",
     "ps": "noun"
    },
    {
     "t": "繼承",
     "en": "inherit",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "(C) 啟用 Workload Identity，建立 Kubernetes Service Account (KSA) 與 Google Service Account (GSA) 之間的綁定關係。",
   "en": "(C) Enable Workload Identity and create a binding relationship between the Kubernetes Service Account (KSA) and the Google Service Account (GSA).",
   "wg": [
    {
     "t": "Workload Identity",
     "en": "Workload Identity",
     "ps": "feature"
    },
    {
     "t": "綁定關係",
     "en": "binding relationship",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(D) 使用 Compute Engine Metadata Server，在應用程式程式碼中直接呼叫 API 獲取暫時性 Access Token。",
   "en": "(D) Use the Compute Engine Metadata Server to verify directly in the application code by calling the API to obtain a temporary Access Token.",
   "wg": [
    {
     "t": "Metadata Server",
     "en": "Metadata Server",
     "ps": "component"
    },
    {
     "t": "暫時性",
     "en": "temporary",
     "ps": "adjective"
    }
   ]
  }
 ],
 "answer": "(C)",
 "why": {
  "t": "Workload Identity 是 GKE 上存取 Google Cloud API 最安全且推薦的標準做法。它允許將 Kubernetes 的服務帳戶 (KSA) 映射到 Google 的 IAM 服務帳戶 (GSA)，無需管理或掛載任何實體金鑰檔案，徹底消除了金鑰洩漏風險。選項 (A) 雖然比直接放在映像檔好，但本質上仍使用了靜態金鑰，需要手動輪替。選項 (B) 違反最小權限原則，因為該節點上的所有 Pods 都會取得相同的權限。選項 (D) 是舊式做法，且無法區分同一節點上不同 Pod 的身分。",
  "en": "Workload Identity is the most secure and recommended standard practice for accessing Google Cloud APIs on GKE. It allows mapping Kubernetes Service Accounts (KSA) to Google IAM Service Accounts (GSA) without managing or mounting any physical key files, completely eliminating key leakage risks. Option (A) is better than baking into images but still uses static keys requiring manual rotation. Option (B) violates the principle of least privilege as all Pods on that node would inherit the same permissions. Option (D) is a legacy approach and cannot distinguish identities of different Pods on the same node.",
  "wg": [
   {
    "t": "推薦的標準做法",
    "en": "recommended standard practice",
    "ps": "noun"
   },
   {
    "t": "靜態金鑰",
    "en": "static keys",
    "ps": "noun"
   }
  ]
 }
},
{
 "no": "32",
 "level": "hard",
 "keywords": "Migration, Data Warehouse, BigQuery, Oracle",
 "question": [
  {
   "t": "TerramEarth 目前使用地端的 Oracle Exadata 運行其資料倉儲，包含了數 PB 的歷史資料以及大量複雜的 PL/SQL 預存程序 (Stored Procedures)。",
   "en": "TerramEarth currently runs its data warehouse on an on-premises Oracle Exadata, containing petabytes of historical data and a large number of complex PL/SQL Stored Procedures.",
   "wg": [
    {
     "t": "資料倉儲",
     "en": "data warehouse",
     "ps": "noun"
    },
    {
     "t": "預存程序",
     "en": "Stored Procedures",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "CIO 決定將此工作負載遷移至 Google Cloud，以降低授權費用並提升分析擴展性。您需要選擇一個目標服務，並規劃如何處理既有的 SQL 邏輯以最小化改寫工作。",
   "en": "The CIO has decided to migrate this workload to Google Cloud to reduce licensing costs and improve analytics scalability. You need to select a target service and plan how to handle existing SQL logic to minimize rewriting efforts.",
   "wg": [
    {
     "t": "授權費用",
     "en": "licensing costs",
     "ps": "noun"
    },
    {
     "t": "改寫工作",
     "en": "rewriting efforts",
     "ps": "noun"
    }
   ]
  }
 ],
 "type": "單選題",
 "options": [
  {
   "t": "(A) 遷移至 Cloud Spanner，因為它支援強一致性與標準 SQL，適合取代企業級關聯式資料庫。",
   "en": "(A) Migrate to Cloud Spanner, as it supports strong consistency and standard SQL, making it suitable for replacing enterprise relational databases.",
   "wg": [
    {
     "t": "Cloud Spanner",
     "en": "Cloud Spanner",
     "ps": "product"
    },
    {
     "t": "強一致性",
     "en": "strong consistency",
     "ps": "feature"
    }
   ]
  },
  {
   "t": "(B) 遷移至 BigQuery，並使用 BigQuery Migration Service 的 SQL Translation 功能來轉換 PL/SQL 腳本。",
   "en": "(B) Migrate to BigQuery and use the SQL Translation feature of BigQuery Migration Service to convert PL/SQL scripts.",
   "wg": [
    {
     "t": "BigQuery Migration Service",
     "en": "BigQuery Migration Service",
     "ps": "product"
    },
    {
     "t": "轉換",
     "en": "convert",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "(C) 遷移至 Bare Metal Solution for Oracle，直接將 Exadata 搬移至 Google 資料中心以避免更改任何程式碼。",
   "en": "(C) Migrate to Bare Metal Solution for Oracle, lifting and shifting Exadata directly to Google data centers to avoid changing any code.",
   "wg": [
    {
     "t": "Bare Metal Solution",
     "en": "Bare Metal Solution",
     "ps": "product"
    },
    {
     "t": "避免更改",
     "en": "avoid changing",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "(D) 遷移至 Cloud SQL for PostgreSQL，並使用 pgLoader 工具將 Oracle schema 轉換為 PostgreSQL 格式。",
   "en": "(D) Migrate to Cloud SQL for PostgreSQL and use the pgLoader tool to convert the Oracle schema to PostgreSQL format.",
   "wg": [
    {
     "t": "Cloud SQL",
     "en": "Cloud SQL",
     "ps": "product"
    },
    {
     "t": "轉換",
     "en": "convert",
     "ps": "verb"
    }
   ]
  }
 ],
 "answer": "(B)",
 "why": {
  "t": "題目明確指出是用於『資料倉儲』(Data Warehouse) 且資料量達『PB 級』，這直接指向 BigQuery 作為目標服務。針對『最小化 PL/SQL 改寫工作』，BigQuery Migration Service 提供了專門的 SQL Translation 功能 (編譯器)，能自動將大部分 Oracle SQL/PLSQL 轉換為 BigQuery GoogleSQL，大幅降低遷移門檻。選項 (A) Spanner 是 OLTP 資料庫，成本高且不適合大規模 OLAP 分析。選項 (C) Bare Metal Solution 雖然可避免改碼，但無法達成 CIO 『降低授權費用』(Oracle 授權極貴) 與『提升分析擴展性』(Serverless) 的目標。選項 (D) Cloud SQL 難以處理 PB 級資料倉儲。",
  "en": "The question explicitly mentions 'Data Warehouse' and 'Petabyte-scale' data, pointing directly to BigQuery as the target service. To 'minimize PL/SQL rewriting', BigQuery Migration Service offers a specialized SQL Translation feature (compiler) that automatically converts most Oracle SQL/PLSQL to BigQuery GoogleSQL, significantly lowering the migration barrier. Option (A) Spanner is an OLTP database, expensive and unsuitable for large-scale OLAP analytics. Option (C) Bare Metal Solution avoids code changes but fails the CIO's goals of 'reducing licensing costs' (Oracle licenses are expensive) and 'improving analytics scalability' (Serverless). Option (D) Cloud SQL struggles with PB-scale data warehousing.",
  "wg": [
   {
    "t": "資料倉儲",
    "en": "Data Warehouse",
    "ps": "noun"
   },
   {
    "t": "SQL Translation 功能",
    "en": "SQL Translation feature",
    "ps": "feature"
   }
  ]
 }
},
{
 "no": "33",
 "level": "hard",
 "keywords": "Security, Encryption, Cloud EKM, Compliance",
 "question": [
  {
   "t": "EHR Healthcare 處理的是高度敏感的病患個資。根據最新的資料主權法規，加密金鑰必須儲存在 Google Cloud 基礎設施之外的第三方硬體安全模組 (HSM) 中，且 Google 絕不能擁有金鑰的存取權。",
   "en": "EHR Healthcare handles highly sensitive patient PII. According to the latest data sovereignty regulations, encryption keys must be stored in a third-party Hardware Security Module (HSM) outside of Google Cloud infrastructure, and Google must never have access to the key material.",
   "wg": [
    {
     "t": "第三方硬體安全模組",
     "en": "third-party Hardware Security Module",
     "ps": "noun"
    },
    {
     "t": "金鑰的存取權",
     "en": "access to the key material",
     "ps": "phrase"
    }
   ]
  },
  {
   "t": "同時，應用程式需要在資料上傳至雲端之前就先進行加密 (Client-side Encryption)，以確保傳輸過程中的絕對安全。請選擇兩項技術來滿足這些需求。",
   "en": "Additionally, the application needs to encrypt data before uploading it to the cloud (Client-side Encryption) to ensure absolute security during transmission. Choose two technologies to meet these requirements.",
   "wg": [
    {
     "t": "客戶端加密",
     "en": "Client-side Encryption",
     "ps": "term"
    },
    {
     "t": "傳輸過程",
     "en": "transmission",
     "ps": "noun"
    }
   ]
  }
 ],
 "type": "複選題",
 "options": [
  {
   "t": "(A) 使用 Cloud External Key Manager (Cloud EKM) 來連結地端的第三方 HSM。",
   "en": "(A) Use Cloud External Key Manager (Cloud EKM) to connect to an on-premises third-party HSM.",
   "wg": [
    {
     "t": "Cloud EKM",
     "en": "Cloud EKM",
     "ps": "product"
    },
    {
     "t": "連結",
     "en": "connect",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "(B) 使用 Google Cloud 原生的 Cloud HSM 服務來產生並儲存金鑰。",
   "en": "(B) Use Google Cloud's native Cloud HSM service to generate and store keys.",
   "wg": [
    {
     "t": "Cloud HSM",
     "en": "Cloud HSM",
     "ps": "product"
    },
    {
     "t": "產生",
     "en": "generate",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "(C) 在應用程式中整合 Tink 加密程式庫，並使用封包加密 (Envelope Encryption) 模式。",
   "en": "(C) Integrate the Tink encryption library into the application and use the Envelope Encryption pattern.",
   "wg": [
    {
     "t": "Tink 加密程式庫",
     "en": "Tink encryption library",
     "ps": "tool"
    },
    {
     "t": "封包加密",
     "en": "Envelope Encryption",
     "ps": "term"
    }
   ]
  },
  {
   "t": "(D) 使用 VPC Service Controls 來確保金鑰不會離開 Google 網路。",
   "en": "(D) Use VPC Service Controls to ensure keys do not leave the Google network.",
   "wg": [
    {
     "t": "VPC Service Controls",
     "en": "VPC Service Controls",
     "ps": "product"
    },
    {
     "t": "離開",
     "en": "leave",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "(E) 啟用 Google 預設加密 (Google-managed default encryption) 並搭配透明資料加密 (TDE)。",
   "en": "(E) Enable Google-managed default encryption combined with Transparent Data Encryption (TDE).",
   "wg": [
    {
     "t": "Google 預設加密",
     "en": "Google-managed default encryption",
     "ps": "feature"
    },
    {
     "t": "透明資料加密",
     "en": "Transparent Data Encryption",
     "ps": "feature"
    }
   ]
  }
 ],
 "answer": "(A), (C)",
 "why": {
  "t": "題目有兩個核心需求：1. 金鑰必須在 Google 基礎設施之外 (External Key)；2. 資料必須在客戶端加密 (Client-side Encryption)。選項 (A) Cloud EKM 正是為了解決需求 1 設計的，它允許 Google Cloud 服務 (如 BigQuery, GCS) 在加密/解密時呼叫外部 HSM，Google 永遠看不到金鑰本身。選項 (C) Tink 是 Google 開發的跨平台加密庫，支援客戶端加密與封包加密模式，符合需求 2。選項 (B) Cloud HSM 的金鑰仍在 Google 資料中心內。選項 (D) VPC SC 與金鑰儲存位置無關。選項 (E) 使用 Google 管理金鑰完全違反題目要求。",
  "en": "The question has two core requirements: 1. Keys must be outside Google infrastructure (External Key); 2. Data must be encrypted client-side. Option (A) Cloud EKM is designed exactly for requirement 1, allowing Google Cloud services (like BigQuery, GCS) to call external HSMs for encryption/decryption without Google ever seeing the key material. Option (C) Tink is a cross-platform encryption library developed by Google, supporting client-side and envelope encryption, meeting requirement 2. Option (B) Cloud HSM keys are still within Google data centers. Option (D) VPC SC is unrelated to key storage location. Option (E) using Google-managed keys directly violates the requirements.",
  "wg": [
   {
    "t": "外部金鑰管理器",
    "en": "External Key Manager",
    "ps": "product"
   },
   {
    "t": "封包加密",
    "en": "Envelope Encryption",
    "ps": "concept"
   }
  ]
 }
},
{
 "no": "34",
 "level": "hard",
 "keywords": "Operations, Logging, Compliance, SIEM",
 "question": [
  {
   "t": "HRL 需要將所有的雲端稽核日誌 (Audit Logs) 匯出到地端的 Splunk SIEM 系統進行合規性分析。由於日誌量極大，直接透過 API 輪詢 (Polling) 會導致限流與延遲。",
   "en": "HRL needs to export all cloud Audit Logs to an on-premises Splunk SIEM system for compliance analysis. Due to the huge volume of logs, direct API polling would cause rate limiting and latency.",
   "wg": [
    {
     "t": "稽核日誌",
     "en": "Audit Logs",
     "ps": "noun"
    },
    {
     "t": "輪詢",
     "en": "Polling",
     "ps": "method"
    }
   ]
  },
  {
   "t": "您需要設計一個高可靠性、低延遲的匯出管道，並確保即使地端系統暫時離線，日誌也不會遺失 (具備緩衝能力)。",
   "en": "You need to design a highly reliable, low-latency export pipeline that ensures logs are not lost even if the on-premises system is temporarily offline (buffering capability).",
   "wg": [
    {
     "t": "匯出管道",
     "en": "export pipeline",
     "ps": "noun"
    },
    {
     "t": "緩衝能力",
     "en": "buffering capability",
     "ps": "noun"
    }
   ]
  }
 ],
 "type": "單選題",
 "options": [
  {
   "t": "(A) 設定 Log Router Sink 將日誌匯出至 Cloud Storage Bucket，並撰寫地端腳本定期下載這些檔案。",
   "en": "(A) Configure a Log Router Sink to export logs to a Cloud Storage Bucket, and write an on-premises script to download these files periodically.",
   "wg": [
    {
     "t": "Log Router Sink",
     "en": "Log Router Sink",
     "ps": "feature"
    },
    {
     "t": "定期下載",
     "en": "download ... periodically",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "(B) 設定 Log Router Sink 將日誌匯出至 BigQuery，並使用 Splunk 的 DB Connect 套件連接 BigQuery 進行查詢。",
   "en": "(B) Configure a Log Router Sink to export logs to BigQuery, and use Splunk's DB Connect add-on to query BigQuery.",
   "wg": [
    {
     "t": "BigQuery",
     "en": "BigQuery",
     "ps": "product"
    },
    {
     "t": "連接",
     "en": "connect",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "(C) 使用 Cloud Functions 監控日誌串流，並透過 HTTP POST 將每條日誌即時推送到 Splunk 的 HEC (HTTP Event Collector)。",
   "en": "(C) Use Cloud Functions to monitor the log stream and push each log in real-time to Splunk's HEC (HTTP Event Collector) via HTTP POST.",
   "wg": [
    {
     "t": "監控",
     "en": "monitor",
     "ps": "verb"
    },
    {
     "t": "即時推送",
     "en": "push ... in real-time",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "(D) 設定 Log Router Sink 將日誌發佈到 Pub/Sub Topic，並在地端部署 Splunk Add-on for Google Cloud 透過訂閱 (Subscription) 拉取日誌。",
   "en": "(D) Configure a Log Router Sink to publish logs to a Pub/Sub Topic, and deploy the Splunk Add-on for Google Cloud on-premises to pull logs via Subscription.",
   "wg": [
    {
     "t": "Pub/Sub Topic",
     "en": "Pub/Sub Topic",
     "ps": "product"
    },
    {
     "t": "拉取",
     "en": "pull",
     "ps": "verb"
    }
   ]
  }
 ],
 "answer": "(D)",
 "why": {
  "t": "這是 Google Cloud 推薦的標準 SIEM 整合模式。使用 Log Router Sink 將日誌導向 Pub/Sub，Pub/Sub 作為一個全託管的訊息緩衝區 (Buffer)，能吸收日誌峰值並保存訊息 (預設 7 天)，滿足『地端離線不掉資料』的需求。Splunk Add-on 則作為消費者 (Subscriber) 主動拉取 (Pull) 資料，效率遠高於 API 輪詢。選項 (A) GCS 是批次處理，延遲高且不即時。選項 (B) BigQuery 用於分析，作為日誌傳輸中介成本過高且延遲較高。選項 (C) Cloud Functions 直接推送缺乏緩衝機制，若 Splunk 離線或過載，資料會遺失或導致函數執行失敗。",
  "en": "This is the Google Cloud recommended standard pattern for SIEM integration. Using a Log Router Sink to direct logs to Pub/Sub allows Pub/Sub to act as a fully managed message buffer, absorbing log spikes and retaining messages (default 7 days), meeting the 'no data loss if on-prem is offline' requirement. The Splunk Add-on acts as a Subscriber to Pull data, which is far more efficient than API polling. Option (A) GCS is batch-oriented, high latency, and not real-time. Option (B) BigQuery is for analytics; using it as a transfer intermediary is costly and higher latency. Option (C) Direct push via Cloud Functions lacks buffering; if Splunk is offline or overloaded, data will be lost or functions will fail.",
  "wg": [
   {
    "t": "標準 SIEM 整合模式",
    "en": "standard SIEM integration pattern",
    "ps": "term"
   },
   {
    "t": "訊息緩衝區",
    "en": "message buffer",
    "ps": "noun"
   }
  ]
 }
},
{
 "no": "35",
 "level": "hard",
 "keywords": "Microservices, Reliability, Circuit Breaker, Istio",
 "question": [
  {
   "t": "HRL 的訂票系統由多個微服務組成。最近發現，當非關鍵的『推薦引擎』服務發生故障或回應變慢時，會導致前端的『訂票頁面』也跟著卡住並最終逾時，造成使用者無法購票。",
   "en": "HRL's booking system consists of multiple microservices. Recently, it was found that when the non-critical 'Recommendation Engine' service fails or responds slowly, it causes the frontend 'Booking Page' to hang and eventually timeout, preventing users from purchasing tickets.",
   "wg": [
    {
     "t": "非關鍵的",
     "en": "non-critical",
     "ps": "adjective"
    },
    {
     "t": "卡住",
     "en": "hang",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "這是典型的串聯故障 (Cascading Failure)。您需要在不修改應用程式程式碼的情況下，防止這種單點故障影響整體系統的可用性。",
   "en": "This is a typical Cascading Failure. You need to prevent this single point of failure from impacting the overall system availability without modifying the application code.",
   "wg": [
    {
     "t": "串聯故障",
     "en": "Cascading Failure",
     "ps": "term"
    },
    {
     "t": "可用性",
     "en": "availability",
     "ps": "noun"
    }
   ]
  }
 ],
 "type": "單選題",
 "options": [
  {
   "t": "(A) 在 GKE 叢集上部署 Istio (Anthos Service Mesh)，並對推薦引擎服務設定斷路器 (Circuit Breaker) 策略。",
   "en": "(A) Deploy Istio (Anthos Service Mesh) on the GKE cluster and configure a Circuit Breaker policy for the Recommendation Engine service.",
   "wg": [
    {
     "t": "Istio",
     "en": "Istio",
     "ps": "product"
    },
    {
     "t": "斷路器",
     "en": "Circuit Breaker",
     "ps": "pattern"
    }
   ]
  },
  {
   "t": "(B) 增加推薦引擎服務的 Horizontal Pod Autoscaler (HPA) 上限，使其能處理更多的負載。",
   "en": "(B) Increase the Horizontal Pod Autoscaler (HPA) limit for the Recommendation Engine service to handle more load.",
   "wg": [
    {
     "t": "HPA",
     "en": "HPA",
     "ps": "feature"
    },
    {
     "t": "負載",
     "en": "load",
     "ps": "noun"
    }
   ]
  },
  {
   "t": "(C) 在前端負載平衡器上設定更長的逾時時間 (Timeout)，讓慢速請求有更多時間完成。",
   "en": "(C) Configure a longer Timeout on the frontend load balancer to allow slow requests more time to complete.",
   "wg": [
    {
     "t": "逾時時間",
     "en": "Timeout",
     "ps": "setting"
    },
    {
     "t": "完成",
     "en": "complete",
     "ps": "verb"
    }
   ]
  },
  {
   "t": "(D) 實作指數退避 (Exponential Backoff) 重試機制在前端應用程式中。",
   "en": "(D) Implement an Exponential Backoff retry mechanism in the frontend application.",
   "wg": [
    {
     "t": "指數退避",
     "en": "Exponential Backoff",
     "ps": "strategy"
    },
    {
     "t": "重試機制",
     "en": "retry mechanism",
     "ps": "noun"
    }
   ]
  }
 ],
 "answer": "(A)",
 "why": {
  "t": "斷路器 (Circuit Breaker) 模式是防止串聯故障的標準解法。當下游服務 (推薦引擎) 錯誤率過高或延遲過長時，斷路器會『跳閘』(Open)，直接對上游回傳錯誤或預設值，而不再等待下游回應，從而保護了上游 (訂票頁面) 不被拖垮。由於題目限制『不修改程式碼』，使用 Istio Service Mesh 來宣告式地配置斷路器是最佳選擇。選項 (B) 擴展故障的服務可能無濟於事 (如果是 bug 造成的)。選項 (C) 增加逾時只會讓使用者等更久，惡化體驗。選項 (D) 重試機制在服務過載時反而會引發重試風暴 (Retry Storm)，加劇故障。",
  "en": "The Circuit Breaker pattern is the standard solution to prevent cascading failures. When a downstream service (Recommendation Engine) has high error rates or latency, the circuit breaker 'trips' (Open), immediately returning an error or default value to the upstream without waiting, thus protecting the upstream (Booking Page) from hanging. Since the question restricts 'no code modification', using Istio Service Mesh to declaratively configure circuit breakers is the best choice. Option (B) Scaling a failing service might not help (if due to a bug). Option (C) Increasing timeouts makes users wait longer, worsening experience. Option (D) Retries during overload can cause a Retry Storm, exacerbating the failure.",
  "wg": [
   {
    "t": "防止串聯故障",
    "en": "prevent cascading failures",
    "ps": "phrase"
   },
   {
    "t": "重試風暴",
    "en": "Retry Storm",
    "ps": "term"
   }
  ]
 }
}
]