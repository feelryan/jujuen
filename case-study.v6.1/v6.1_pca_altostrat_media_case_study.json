[
  {
    "no": "1",
    "level": "hard",
    "keywords": "Hybrid Connectivity, Storage Optimization, Data Ingestion",
    "question": [
      {
        "t": "Altostrat Media 正在啟動其現代化計畫，首要任務是將位於地端資料中心的數 PB 舊有媒體封存檔案遷移至 Google Cloud，同時必須確保這些資料能被生成式 AI 模型即時存取以進行訓練與內容生成。",
        "en": "Altostrat Media is initiating its modernization program, with the priority of migrating petabytes of legacy media archives from on-premises data centers to Google Cloud, while ensuring this data is immediately accessible for Generative AI model training and content generation.",
        "wg": [
          { "t": "遷移", "en": "migrating", "ps": "v" },
          { "t": "數 PB", "en": "petabytes", "ps": "n" },
          { "t": "生成式 AI", "en": "Generative AI", "ps": "n" }
        ]
      },
      {
        "t": "業務需求強調必須建立「安全、高效能的混合雲連線」以支援持續的資料攝取，並且針對不斷增長的媒體庫進行嚴格的成本控制，但不能犧牲資料的高可用性或存取效能。",
        "en": "Business requirements emphasize establishing 'secure, high-performance hybrid cloud connectivity' to support ongoing data ingestion, and strict cost control for the growing media library without sacrificing data high availability or access performance.",
        "wg": [
          { "t": "混合雲連線", "en": "hybrid cloud connectivity", "ps": "n" },
          { "t": "成本控制", "en": "cost control", "ps": "n" },
          { "t": "高可用性", "en": "high availability", "ps": "n" }
        ]
      },
      {
        "t": "目前的網際網路連線無法滿足頻寬需求，且現有防火牆硬體已接近壽命終點，您需要設計一個符合未來擴充性且營運負擔最低的架構。",
        "en": "Current internet connectivity cannot meet bandwidth demands, and existing firewall hardware is nearing end-of-life; you need to design an architecture that is future-proof and minimizes operational burden.",
        "wg": [
          { "t": "頻寬需求", "en": "bandwidth demands", "ps": "n" },
          { "t": "營運負擔", "en": "operational burden", "ps": "n" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "(A) 佈建合作夥伴互連 (Partner Interconnect) 以連接地端環境，並使用 Storage Transfer Service 進行資料遷移；將資料儲存在標準級 (Standard) Cloud Storage 以確保最高效能。",
        "en": "(A) Provision Partner Interconnect to connect the on-premises environment and use Storage Transfer Service for migration; store data in Standard Cloud Storage to ensure maximum performance.",
        "wg": [
          { "t": "合作夥伴互連", "en": "Partner Interconnect", "ps": "n" },
          { "t": "標準級", "en": "Standard", "ps": "n" }
        ]
      },
      {
        "t": "(B) 建立 Cloud VPN 高可用性 (HA) 通道以節省硬體成本，利用 gsutil 工具並行上傳資料；啟用物件生命週期管理 (Object Lifecycle Management) 規則，將超過 30 天的資料移至 Coldline。",
        "en": "(B) Establish Cloud VPN High Availability (HA) tunnels to save hardware costs, utilizing gsutil for parallel uploads; enable Object Lifecycle Management rules to move data older than 30 days to Coldline.",
        "wg": [
          { "t": "Cloud VPN 高可用性", "en": "Cloud VPN HA", "ps": "n" },
          { "t": "物件生命週期管理", "en": "Object Lifecycle Management", "ps": "n" }
        ]
      },
      {
        "t": "(C) 透過公共網際網路配置 Cloud CDN 進行傳輸加速，並使用 Transfer Appliance 進行一次性遷移；將資料儲存在近線 (Nearline) 儲存空間以平衡成本與存取速度。",
        "en": "(C) Configure Cloud CDN over the public internet for transfer acceleration and use Transfer Appliance for a one-time migration; store data in Nearline storage to balance cost and access speed.",
        "wg": [
          { "t": "Cloud CDN", "en": "Cloud CDN", "ps": "n" },
          { "t": "Transfer Appliance", "en": "Transfer Appliance", "ps": "n" }
        ]
      },
      {
        "t": "(D) 佈建專用互連 (Dedicated Interconnect) 以滿足高效能需求，並使用 Storage Transfer Service 進行持續同步；在 Cloud Storage 儲存桶上啟用 Autoclass 功能以自動最佳化成本與存取模式。",
        "en": "(D) Provision Dedicated Interconnect to meet high-performance requirements and use Storage Transfer Service for continuous synchronization; enable Autoclass on Cloud Storage buckets to automatically optimize costs and access patterns.",
        "wg": [
          { "t": "專用互連", "en": "Dedicated Interconnect", "ps": "n" },
          { "t": "Autoclass", "en": "Autoclass", "ps": "n" }
        ]
      }
    ],
    "answer": "(D)",
    "why": {
      "t": "選項 (D) 是最佳解，因為 Dedicated Interconnect 提供了從地端傳輸數 PB 資料所需的高頻寬與 SLA（滿足高效能需求）；Autoclass 能根據存取模式自動將物件在不同儲存級別間移動，完美解決了 Altostrat 媒體庫「龐大且存取模式動態」的成本優化痛點，且無需手動設定生命週期規則。選項 (A) 的 Partner Interconnect 頻寬通常較低且 Standard 儲存成本過高；選項 (B) VPN 對於 PB 級資料傳輸效能不足且 gsutil 管理負擔大；選項 (C) Transfer Appliance 無法解決「持續」攝取的需求。",
      "en": "Option (D) is the optimal solution because Dedicated Interconnect provides the high bandwidth and SLA required to transfer petabytes of data from on-premises (meeting high-performance needs); Autoclass automatically moves objects between storage classes based on access patterns, perfectly addressing the cost optimization pain point for Altostrat's 'vast and dynamic' media library without manual lifecycle rules. Option (A) Partner Interconnect typically offers lower bandwidth and Standard storage is too costly; Option (B) VPN lacks performance for PB-scale data and gsutil has high management overhead; Option (C) Transfer Appliance does not solve the need for 'continuous' ingestion.",
      "wg": [
        { "t": "SLA", "en": "SLA", "ps": "n" },
        { "t": "存取模式", "en": "access patterns", "ps": "n" }
      ]
    }
  },
  {
    "no": "2",
    "level": "hard",
    "keywords": "AI Compliance, Harmful Content Detection, Vertex AI",
    "question": [
      {
        "t": "Altostrat 希望利用大型語言模型 (LLM) 來增強使用者互動並自動生成內容摘要，但管理層對於 AI 的安全性與合規性表達了高度關切，特別是關於「偵測有害內容」與「決策的可解釋性」。",
        "en": "Altostrat wants to leverage Large Language Models (LLMs) to enhance user interaction and automated content summarization, but management has expressed high concern regarding AI safety and compliance, specifically regarding 'detecting harmful content' and 'decision explainability'.",
        "wg": [
          { "t": "大型語言模型", "en": "Large Language Models", "ps": "n" },
          { "t": "決策的可解釋性", "en": "decision explainability", "ps": "n" }
        ]
      },
      {
        "t": "您需要設計一個解決方案，讓開發團隊能夠快速部署對話式 AI，同時滿足技術需求中的「確保 AI 系統可被稽核」以及「偵測並過濾不當內容」，且需盡量減少自建模型的維運負擔。",
        "en": "You need to design a solution that allows the development team to rapidly deploy conversational AI while meeting the technical requirements of 'ensuring AI systems are auditable' and 'detecting and filtering inappropriate content', with minimal operational overhead for self-built models.",
        "wg": [
          { "t": "快速部署", "en": "rapidly deploy", "ps": "v" },
          { "t": "維運負擔", "en": "operational overhead", "ps": "n" }
        ]
      },
      {
        "t": "該解決方案必須與現有的 Google Cloud 環境緊密整合，並為未來的個人化推薦功能預留擴充空間。",
        "en": "The solution must be tightly integrated with the existing Google Cloud environment and allow room for expansion for future personalized recommendation features.",
        "wg": [
          { "t": "緊密整合", "en": "tightly integrated", "ps": "adj" },
          { "t": "擴充空間", "en": "room for expansion", "ps": "n" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "(A) 在 GKE 上部署開源 LLM 模型 (如 Llama)，並自行開發基於規則的過濾層 (Filter Layer) 來攔截有害關鍵字；使用 Cloud Logging 記錄所有 API 呼叫以供稽核。",
        "en": "(A) Deploy open-source LLM models (like Llama) on GKE and develop a custom rule-based filter layer to block harmful keywords; use Cloud Logging to record all API calls for auditing.",
        "wg": [
          { "t": "開源 LLM", "en": "open-source LLM", "ps": "n" },
          { "t": "基於規則", "en": "rule-based", "ps": "adj" }
        ]
      },
      {
        "t": "(B) 使用 Cloud Natural Language API 進行情緒分析，結合 DLP API 過濾敏感數據；將使用者輸入導向至預先定義好的靜態回應腳本以確保安全性。",
        "en": "(B) Use Cloud Natural Language API for sentiment analysis combined with DLP API to filter sensitive data; redirect user input to pre-defined static response scripts to ensure safety.",
        "wg": [
          { "t": "情緒分析", "en": "sentiment analysis", "ps": "n" },
          { "t": "靜態回應", "en": "static response", "ps": "n" }
        ]
      },
      {
        "t": "(C) 利用 Vertex AI Model Garden 存取基礎模型，啟用內建的安全屬性 (Safety Attributes) 來過濾有害內容；使用 Vertex Explainable AI 評估模型行為，並將中繼資料匯出至 BigQuery 進行稽核。",
        "en": "(C) Leverage Vertex AI Model Garden to access foundation models, enabling built-in Safety Attributes to filter harmful content; use Vertex Explainable AI to evaluate model behavior and export metadata to BigQuery for auditing.",
        "wg": [
          { "t": "基礎模型", "en": "foundation models", "ps": "n" },
          { "t": "安全屬性", "en": "Safety Attributes", "ps": "n" }
        ]
      },
      {
        "t": "(D) 使用 Dialogflow CX 建構聊天機器人，並透過 Webhook 呼叫自定義的 Python 函式來驗證每個回應的內容安全性；將對話日誌儲存在 Cloud Storage 冷存儲中。",
        "en": "(D) Use Dialogflow CX to build chatbots and call custom Python functions via Webhooks to validate the content safety of each response; store conversation logs in Cloud Storage Coldline.",
        "wg": [
          { "t": "聊天機器人", "en": "chatbots", "ps": "n" },
          { "t": "Webhook", "en": "Webhook", "ps": "n" }
        ]
      }
    ],
    "answer": "(C)",
    "why": {
      "t": "選項 (C) 直接呼應了 Vertex AI 在生成式 AI 方面的優勢。Vertex AI 提供了內建的「安全過濾器 (Safety Filters)」來滿足「偵測並過濾不當內容」的需求，且 Explainable AI 與整合的稽核功能解決了「可稽核性」與「可解釋性」的商業需求，大幅降低了自建過濾層的維運成本。選項 (A) 在 GKE 上自管模型與過濾規則會帶來極高的維運負擔且不易擴展；選項 (B) 僅使用靜態腳本無法達成「自然語言互動」的生成式體驗；選項 (D) 雖然 Dialogflow 適合對話，但依賴自定義 Webhook 進行安全過濾缺乏效率且不如 Vertex AI 的原生安全功能全面。",
      "en": "Option (C) directly addresses the strengths of Vertex AI for Generative AI. Vertex AI provides built-in 'Safety Filters' to meet the 'detect and filter inappropriate content' requirement, and Explainable AI along with integrated auditing features solves the business needs for 'auditability' and 'explainability', significantly reducing the operational cost of building custom filter layers. Option (A) self-managing models and filter rules on GKE incurs very high operational overhead and is hard to scale; Option (B) using static scripts fails to achieve the 'natural language interaction' generative experience; Option (D) while Dialogflow is good for conversation, relying on custom Webhooks for safety filtering is inefficient and less comprehensive than Vertex AI's native safety features.",
      "wg": [
        { "t": "安全過濾器", "en": "Safety Filters", "ps": "n" },
        { "t": "原生安全功能", "en": "native safety features", "ps": "n" }
      ]
    }
  },
  {
    "no": "3",
    "level": "hard",
    "keywords": "BigQuery ML, Real-time Analytics, Dynamic Pricing",
    "question": [
      {
        "t": "Altostrat 的行銷團隊希望實作「動態定價」策略，根據使用者當下的內容消費模式與人口統計數據即時調整訂閱價格。所有相關的使用者行為數據目前都持續串流至 BigQuery 中。",
        "en": "Altostrat's marketing team wants to implement a 'dynamic pricing' strategy, adjusting subscription prices in real-time based on users' current content consumption patterns and demographics. All relevant user behavior data is currently streaming into BigQuery.",
        "wg": [
          { "t": "動態定價", "en": "dynamic pricing", "ps": "n" },
          { "t": "持續串流", "en": "streaming", "ps": "v" }
        ]
      },
      {
        "t": "您需要架構一個機器學習管道，能夠直接利用現有數據進行模型訓練與預測，同時必須符合「利用數據為內容策略和決策提供資訊」的業務需求。且需將資料移動與維運管理的複雜度降至最低。",
        "en": "You need to architect a machine learning pipeline that can leverage existing data directly for model training and prediction, while meeting the business requirement to 'inform content strategy and decision-making with data'. Data movement and operational complexity must be minimized.",
        "wg": [
          { "t": "機器學習管道", "en": "machine learning pipeline", "ps": "n" },
          { "t": "複雜度", "en": "complexity", "ps": "n" }
        ]
      },
      {
        "t": "該解決方案應具備從原始數據到預測結果的快速迭代能力，以應對快速變化的媒體趨勢。",
        "en": "The solution should be capable of rapid iteration from raw data to prediction results to cope with fast-changing media trends.",
        "wg": [
          { "t": "快速迭代", "en": "rapid iteration", "ps": "n" },
          { "t": "預測結果", "en": "prediction results", "ps": "n" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "(A) 使用 Dataflow 將 BigQuery 數據匯出至 Cloud Storage，啟動 Dataproc 叢集運行 Spark MLlib 進行訓練，並將模型部署至 Compute Engine 進行推論。",
        "en": "(A) Use Dataflow to export BigQuery data to Cloud Storage, spin up a Dataproc cluster to run Spark MLlib for training, and deploy the model to Compute Engine for inference.",
        "wg": [
          { "t": "匯出", "en": "export", "ps": "v" },
          { "t": "推論", "en": "inference", "ps": "n" }
        ]
      },
      {
        "t": "(B) 建立 Vertex AI Notebooks 實例，透過 Pandas 讀取 BigQuery 數據進行本地訓練，將訓練好的 Scikit-learn 模型容器化後部署至 Cloud Run。",
        "en": "(B) Create a Vertex AI Notebooks instance, read BigQuery data via Pandas for local training, containerize the trained Scikit-learn model, and deploy it to Cloud Run.",
        "wg": [
          { "t": "本地訓練", "en": "local training", "ps": "n" },
          { "t": "容器化", "en": "containerize", "ps": "v" }
        ]
      },
      {
        "t": "(C) 使用 BigQuery ML 直接在數據倉儲內建立並訓練回歸模型 (Regression Model)，利用 `ML.PREDICT` 函式對即時傳入的數據進行批次或即時預測，並透過 Looker 儀表板視覺化結果。",
        "en": "(C) Use BigQuery ML to create and train a regression model directly within the data warehouse, use the `ML.PREDICT` function to perform batch or real-time predictions on incoming data, and visualize results via Looker dashboards.",
        "wg": [
          { "t": "數據倉儲", "en": "data warehouse", "ps": "n" },
          { "t": "視覺化", "en": "visualize", "ps": "v" }
        ]
      },
      {
        "t": "(D) 開發一個 Cloud Run 服務來接收使用者事件，即時呼叫 Vision API 分析內容，並將結果寫入 Cloud SQL，再由夜間批次作業計算隔日的定價。",
        "en": "(D) Develop a Cloud Run service to ingest user events, call Vision API in real-time to analyze content, write results to Cloud SQL, and use a nightly batch job to calculate pricing for the next day.",
        "wg": [
          { "t": "夜間批次作業", "en": "nightly batch job", "ps": "n" },
          { "t": "定價", "en": "pricing", "ps": "n" }
        ]
      }
    ],
    "answer": "(C)",
    "why": {
      "t": "選項 (C) 完美符合「將資料移動與維運管理的複雜度降至最低」的需求。BigQuery ML 允許使用 SQL 語法直接在數據存放處 (BigQuery) 訓練模型並進行預測，消除了導出資料的需要，並能處理大規模數據集，非常適合 Altostrat 的動態定價與分析需求。選項 (A) 和 (B) 都涉及繁瑣的資料搬移 (ETL) 與基礎架構管理，增加了延遲與維運成本；選項 (D) 依賴 Cloud SQL 與夜間批次，無法滿足「即時」調整定價的需求，且架構過於複雜。",
      "en": "Option (C) perfectly meets the requirement to 'minimize data movement and operational complexity'. BigQuery ML allows training models and making predictions directly where the data resides (BigQuery) using SQL syntax, eliminating the need for data export and handling large-scale datasets, making it ideal for Altostrat's dynamic pricing and analytics needs. Options (A) and (B) involve cumbersome data movement (ETL) and infrastructure management, adding latency and operational cost; Option (D) relies on Cloud SQL and nightly batches, failing to meet the 'real-time' pricing adjustment requirement and is overly complex.",
      "wg": [
        { "t": "SQL 語法", "en": "SQL syntax", "ps": "n" },
        { "t": "資料搬移", "en": "data movement", "ps": "n" }
      ]
    }
  },
  {
    "no": "4",
    "level": "hard",
    "keywords": "GKE Enterprise, Hybrid, Config Management",
    "question": [
      {
        "t": "Altostrat 依然保留部分舊有的地端系統用於內容攝取，並計劃將應用程式現代化為容器架構。目前的技術限制是「地端與雲端環境並存」，且業務端要求「簡化基礎架構管理以實現快速應用程式部署」。",
        "en": "Altostrat retains some legacy on-premises systems for content ingestion and plans to modernize applications into a container architecture. The current technical constraint is the 'coexistence of on-premises and cloud environments', and the business requires 'simplifying infrastructure management for rapid application deployment'.",
        "wg": [
          { "t": "容器架構", "en": "container architecture", "ps": "n" },
          { "t": "並存", "en": "coexistence", "ps": "n" }
        ]
      },
      {
        "t": "維運團隊目前面臨跨環境設定不一致的問題，導致部署失敗率高。您需要建議一個能夠統一管理地端 (Bare Metal) 與 Google Cloud 上 Kubernetes 叢集的架構，並確保安全政策的一致性。",
        "en": "The operations team currently faces issues with inconsistent configurations across environments, leading to high deployment failure rates. You need to recommend an architecture that centrally manages Kubernetes clusters on both on-premises (Bare Metal) and Google Cloud, ensuring consistency in security policies.",
        "wg": [
          { "t": "不一致", "en": "inconsistent", "ps": "adj" },
          { "t": "統一管理", "en": "centrally manages", "ps": "v" }
        ]
      },
      {
        "t": "解決方案必須支援 GitOps 工作流程，以實現全自動化的組態派送。",
        "en": "The solution must support a GitOps workflow to enable fully automated configuration delivery.",
        "wg": [
          { "t": "GitOps 工作流程", "en": "GitOps workflow", "ps": "n" },
          { "t": "全自動化", "en": "fully automated", "ps": "adj" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "(A) 在地端安裝 GKE on Bare Metal，並將其與雲端的 GKE 叢集註冊到 GKE Enterprise (Anthos) Fleet；使用 Anthos Config Management 連結至 Git 儲存庫，將統一的 Policy 和 Deployment 透過 Policy Controller 同步至所有叢集。",
        "en": "(A) Install GKE on Bare Metal on-premises and register it along with cloud GKE clusters to a GKE Enterprise (Anthos) Fleet; use Anthos Config Management linked to a Git repository to sync unified Policies and Deployments via Policy Controller to all clusters.",
        "wg": [
          { "t": "註冊", "en": "register", "ps": "v" },
          { "t": "同步", "en": "sync", "ps": "v" }
        ]
      },
      {
        "t": "(B) 維持地端使用 Docker Swarm，雲端使用 GKE Standard；編寫多個 Terraform 模組分別針對兩個環境進行基礎架構即程式碼 (IaC) 的部署，並使用 Jenkins Pipeline 協調兩邊的發布順序。",
        "en": "(B) Maintain Docker Swarm on-premises and use GKE Standard in the cloud; write multiple Terraform modules targeting each environment separately for Infrastructure as Code (IaC) deployment, and use Jenkins Pipeline to coordinate release order.",
        "wg": [
          { "t": "基礎架構即程式碼", "en": "Infrastructure as Code", "ps": "n" },
          { "t": "協調", "en": "coordinate", "ps": "v" }
        ]
      },
      {
        "t": "(C) 將地端伺服器遷移至 Compute Engine 實例，並使用 Managed Instance Groups (MIG) 替換容器架構；使用 Cloud Deployment Manager 集中管理所有 VM 的腳本配置。",
        "en": "(C) Migrate on-premises servers to Compute Engine instances and replace container architecture with Managed Instance Groups (MIG); use Cloud Deployment Manager to centrally manage script configurations for all VMs.",
        "wg": [
          { "t": "遷移", "en": "Migrate", "ps": "v" },
          { "t": "集中管理", "en": "centrally manage", "ps": "v" }
        ]
      },
      {
        "t": "(D) 在地端與雲端分別建立獨立的 GKE 叢集，並使用 Cloud Build 的多目標觸發器 (Multi-target triggers) 將映像檔分別推送到兩個環境；使用 Binary Authorization 確保只有簽章過的映像檔能被部署。",
        "en": "(D) Create independent GKE clusters on-premises and in the cloud, and use Cloud Build multi-target triggers to push images to both environments separately; use Binary Authorization to ensure only signed images can be deployed.",
        "wg": [
          { "t": "獨立的", "en": "independent", "ps": "adj" },
          { "t": "簽章過的", "en": "signed", "ps": "adj" }
        ]
      }
    ],
    "answer": "(A)",
    "why": {
      "t": "選項 (A) 是針對混合雲 Kubernetes 管理的最佳實踐。GKE Enterprise (Anthos) 提供了統一的控制平面 (Fleet)，而 Config Management (ACM) 則實現了 GitOps 和 Policy Controller 的功能，確保地端與雲端配置的一致性，解決了「設定不一致」與「簡化管理」的痛點。選項 (B) 使用異質編排系統 (Swarm vs GKE) 會增加管理複雜度；選項 (C) 放棄了容器化與 Kubernetes 的優勢，退回到 VM 管理，不符合現代化目標；選項 (D) 雖然使用了 CI/CD，但缺乏統一的叢集組態管理 (Configuration Management)，無法保證執行時期的政策一致性。",
      "en": "Option (A) is the best practice for hybrid cloud Kubernetes management. GKE Enterprise (Anthos) provides a unified control plane (Fleet), and Config Management (ACM) enables GitOps and Policy Controller capabilities, ensuring consistency in configuration between on-premises and cloud, solving the 'inconsistent configuration' and 'simplified management' pain points. Option (B) using heterogeneous orchestration systems (Swarm vs GKE) increases complexity; Option (C) abandons containerization and Kubernetes advantages, reverting to VM management which contradicts modernization goals; Option (D) uses CI/CD but lacks unified cluster Configuration Management, failing to guarantee runtime policy consistency.",
      "wg": [
        { "t": "統一的控制平面", "en": "unified control plane", "ps": "n" },
        { "t": "異質編排系統", "en": "heterogeneous orchestration systems", "ps": "n" }
      ]
    }
  },
  {
    "no": "5",
    "level": "hard",
    "keywords": "Cost Optimization, CI/CD, Development Environment",
    "question": [
      {
        "t": "Altostrat 的開發團隊已被要求將其開發基礎架構完全遷移至 Google Cloud，以支援「現代化 CI/CD」並減少地端硬體依賴。開發環境具有高度動態性，每天會有多次的開啟與關閉，但需要保留開發狀態。",
        "en": "Altostrat's development team has been required to fully migrate their development infrastructure to Google Cloud to support 'modernize CI/CD' and reduce reliance on on-premises hardware. The development environment is highly dynamic, with multiple start/stop events daily, but requires state persistence.",
        "wg": [
          { "t": "減少依賴", "en": "reduce reliance", "ps": "v" },
          { "t": "狀態保留", "en": "state persistence", "ps": "n" }
        ]
      },
      {
        "t": "財務部門要求對開發成本進行精細的追蹤，並希望能在不影響開發效率的前提下盡可能降低閒置成本。您需要設計一個符合這些需求的流程與架構。",
        "en": "The finance department requires granular tracking of development costs and wants to minimize idle costs as much as possible without impacting development efficiency. You need to design a process and architecture that meets these requirements.",
        "wg": [
          { "t": "精細的追蹤", "en": "granular tracking", "ps": "n" },
          { "t": "閒置成本", "en": "idle costs", "ps": "n" }
        ]
      },
      {
        "t": "請選擇兩項最適合的步驟來達成此目標。(請選擇兩項)",
        "en": "Choose the two most appropriate steps to achieve this goal. (Choose two)",
        "wg": [
          { "t": "步驟", "en": "steps", "ps": "n" },
          { "t": "達成", "en": "achieve", "ps": "v" }
        ]
      }
    ],
    "type": "複選題",
    "options": [
      {
        "t": "(A) 強制所有開發 VM 使用 Local SSD 以獲得最高效能，並編寫關機腳本將狀態備份至 Cloud Storage；重新啟動時再從 Cloud Storage 還原。",
        "en": "(A) Enforce all development VMs to use Local SSD for maximum performance, and write shutdown scripts to backup state to Cloud Storage; restore from Cloud Storage upon restart.",
        "wg": [
          { "t": "強制", "en": "Enforce", "ps": "v" },
          { "t": "還原", "en": "restore", "ps": "v" }
        ]
      },
      {
        "t": "(B) 配置 Cloud Billing Export 至 BigQuery，並強制要求所有開發資源必須套用「Team」與「Environment」的標籤 (Labels)，以便財務部門依據標籤進行成本群組分析。",
        "en": "(B) Configure Cloud Billing Export to BigQuery and mandate that all development resources must have 'Team' and 'Environment' labels applied, allowing the finance department to group costs based on labels.",
        "wg": [
          { "t": "標籤", "en": "labels", "ps": "n" },
          { "t": "成本群組分析", "en": "cost grouping analysis", "ps": "n" }
        ]
      },
      {
        "t": "(C) 使用先佔式執行個體 (Preemptible VMs) 作為主要開發環境，並配置自動重新啟動腳本以節省 80% 的運算成本。",
        "en": "(C) Use Preemptible VMs as the primary development environment and configure auto-restart scripts to save 80% on compute costs.",
        "wg": [
          { "t": "先佔式執行個體", "en": "Preemptible VMs", "ps": "n" },
          { "t": "運算成本", "en": "compute costs", "ps": "n" }
        ]
      },
      {
        "t": "(D) 使用標準永久磁碟 (Standard Persistent Disks) 儲存開發狀態。教導開發人員在不使用時僅「停止」VM 實例而非刪除；停止的實例不產生運算費用，僅產生磁碟儲存費用。",
        "en": "(D) Use Standard Persistent Disks to store development state. Instruct developers to only 'stop' VM instances when not in use, rather than deleting them; stopped instances incur no compute charges, only disk storage charges.",
        "wg": [
          { "t": "永久磁碟", "en": "Persistent Disks", "ps": "n" },
          { "t": "運算費用", "en": "compute charges", "ps": "n" }
        ]
      },
      {
        "t": "(E) 為所有開發專案啟用承諾使用折扣 (Committed Use Discounts)，並購買 3 年期的資源承諾以獲得最低費率。",
        "en": "(E) Enable Committed Use Discounts for all development projects and purchase a 3-year resource commitment to secure the lowest rates.",
        "wg": [
          { "t": "承諾使用折扣", "en": "Committed Use Discounts", "ps": "n" },
          { "t": "最低費率", "en": "lowest rates", "ps": "n" }
        ]
      }
    ],
    "answer": "(B), (D)",
    "why": {
      "t": "選項 (B) 和 (D) 結合解決了「成本可視性」與「降低閒置成本且保留狀態」的需求。選項 (B) 透過 Billing Export 和 Labels 提供了財務部門所需的精細成本視圖；選項 (D) 利用 GCP 的特性，即停止的 VM 不計費但 PD 會保留數據，這符合開發環境頻繁啟停且需保留狀態的特性。選項 (A) 使用 Local SSD 在 VM 停止時會遺失數據，備份還原流程過於複雜且影響開發體驗；選項 (C) 先佔式實例隨時會被中斷，不適合需要穩定狀態的互動式開發環境；選項 (E) 對於高度動態且可能隨時縮減的開發環境來說，購買 3 年承諾缺乏彈性且風險過高。",
      "en": "Options (B) and (D) combined address the needs for 'cost visibility' and 'reducing idle costs while persisting state'. Option (B) provides the granular cost view required by finance via Billing Export and Labels; Option (D) leverages the GCP feature where stopped VMs are not billed for compute but PDs retain data, fitting the frequent start/stop nature of development environments requiring state persistence. Option (A) using Local SSD loses data on stop, and backup/restore is too complex and hinders developer experience; Option (C) Preemptible VMs can be preempted anytime, making them unsuitable for interactive development environments needing stability; Option (E) purchasing 3-year commitments is too inflexible and risky for highly dynamic development environments that might scale down.",
      "wg": [
        { "t": "成本可視性", "en": "cost visibility", "ps": "n" },
        { "t": "互動式開發環境", "en": "interactive development environment", "ps": "n" }
      ]
    }
  },
  {
    "no": "6",
    "level": "hard",
    "keywords": "Observability, SRE, Prometheus, Managed Service",
    "question": [
      {
        "t": "Altostrat 目前依賴混合了 Cloud Monitoring 與開源 Prometheus 的監控工具，且警報主要透過電子郵件發送，這導致了嚴重的「警報疲勞」並延誤了對關鍵事故的反應時間。",
        "en": "Altostrat currently relies on a mix of Cloud Monitoring and open-source Prometheus for monitoring, with alerts primarily delivered via email, which has led to severe 'alert fatigue' and delayed response times for critical incidents.",
        "wg": [
          { "t": "警報疲勞", "en": "alert fatigue", "ps": "n" },
          { "t": "延誤", "en": "delayed", "ps": "v" }
        ]
      },
      {
        "t": "為了滿足「加速並增強營運工作流程可靠性」的業務需求，您需要設計一個現代化的可觀測性堆疊 (Observability Stack)。該方案必須保留開發團隊習慣的 Prometheus 查詢語法 (PromQL) 以減少學習曲線，同時必須消除自行管理 Prometheus 伺服器的擴展性瓶頸。",
        "en": "To meet the business requirement to 'accelerate and enhance the reliability of operational workflows', you need to design a modern Observability Stack. The solution must retain the Prometheus query language (PromQL) familiar to the development team to reduce the learning curve, while eliminating the scalability bottlenecks of self-managing Prometheus servers.",
        "wg": [
          { "t": "可觀測性堆疊", "en": "Observability Stack", "ps": "n" },
          { "t": "擴展性瓶頸", "en": "scalability bottlenecks", "ps": "n" }
        ]
      },
      {
        "t": "此外，警報系統必須整合至事故管理平台，而非依賴電子郵件。",
        "en": "Furthermore, the alerting system must be integrated into an incident management platform rather than relying on email.",
        "wg": [
          { "t": "事故管理平台", "en": "incident management platform", "ps": "n" },
          { "t": "依賴", "en": "relying on", "ps": "v" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "(A) 在每個 GKE 叢集中部署 Sidecar 模式的 Prometheus 伺服器，並使用 Grafana Agent 將指標寫入 BigQuery；設定 BigQuery 定排程查詢 (Scheduled Queries) 來偵測異常並透過 SendGrid 發送通知。",
        "en": "(A) Deploy Prometheus servers in Sidecar mode within each GKE cluster and use Grafana Agent to write metrics to BigQuery; configure BigQuery Scheduled Queries to detect anomalies and send notifications via SendGrid.",
        "wg": [
          { "t": "定排程查詢", "en": "Scheduled Queries", "ps": "n" },
          { "t": "異常", "en": "anomalies", "ps": "n" }
        ]
      },
      {
        "t": "(B) 遷移至 Google Cloud Managed Service for Prometheus，部署受管理的收集器 (Managed Collectors) 以自動擷取指標；使用 Cloud Monitoring 定義以服務水準目標 (SLO) 為基礎的警報政策，並將通知通道設定為 Pub/Sub 以觸發 PagerDuty。",
        "en": "(B) Migrate to Google Cloud Managed Service for Prometheus, deploying Managed Collectors to automatically scrape metrics; use Cloud Monitoring to define SLO-based alerting policies, and configure the notification channel to Pub/Sub to trigger PagerDuty.",
        "wg": [
          { "t": "受管理的收集器", "en": "Managed Collectors", "ps": "n" },
          { "t": "服務水準目標", "en": "SLO", "ps": "n" }
        ]
      },
      {
        "t": "(C) 使用 Terraform 在 Compute Engine 上建立一個集中式的 Prometheus 叢集，透過 VPC 對等互連 (VPC Peering) 收集所有環境的資料；安裝 Alertmanager 並編寫 Webhook 將警報轉發至 Google Chat。",
        "en": "(C) Use Terraform to create a centralized Prometheus cluster on Compute Engine, collecting data from all environments via VPC Peering; install Alertmanager and write Webhooks to forward alerts to Google Chat.",
        "wg": [
          { "t": "集中式的", "en": "centralized", "ps": "adj" },
          { "t": "轉發", "en": "forward", "ps": "v" }
        ]
      },
      {
        "t": "(D) 將所有應用程式日誌與指標直接串流至 Cloud Logging，並使用 Log Analytics 進行分析；針對每個錯誤日誌 (Error Log) 建立基於日誌的指標 (Log-based Metric) 並設定閾值警報。",
        "en": "(D) Stream all application logs and metrics directly to Cloud Logging and use Log Analytics for analysis; create Log-based Metrics for every error log and set threshold alerts.",
        "wg": [
          { "t": "基於日誌的指標", "en": "Log-based Metric", "ps": "n" },
          { "t": "閾值", "en": "threshold", "ps": "n" }
        ]
      }
    ],
    "answer": "(B)",
    "why": {
      "t": "選項 (B) 是最佳解，因為 Google Cloud Managed Service for Prometheus 允許 Altostrat 繼續使用現有的 PromQL 儀表板與警報邏輯（滿足減少學習曲線），同時由 Google 管理後端基礎架構（解決擴展性與維運負擔）。採用 SLO 為基礎的警報並整合 PagerDuty (透過 Pub/Sub 或 Webhook) 直接解決了「警報疲勞」與「電子郵件通知」的問題。選項 (A) 使用 BigQuery 進行監控延遲過高且非即時；選項 (C) 雖然使用了 Prometheus，但仍需自行維護基礎架構 (IaaS)，未達成減少維運負擔的目標；選項 (D) 僅依賴日誌進行監控是不夠的，且無法取代 Prometheus 的指標監控能力。",
      "en": "Option (B) is the optimal solution because Google Cloud Managed Service for Prometheus allows Altostrat to continue using existing PromQL dashboards and alerting logic (meeting the reduced learning curve requirement), while Google manages the backend infrastructure (solving scalability and operational burden). Adopting SLO-based alerting and integrating with PagerDuty (via Pub/Sub or Webhook) directly addresses 'alert fatigue' and the 'email notification' issue. Option (A) using BigQuery for monitoring has too much latency and is not real-time; Option (C) although using Prometheus, still requires self-managed infrastructure (IaaS), failing to meet the goal of reducing operational burden; Option (D) relying solely on logs for monitoring is insufficient and cannot replace Prometheus's metric monitoring capabilities.",
      "wg": [
        { "t": "儀表板", "en": "dashboards", "ps": "n" },
        { "t": "即時", "en": "real-time", "ps": "adj" }
      ]
    }
  },
  {
    "no": "7",
    "level": "hard",
    "keywords": "Cloud CDN, Security, Signed URLs, Media Delivery",
    "question": [
      {
        "t": "Altostrat 計畫推出一項新的優質影音訂閱服務，該服務將向全球付費會員提供高解析度的獨家內容。為了「開啟新的營收來源」，必須嚴格防止未經授權的內容存取（例如使用者將影片連結分享給非會員）。",
        "en": "Altostrat plans to launch a new premium video subscription service, delivering high-resolution exclusive content to paid members globally. To 'unlock new revenue streams', unauthorized content access (e.g., users sharing video links with non-members) must be strictly prevented.",
        "wg": [
          { "t": "優質影音訂閱服務", "en": "premium video subscription service", "ps": "n" },
          { "t": "未經授權的", "en": "unauthorized", "ps": "adj" }
        ]
      },
      {
        "t": "同時，為了確保優異的使用者體驗，影片內容必須盡可能接近使用者端進行快取 (Cache)。您需要設計一個既能滿足全球低延遲傳輸，又能確保連結具有時效性與身分驗證能力的架構。",
        "en": "At the same time, to ensure an excellent user experience, video content must be cached as close to the user as possible. You need to design an architecture that satisfies both global low-latency delivery and ensures links have time-bound validity and authentication capabilities.",
        "wg": [
          { "t": "快取", "en": "Cache", "ps": "v" },
          { "t": "時效性", "en": "time-bound validity", "ps": "n" }
        ]
      },
      {
        "t": "應用程式後端已經整合了使用者身分驗證系統。",
        "en": "The application backend is already integrated with a user identity authentication system.",
        "wg": [
          { "t": "後端", "en": "backend", "ps": "n" },
          { "t": "整合", "en": "integrated", "ps": "v" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "(A) 設定 Cloud Storage 儲存桶為公開讀取，但在前端應用程式中使用 JavaScript 混淆影片 URL；使用 Cloud Load Balancing 的地理位置路由 (Geomapping) 功能將使用者導向最近的區域。",
        "en": "(A) Configure Cloud Storage buckets as public read, but obfuscate video URLs in the frontend application using JavaScript; use Cloud Load Balancing's Geomapping feature to direct users to the nearest region.",
        "wg": [
          { "t": "混淆", "en": "obfuscate", "ps": "v" },
          { "t": "地理位置路由", "en": "Geomapping", "ps": "n" }
        ]
      },
      {
        "t": "(B) 使用 Cloud CDN 快取內容，並配置簽署網址 (Signed URLs) 與簽署 Cookie (Signed Cookies)。應用程式後端在驗證使用者訂閱狀態後，動態產生包含短暫過期時間的簽章連結回傳給前端。",
        "en": "(B) Use Cloud CDN to cache content and configure Signed URLs and Signed Cookies. After verifying the user's subscription status, the application backend dynamically generates a signed link with a short expiration time and returns it to the frontend.",
        "wg": [
          { "t": "簽署網址", "en": "Signed URLs", "ps": "n" },
          { "t": "過期時間", "en": "expiration time", "ps": "n" }
        ]
      },
      {
        "t": "(C) 部署 Identity-Aware Proxy (IAP) 來保護 Cloud Storage 儲存桶，要求所有存取請求都必須通過 Google 帳戶驗證；將 Cloud CDN 連結至 IAP 後端服務以加速傳輸。",
        "en": "(C) Deploy Identity-Aware Proxy (IAP) to protect Cloud Storage buckets, requiring all access requests to be authenticated via Google Accounts; link Cloud CDN to the IAP backend service to accelerate delivery.",
        "wg": [
          { "t": "保護", "en": "protect", "ps": "v" },
          { "t": "加速傳輸", "en": "accelerate delivery", "ps": "v" }
        ]
      },
      {
        "t": "(D) 將影片內容加密後儲存於 Cloud Storage，並將解密金鑰儲存在 Cloud KMS。每次播放時，使用者端播放器需呼叫 API 取得金鑰進行解密，且僅允許來自特定 IP 範圍的請求。",
        "en": "(D) Encrypt video content before storing in Cloud Storage, keeping decryption keys in Cloud KMS. For each playback, the client player must call an API to retrieve the key for decryption, allowing requests only from specific IP ranges.",
        "wg": [
          { "t": "解密金鑰", "en": "decryption keys", "ps": "n" },
          { "t": "特定 IP 範圍", "en": "specific IP ranges", "ps": "n" }
        ]
      }
    ],
    "answer": "(B)",
    "why": {
      "t": "選項 (B) 是標準的內容傳遞網路 (CDN) 安全最佳實務。Cloud CDN 的 Signed URLs/Cookies 允許 Altostrat 精確控制誰可以存取快取的內容以及存取多久，這直接滿足了「防止未經授權存取」與「全球低延遲快取」的需求，且無需對儲存桶進行複雜的 ACL 設定。選項 (A) 的「隱藏 URL」是無效的安全性（Security by obscurity），一旦連結洩漏即可被無限分享；選項 (C) IAP 適用於企業內部應用程式存取，不適合用於向數百萬外部消費者傳遞高頻寬媒體內容，且 IAP 與 CDN 的整合主要針對靜態資產保護而非動態訂閱驗證；選項 (D) 用戶端解密管理複雜且會造成巨大的播放延遲，且 IP 限制無法解決行動裝置 IP 變動的問題。",
      "en": "Option (B) is the standard best practice for Content Delivery Network (CDN) security. Cloud CDN Signed URLs/Cookies allow Altostrat to precisely control who can access cached content and for how long, directly meeting the requirements for 'preventing unauthorized access' and 'global low-latency caching' without complex ACL settings on buckets. Option (A) 'obfuscating URLs' is security by obscurity and ineffective; once a link is leaked, it can be shared infinitely; Option (C) IAP is designed for internal enterprise app access, not for delivering high-bandwidth media to millions of external consumers, and IAP integration focuses on static asset protection rather than dynamic subscription validation; Option (D) client-side decryption is complex to manage and introduces significant playback latency, and IP restrictions fail for mobile devices with changing IPs.",
      "wg": [
        { "t": "無效的安全性", "en": "security by obscurity", "ps": "n" },
        { "t": "播放延遲", "en": "playback latency", "ps": "n" }
      ]
    }
  },
  {
    "no": "8",
    "level": "hard",
    "keywords": "Serverless, Metadata Extraction, Event-Driven, Video Intelligence API",
    "question": [
      {
        "t": "為了實現「從媒體資產中擷取豐富中繼資料」的技術需求，Altostrat 希望建立一個全自動化的處理管道。每當新的影片檔案上傳至 Cloud Storage 特定儲存桶時，系統應自動觸發分析流程。",
        "en": "To realize the technical requirement of 'extracting rich metadata from media assets', Altostrat wants to build a fully automated processing pipeline. Whenever a new video file is uploaded to a specific Cloud Storage bucket, the system should automatically trigger the analysis process.",
        "wg": [
          { "t": "全自動化", "en": "fully automated", "ps": "adj" },
          { "t": "觸發", "en": "trigger", "ps": "v" }
        ]
      },
      {
        "t": "分析內容包括物件偵測、文字轉錄與場景識別。提取出的資料需寫入 BigQuery 以供後續的內容推薦系統使用。考慮到媒體檔案大小不一（從幾 MB 到數十 GB），解決方案必須具備高度的擴展性與強健性，且不能受限於單個請求的執行時間限制。",
        "en": "The analysis includes object detection, text transcription, and scene recognition. Extracted data must be written to BigQuery for use by the content recommendation system. Considering media files vary in size (from a few MBs to tens of GBs), the solution must be highly scalable and robust, and not constrained by single-request execution time limits.",
        "wg": [
          { "t": "物件偵測", "en": "object detection", "ps": "n" },
          { "t": "強健性", "en": "robustness", "ps": "n" }
        ]
      },
      {
        "t": "您應選擇哪種架構組合來達成此目標？",
        "en": "Which architectural combination should you choose to achieve this goal?",
        "wg": [
          { "t": "架構組合", "en": "architectural combination", "ps": "n" },
          { "t": "達成", "en": "achieve", "ps": "v" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "(A) 設定 Cloud Storage 觸發 Cloud Run functions (第 2 代)。函式接收到事件後，呼叫 Video Intelligence API 啟動非同步作業 (LRO)，並將作業 ID 寫入 Pub/Sub；另一個訂閱 Pub/Sub 的 Cloud Run 服務負責輪詢 API 狀態並將結果寫入 BigQuery。",
        "en": "(A) Configure Cloud Storage to trigger Cloud Run functions (2nd gen). Upon receiving the event, the function calls the Video Intelligence API to start a Long-Running Operation (LRO) and writes the Job ID to Pub/Sub; another Cloud Run service subscribing to Pub/Sub polls the API status and writes results to BigQuery.",
        "wg": [
          { "t": "非同步作業", "en": "Long-Running Operation", "ps": "n" },
          { "t": "輪詢", "en": "polls", "ps": "v" }
        ]
      },
      {
        "t": "(B) 設定 Cloud Storage 觸發 Cloud Run functions。函式直接下載影片檔案，使用 OpenCV 程式庫進行本地分析，處理完成後將 JSON 結果串流插入 BigQuery。",
        "en": "(B) Configure Cloud Storage to trigger Cloud Run functions. The function downloads the video file directly, uses the OpenCV library for local analysis, and streams the JSON results into BigQuery upon completion.",
        "wg": [
          { "t": "本地分析", "en": "local analysis", "ps": "n" },
          { "t": "串流插入", "en": "streams", "ps": "v" }
        ]
      },
      {
        "t": "(C) 使用 Eventarc 監聽 Cloud Storage 事件並將其路由至 Cloud Run 服務。該服務同步呼叫 Video Intelligence API 的 `annotate` 方法，等待回應後直接將回應寫入 BigQuery。",
        "en": "(C) Use Eventarc to listen for Cloud Storage events and route them to a Cloud Run service. The service synchronously calls the Video Intelligence API's `annotate` method, waiting for the response before writing it directly to BigQuery.",
        "wg": [
          { "t": "同步呼叫", "en": "synchronously calls", "ps": "v" },
          { "t": "路由", "en": "route", "ps": "v" }
        ]
      },
      {
        "t": "(D) 使用 Dataflow 建立一個串流管道 (Streaming Pipeline)，監控 Cloud Storage 儲存桶的變更通知。使用 Dataflow 的 `VideoIntelligence` 轉換 (Transform) 來處理影片，並使用 BigQuery IO 連接器寫入結果。",
        "en": "(D) Use Dataflow to create a Streaming Pipeline that monitors Cloud Storage bucket change notifications. Use Dataflow's `VideoIntelligence` transform to process videos and the BigQuery IO connector to write results.",
        "wg": [
          { "t": "串流管道", "en": "Streaming Pipeline", "ps": "n" },
          { "t": "轉換", "en": "transform", "ps": "n" }
        ]
      }
    ],
    "answer": "(A)",
    "why": {
      "t": "選項 (A) 正確處理了長影片分析的非同步特性。Video Intelligence API 處理大型影片需要時間，不能在單個同步請求中完成（會導致 Timeout）。透過 Cloud Run functions 啟動 LRO (Long-Running Operation) 並解耦狀態監控，是最具彈性且無伺服器的做法。選項 (B) 試圖在 Function 內下載並處理 GB 級影片會立即耗盡記憶體與執行時間限制 (Timeout)，且 OpenCV 無法提供所需的 NLP 功能；選項 (C) 同步呼叫 (Synchronous call) 僅適用於極短影片，對於長影片會導致連線逾時；選項 (D) 雖然可行，但 Dataflow 對於單純的「事件驅動 API 呼叫」來說過於沈重 (Overkill) 且成本較高，且不如 Cloud Run functions 符合案例中「無伺服器執行事件驅動任務」的描述。",
      "en": "Option (A) correctly handles the asynchronous nature of long video analysis. Video Intelligence API takes time to process large videos and cannot be completed in a single synchronous request (leading to Timeouts). Starting an LRO (Long-Running Operation) via Cloud Run functions and decoupling status monitoring is the most resilient and serverless approach. Option (B) attempting to download and process GB-sized videos within a Function will immediately exhaust memory and execution time limits, and OpenCV lacks the required NLP capabilities; Option (C) synchronous calls apply only to very short videos, leading to connection timeouts for long ones; Option (D) while feasible, Dataflow is overkill and costlier for simple 'event-driven API calls' compared to Cloud Run functions, which aligns better with the case study's description of 'serverless execution of event-driven tasks'.",
      "wg": [
        { "t": "非同步特性", "en": "asynchronous nature", "ps": "n" },
        { "t": "耗盡", "en": "exhaust", "ps": "v" }
      ]
    }
  },
  {
    "no": "9",
    "level": "hard",
    "keywords": "Identity Platform, CIAM, User Management, Migration",
    "question": [
      {
        "t": "Altostrat 現有的使用者管理系統是基於舊有的地端 SQL 資料庫與自行開發的驗證邏輯，維護成本高且難以擴充。作為現代化的一部分，他們希望將數百萬使用者的驗證遷移至 Google Cloud。",
        "en": "Altostrat's existing user management system is based on legacy on-premises SQL databases and custom authentication logic, which is costly to maintain and hard to scale. As part of modernization, they want to migrate authentication for millions of users to Google Cloud.",
        "wg": [
          { "t": "驗證邏輯", "en": "authentication logic", "ps": "n" },
          { "t": "維護成本", "en": "maintenance costs", "ps": "n" }
        ]
      },
      {
        "t": "新的解決方案必須支援「無縫的自助服務支援」與個人化體驗，允許使用者使用社交帳號登入，同時必須保留現有使用者的帳號密碼而不強制重設。此外，行銷團隊希望能獲得使用者的登入活動數據以進行分析。",
        "en": "The new solution must support 'seamless self-service support' and personalized experiences, allowing users to log in with social accounts, while preserving existing users' credentials without forcing a reset. Additionally, the marketing team wants access to user login activity data for analysis.",
        "wg": [
          { "t": "社交帳號", "en": "social accounts", "ps": "n" },
          { "t": "強制重設", "en": "forcing a reset", "ps": "v" }
        ]
      },
      {
        "t": "您需要推薦一個能夠處理大規模消費者身分識別 (CIAM) 的全託管服務。",
        "en": "You need to recommend a fully managed service capable of handling large-scale Consumer Identity and Access Management (CIAM).",
        "wg": [
          { "t": "消費者身分識別", "en": "CIAM", "ps": "n" },
          { "t": "全託管服務", "en": "fully managed service", "ps": "n" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "(A) 使用 Identity Platform 匯入現有使用者雜湊密碼 (Password Hash)；啟用社交登入提供者 (Social Providers)；配置活動記錄匯出至 Cloud Logging 與 BigQuery 以供行銷分析。",
        "en": "(A) Use Identity Platform to import existing user password hashes; enable Social Providers; configure activity log export to Cloud Logging and BigQuery for marketing analysis.",
        "wg": [
          { "t": "雜湊密碼", "en": "password hashes", "ps": "n" },
          { "t": "社交登入提供者", "en": "Social Providers", "ps": "n" }
        ]
      },
      {
        "t": "(B) 在 GKE 上部署 Keycloak 叢集，並連接至 Cloud SQL for PostgreSQL 儲存使用者資料；開發客製化連接器以同步地端 SQL 資料庫的變更。",
        "en": "(B) Deploy a Keycloak cluster on GKE, connecting to Cloud SQL for PostgreSQL to store user data; develop custom connectors to sync changes from the on-premises SQL database.",
        "wg": [
          { "t": "客製化連接器", "en": "custom connectors", "ps": "n" },
          { "t": "同步", "en": "sync", "ps": "v" }
        ]
      },
      {
        "t": "(C) 使用 Cloud Identity Premium 建立使用者帳號，並透過 Google Cloud Directory Sync (GCDS) 同步地端 LDAP 目錄；使用 Context-Aware Access 控制存取。",
        "en": "(C) Use Cloud Identity Premium to create user accounts and synchronize the on-premises LDAP directory via Google Cloud Directory Sync (GCDS); use Context-Aware Access to control access.",
        "wg": [
          { "t": "同步", "en": "synchronize", "ps": "v" },
          { "t": "目錄", "en": "directory", "ps": "n" }
        ]
      },
      {
        "t": "(D) 使用 Firebase Authentication 的電子郵件/密碼登入功能。由於 Firebase 不支援匯入雜湊密碼，需透過發送電子郵件要求所有使用者在第一次登入新平台時重設密碼。",
        "en": "(D) Use Firebase Authentication's email/password login feature. Since Firebase does not support importing password hashes, email all users requiring them to reset their passwords upon first login to the new platform.",
        "wg": [
          { "t": "重設密碼", "en": "reset passwords", "ps": "v" },
          { "t": "不支援", "en": "does not support", "ps": "v" }
        ]
      }
    ],
    "answer": "(A)",
    "why": {
      "t": "選項 (A) 正確應用了 Identity Platform (Google Cloud 的企業級 CIAM 解決方案)。它支援匯入各種標準加密演算法的雜湊密碼，實現「不強制重設密碼」的無縫遷移，並原生支援社交登入與大規模擴充性，完全符合 Altostrat 作為媒體平台面對大量消費者的需求。選項 (B) 自行架設 Keycloak 違背了「簡化管理」與「全託管」的原則；選項 (C) Cloud Identity 主要針對企業內部員工 (Workforce) 管理，而非消費者 (Customer) 用戶端；選項 (D) 強制使用者重設密碼會嚴重損害使用者體驗 (UX) 並導致流失率上升，且 Identity Platform 其實是 Firebase Auth 的企業升級版，是支援匯入功能的。",
      "en": "Option (A) correctly applies Identity Platform (Google Cloud's enterprise-grade CIAM solution). It supports importing password hashes of various standard encryption algorithms, achieving 'seamless migration without forced password resets', and natively supports social login and large-scale scalability, perfectly fitting Altostrat's needs as a media platform facing massive consumers. Option (B) self-hosting Keycloak contradicts the 'simplified management' and 'fully managed' principles; Option (C) Cloud Identity is primarily for internal Workforce management, not Customer users; Option (D) forcing users to reset passwords severely harms User Experience (UX) and increases churn, and Identity Platform (the enterprise upgrade of Firebase Auth) actually does support import capabilities.",
      "wg": [
        { "t": "企業級", "en": "enterprise-grade", "ps": "adj" },
        { "t": "流失率", "en": "churn", "ps": "n" }
      ]
    }
  },
  {
    "no": "10",
    "level": "hard",
    "keywords": "Software Supply Chain, Security, Binary Authorization, CI/CD",
    "question": [
      {
        "t": "Altostrat 正致力於「現代化 CI/CD 以進行容器化部署」，但最近的行業安全事件讓資安團隊非常擔心軟體供應鏈攻擊。他們要求在部署任何容器至 GKE 生產環境之前，必須經過嚴格的驗證。",
        "en": "Altostrat is working on 'modernizing CI/CD for containerized deployments', but recent industry security incidents have the security team very concerned about software supply chain attacks. They require strict verification before deploying any container to the GKE production environment.",
        "wg": [
          { "t": "軟體供應鏈攻擊", "en": "software supply chain attacks", "ps": "n" },
          { "t": "嚴格的驗證", "en": "strict verification", "ps": "n" }
        ]
      },
      {
        "t": "具體要求包括：1. 確保映像檔是由受信任的 Cloud Build 管道建構的。2. 確保映像檔已通過漏洞掃描且無重大弱點。3. 確保部署流程不可被繞過。",
        "en": "Specific requirements include: 1. Ensure images are built by trusted Cloud Build pipelines. 2. Ensure images have passed vulnerability scanning with no critical vulnerabilities. 3. Ensure the deployment process cannot be bypassed.",
        "wg": [
          { "t": "受信任的", "en": "trusted", "ps": "adj" },
          { "t": "繞過", "en": "bypassed", "ps": "v" }
        ]
      },
      {
        "t": "請選擇兩項步驟來構建此安全防護網。(請選擇兩項)",
        "en": "Choose two steps to build this security safety net. (Choose two)",
        "wg": [
          { "t": "安全防護網", "en": "security safety net", "ps": "n" },
          { "t": "構建", "en": "build", "ps": "v" }
        ]
      }
    ],
    "type": "複選題",
    "options": [
      {
        "t": "(A) 在 Cloud Build 管道中加入 Artifact Analysis 掃描步驟；若發現重大漏洞則使建構失敗。利用 Cloud KMS 對映像檔進行加密儲存。",
        "en": "(A) Add an Artifact Analysis scan step in the Cloud Build pipeline; fail the build if critical vulnerabilities are found. Use Cloud KMS to encrypt the stored images.",
        "wg": [
          { "t": "掃描步驟", "en": "scan step", "ps": "n" },
          { "t": "加密儲存", "en": "encrypt", "ps": "v" }
        ]
      },
      {
        "t": "(B) 啟用 Binary Authorization，並配置政策要求所有部署映像檔必須附帶由 Cloud Build 建立的「建構者證明 (Attestation)」以及漏洞掃描通過的證明。",
        "en": "(B) Enable Binary Authorization and configure policies requiring all deployed images to carry a 'Builder Attestation' created by Cloud Build and an attestation of passing vulnerability scanning.",
        "wg": [
          { "t": "證明", "en": "Attestation", "ps": "n" },
          { "t": "政策", "en": "policies", "ps": "n" }
        ]
      },
      {
        "t": "(C) 在 GKE 叢集中啟用「防護模式 (Shielded Nodes)」，並使用 Workload Identity 來限制 Pod 的權限，防止受駭容器攻擊底層節點。",
        "en": "(C) Enable 'Shielded Nodes' in the GKE cluster and use Workload Identity to restrict Pod permissions, preventing compromised containers from attacking underlying nodes.",
        "wg": [
          { "t": "防護模式", "en": "Shielded Nodes", "ps": "n" },
          { "t": "受駭", "en": "compromised", "ps": "adj" }
        ]
      },
      {
        "t": "(D) 在 Cloud Build 建立映像檔後，使用 Kritis Signer 或 Cloud Build 原生功能對映像檔摘要 (Digest) 進行數位簽章，並將簽章作為證明 (Attestation) 儲存在 Container Analysis 中。",
        "en": "(D) After Cloud Build creates the image, use Kritis Signer or Cloud Build native features to digitally sign the image digest and store the signature as an Attestation in Container Analysis.",
        "wg": [
          { "t": "數位簽章", "en": "digitally sign", "ps": "v" },
          { "t": "摘要", "en": "Digest", "ps": "n" }
        ]
      },
      {
        "t": "(E) 設定 VPC Service Controls 邊界，將 GKE 叢集與 Artifact Registry 包含在內，確保只有來自內部網路的流量可以拉取映像檔。",
        "en": "(E) Configure VPC Service Controls perimeters to include GKE clusters and Artifact Registry, ensuring only traffic from the internal network can pull images.",
        "wg": [
          { "t": "邊界", "en": "perimeters", "ps": "n" },
          { "t": "拉取", "en": "pull", "ps": "v" }
        ]
      }
    ],
    "answer": "(B), (D)",
    "why": {
      "t": "選項 (B) 和 (D) 共同構成了完整的軟體供應鏈安全解決方案。選項 (D) 負責在建構階段「產生證明 (Create Attestation)」，即當映像檔成功構建並通過掃描後，對其進行數位簽章。選項 (B) 則是在部署階段「強制執行證明 (Enforce Attestation)」，Binary Authorization 會拒絕任何沒有正確簽章的映像檔部署至 GKE。這兩者結合確保了只有經過受信任管道驗證的程式碼能上線。選項 (A) 加密映像檔是針對資料靜態安全，不防止惡意軟體注入；選項 (C) Shielded Nodes 保護節點層級，不防止應用程式層級的供應鏈攻擊；選項 (E) VPC-SC 防止資料外洩，但不驗證軟體來源的完整性。",
      "en": "Options (B) and (D) together form a complete software supply chain security solution. Option (D) handles 'Create Attestation' during the build phase, digitally signing the image after it is successfully built and scanned. Option (B) handles 'Enforce Attestation' during the deployment phase, where Binary Authorization rejects any image without valid signatures from deploying to GKE. Combining these ensures only code verified by trusted pipelines goes live. Option (A) encrypting images addresses data-at-rest security, not malware injection; Option (C) Shielded Nodes protect the node level, not preventing application-level supply chain attacks; Option (E) VPC-SC prevents data exfiltration but does not verify the integrity of the software source.",
      "wg": [
        { "t": "軟體供應鏈", "en": "software supply chain", "ps": "n" },
        { "t": "強制執行", "en": "Enforce", "ps": "v" }
      ]
    }
  },
  {
    "no": "11",
    "level": "hard",
    "keywords": "GKE Enterprise, Multi-Cluster Ingress, Global Load Balancing",
    "question": [
      {
        "t": "為了實現「賦能客戶無縫自助服務支援」與全球內容傳遞的目標，Altostrat 在美洲、歐洲與亞太地區的多個 Google Cloud 區域以及地端資料中心部署了 GKE 叢集。",
        "en": "To achieve the goal of 'empowering customers with seamless self-service support' and global content delivery, Altostrat has deployed GKE clusters across multiple Google Cloud regions in the Americas, Europe, and Asia-Pacific, as well as in on-premises data centers.",
        "wg": [
          { "t": "無縫", "en": "seamless", "ps": "adj" },
          { "t": "部署", "en": "deployed", "ps": "v" }
        ]
      },
      {
        "t": "目前使用者在跨區存取時面臨高延遲，且單一區域故障時需要手動切換流量，這不符合 99.99% 高可用性的目標。您需要設計一個統一的流量進入點，能夠根據使用者的地理位置將其導向最近的健康叢集，並在發生故障時自動容錯移轉。",
        "en": "Users currently face high latency when accessing across regions, and manual traffic switching is required during single-region failures, which does not meet the 99.99% high availability goal. You need to design a unified traffic entry point that routes users to the nearest healthy cluster based on their geolocation and automatically fails over during outages.",
        "wg": [
          { "t": "統一的流量進入點", "en": "unified traffic entry point", "ps": "n" },
          { "t": "容錯移轉", "en": "fails over", "ps": "v" }
        ]
      },
      {
        "t": "解決方案需盡可能減少對應用程式程式碼的更改。",
        "en": "The solution should minimize changes to the application code as much as possible.",
        "wg": [
          { "t": "盡可能減少", "en": "minimize", "ps": "v" },
          { "t": "更改", "en": "changes", "ps": "n" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "(A) 在每個區域部署獨立的 HTTP(S) 負載平衡器，並設定 Cloud DNS 地理位置路由策略 (Geolocation Routing Policy)；設定 DNS 健康檢查以在端點無回應時更新 DNS 紀錄。",
        "en": "(A) Deploy independent HTTP(S) Load Balancers in each region and configure Cloud DNS Geolocation Routing Policies; set up DNS health checks to update DNS records when endpoints are unresponsive.",
        "wg": [
          { "t": "地理位置路由策略", "en": "Geolocation Routing Policy", "ps": "n" },
          { "t": "DNS 紀錄", "en": "DNS records", "ps": "n" }
        ]
      },
      {
        "t": "(B) 使用 Traffic Director 建立全域服務網格 (Global Service Mesh)，將所有 GKE 叢集加入網格中；透過 Sidecar 代理程式控制流量分配與故障轉移邏輯。",
        "en": "(B) Use Traffic Director to create a Global Service Mesh, adding all GKE clusters to the mesh; control traffic distribution and failover logic via Sidecar proxies.",
        "wg": [
          { "t": "服務網格", "en": "Service Mesh", "ps": "n" },
          { "t": "代理程式", "en": "proxies", "ps": "n" }
        ]
      },
      {
        "t": "(C) 啟用 GKE Enterprise (Anthos) 的多叢集 Ingress (Multi-Cluster Ingress, MCI) 功能。部署 `MultiClusterIngress` 資源以設定全域外部 HTTP(S) 負載平衡器，並透過 `MultiClusterService` 定義跨叢集的後端服務。",
        "en": "(C) Enable the Multi-Cluster Ingress (MCI) feature of GKE Enterprise (Anthos). Deploy a `MultiClusterIngress` resource to configure a global external HTTP(S) Load Balancer, and define cross-cluster backend services via `MultiClusterService`.",
        "wg": [
          { "t": "多叢集 Ingress", "en": "Multi-Cluster Ingress", "ps": "n" },
          { "t": "全域外部", "en": "global external", "ps": "adj" }
        ]
      },
      {
        "t": "(D) 配置全域 Anycast IP，將其綁定到所有區域的 GKE Ingress Controller；利用 BGP 協定自動將使用者流量導向網路路徑最短的區域。",
        "en": "(D) Configure a global Anycast IP and bind it to the GKE Ingress Controllers in all regions; use the BGP protocol to automatically route user traffic to the region with the shortest network path.",
        "wg": [
          { "t": "綁定", "en": "bind", "ps": "v" },
          { "t": "最短", "en": "shortest", "ps": "adj" }
        ]
      }
    ],
    "answer": "(C)",
    "why": {
      "t": "選項 (C) 是管理多區域 GKE 叢集流量的最佳解決方案。Multi-Cluster Ingress (MCI) 專為 GKE Enterprise 設計，它自動配置單一全域負載平衡器 (Global Load Balancer)，能根據使用者鄰近度與叢集健康狀態智慧路由流量，完全符合「統一進入點」與「自動容錯移轉」的需求。選項 (A) 依賴 DNS 進行故障轉移會受到 TTL (存活時間) 快取影響，導致停機時間過長，無法達到即時的高可用性；選項 (B) Traffic Director 主要用於服務對服務 (East-West) 的流量管理，而非面向使用者的外部 (North-South) 流量進入點；選項 (D) 描述的 Anycast/BGP 設定過於底層且複雜，通常由 Google Cloud 負載平衡器自動處理，並非標準的 GKE 設定方式。",
      "en": "Option (C) is the optimal solution for managing traffic across multi-region GKE clusters. Multi-Cluster Ingress (MCI) is designed for GKE Enterprise; it automatically provisions a single Global Load Balancer that intelligently routes traffic based on user proximity and cluster health, perfectly meeting the 'unified entry point' and 'automatic failover' requirements. Option (A) relying on DNS for failover is subject to TTL caching, leading to prolonged downtime and failing to meet real-time high availability; Option (B) Traffic Director is primarily for service-to-service (East-West) traffic management, not user-facing external (North-South) ingress; Option (D) describes low-level Anycast/BGP settings that are overly complex and typically handled automatically by Google Cloud Load Balancers, not a standard GKE configuration.",
      "wg": [
        { "t": "鄰近度", "en": "proximity", "ps": "n" },
        { "t": "存活時間", "en": "TTL", "ps": "n" }
      ]
    }
  },
  {
    "no": "12",
    "level": "hard",
    "keywords": "BigQuery, Partitioning, Clustering, Cost Optimization",
    "question": [
      {
        "t": "Altostrat 使用 BigQuery 作為主要資料倉儲，儲存了數 PB 的使用者觀看紀錄與點擊流數據。分析師需要頻繁執行查詢以「識別趨勢並提取洞察」，例如查詢「特定使用者在過去 30 天內的觀看行為」。",
        "en": "Altostrat uses BigQuery as its primary data warehouse, storing petabytes of user viewing history and clickstream data. Analysts frequently execute queries to 'identify trends and extract insights', such as querying 'specific user viewing behavior over the last 30 days'.",
        "wg": [
          { "t": "點擊流數據", "en": "clickstream data", "ps": "n" },
          { "t": "頻繁執行", "en": "frequently execute", "ps": "v" }
        ]
      },
      {
        "t": "目前的查詢成本過高且掃描速度緩慢，因為每次查詢幾乎都會掃描整張資料表。您需要優化資料表架構以降低成本並提升查詢效能，同時必須確保這些變更對現有的 SQL 查詢語句影響最小。",
        "en": "Current query costs are excessive and scanning is slow because almost every query scans the entire table. You need to optimize the table schema to reduce costs and improve query performance, while ensuring these changes have minimal impact on existing SQL query statements.",
        "wg": [
          { "t": "優化", "en": "optimize", "ps": "v" },
          { "t": "影響最小", "en": "minimal impact", "ps": "n" }
        ]
      },
      {
        "t": "請選擇最有效的優化策略。",
        "en": "Choose the most effective optimization strategy.",
        "wg": [
          { "t": "優化策略", "en": "optimization strategy", "ps": "n" },
          { "t": "最有效的", "en": "most effective", "ps": "adj" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "(A) 建立具體化視圖 (Materialized Views) 來預先計算常見的聚合查詢結果；啟用智慧調整 (Smart Tuning) 以自動管理視圖的更新。",
        "en": "(A) Create Materialized Views to pre-compute common aggregation query results; enable Smart Tuning to automatically manage view updates.",
        "wg": [
          { "t": "具體化視圖", "en": "Materialized Views", "ps": "n" },
          { "t": "聚合查詢", "en": "aggregation query", "ps": "n" }
        ]
      },
      {
        "t": "(B) 將現有的大型資料表依據 `timestamp` 欄位進行分區 (Partitioning)，並依據 `user_id` 欄位進行叢集 (Clustering)。",
        "en": "(B) Partition the existing large tables based on the `timestamp` column and Cluster them based on the `user_id` column.",
        "wg": [
          { "t": "分區", "en": "Partitioning", "ps": "v" },
          { "t": "叢集", "en": "Clustering", "ps": "v" }
        ]
      },
      {
        "t": "(C) 使用 BigQuery Data Transfer Service 將舊資料定期封存至 Cloud Storage Coldline，僅在 BigQuery 中保留最近 30 天的熱數據。",
        "en": "(C) Use BigQuery Data Transfer Service to regularly archive old data to Cloud Storage Coldline, keeping only the last 30 days of hot data in BigQuery.",
        "wg": [
          { "t": "定期封存", "en": "regularly archive", "ps": "v" },
          { "t": "熱數據", "en": "hot data", "ps": "n" }
        ]
      },
      {
        "t": "(D) 將資料表轉換為外聯資料表 (External Tables)，讓資料直接儲存在 Google Cloud Storage 中以節省儲存成本，並利用 Hive 分區佈局。",
        "en": "(D) Convert tables to External Tables, allowing data to be stored directly in Google Cloud Storage to save on storage costs, and utilize Hive partition layout.",
        "wg": [
          { "t": "外聯資料表", "en": "External Tables", "ps": "n" },
          { "t": "節省", "en": "save", "ps": "v" }
        ]
      }
    ],
    "answer": "(B)",
    "why": {
      "t": "選項 (B) 是針對這類查詢模式（過濾時間範圍 + 特定使用者 ID）的標準最佳實踐。分區 (Partitioning) 確保查詢只掃描特定日期的數據（大幅減少位元組讀取量，降低成本），而叢集 (Clustering) 則將相同 `user_id` 的資料實體上儲存在一起，進一步加速過濾效能。這不需要更改 SQL 語法。選項 (A) 具體化視圖適用於「聚合」(SUM, AVG) 查詢，但題目提到的是查詢「觀看行為」等明細數據，且無法解決原始資料掃描的根本低效問題；選項 (C) 將數據移出會導致分析師無法查詢歷史趨勢，違背業務需求；選項 (D) 外聯資料表通常比原生儲存查詢速度慢且不支援某些優化功能，無法解決效能問題。",
      "en": "Option (B) is the standard best practice for this query pattern (filtering by time range + specific user ID). Partitioning ensures the query only scans data for specific dates (drastically reducing bytes read and cost), while Clustering physically stores data with the same `user_id` together, further accelerating filtering performance. This requires no changes to SQL syntax. Option (A) Materialized Views are for 'aggregation' (SUM, AVG) queries, but the question mentions querying detailed 'viewing behavior', and it doesn't solve the fundamental inefficiency of raw data scanning; Option (C) moving data out prevents analysts from querying historical trends, violating business requirements; Option (D) External Tables are typically slower than native storage and lack certain optimizations, failing to solve the performance issue.",
      "wg": [
        { "t": "標準最佳實踐", "en": "standard best practice", "ps": "n" },
        { "t": "原始資料", "en": "raw data", "ps": "n" }
      ]
    }
  },
  {
    "no": "13",
    "level": "hard",
    "keywords": "Data Loss Prevention, PII, Compliance, Log Analysis",
    "question": [
      {
        "t": "作為 Altostrat 的資安架構師，您必須解決「偵測並過濾不當內容」與合規性的挑戰。行銷團隊計畫將使用者的客服對話紀錄匯入 BigQuery 進行情感分析，但這些紀錄中包含大量個人識別資訊 (PII)，如電子郵件與電話號碼。",
        "en": "As Altostrat's Security Architect, you must address the challenges of 'detecting and filtering inappropriate content' and compliance. The marketing team plans to import user customer service chat logs into BigQuery for sentiment analysis, but these logs contain significant Personally Identifiable Information (PII), such as emails and phone numbers.",
        "wg": [
          { "t": "情感分析", "en": "sentiment analysis", "ps": "n" },
          { "t": "個人識別資訊", "en": "Personally Identifiable Information", "ps": "n" }
        ]
      },
      {
        "t": "為了符合 GDPR 等法規，您必須確保在資料寫入 BigQuery 之前，所有敏感資訊都已被去識別化 (De-identified)，同時保留部分資訊格式以供分析（例如保留電子郵件的網域部分）。",
        "en": "To comply with regulations like GDPR, you must ensure that all sensitive information is De-identified before data is written to BigQuery, while preserving some information format for analysis (e.g., keeping the domain part of email addresses).",
        "wg": [
          { "t": "去識別化", "en": "De-identified", "ps": "v" },
          { "t": "保留", "en": "preserving", "ps": "v" }
        ]
      },
      {
        "t": "您應該如何設計這個資料處理流程？",
        "en": "How should you design this data processing flow?",
        "wg": [
          { "t": "資料處理流程", "en": "data processing flow", "ps": "n" },
          { "t": "設計", "en": "design", "ps": "v" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "(A) 在應用程式層級使用正則表達式 (Regex) 替換所有 PII，然後將清洗後的資料寫入 BigQuery；使用 Cloud IAM 限制誰可以存取原始日誌。",
        "en": "(A) Use Regular Expressions (Regex) at the application level to replace all PII, then write the cleansed data to BigQuery; use Cloud IAM to restrict who can access the raw logs.",
        "wg": [
          { "t": "正則表達式", "en": "Regular Expressions", "ps": "n" },
          { "t": "清洗後的", "en": "cleansed", "ps": "adj" }
        ]
      },
      {
        "t": "(B) 建立一個 Dataflow 管道讀取原始日誌。在管道中使用 Cloud Data Loss Prevention (DLP) API 的 `deidentify` 轉換，配置遮罩 (Masking) 或替換 (Replacement) 規則來處理 PII，最後將結果寫入 BigQuery。",
        "en": "(B) Create a Dataflow pipeline to read raw logs. Use the Cloud Data Loss Prevention (DLP) API's `deidentify` transform within the pipeline, configuring Masking or Replacement rules to handle PII, and finally write the results to BigQuery.",
        "wg": [
          { "t": "管道", "en": "pipeline", "ps": "n" },
          { "t": "轉換", "en": "transform", "ps": "n" }
        ]
      },
      {
        "t": "(C) 將資料直接匯入 BigQuery，然後使用 BigQuery 的列級別安全性 (Column-level Security) 與動態資料遮罩 (Dynamic Data Masking) 功能，設定政策讓分析師只能看到遮罩後的數據。",
        "en": "(C) Import data directly into BigQuery, then use BigQuery's Column-level Security and Dynamic Data Masking features to set policies so analysts can only see masked data.",
        "wg": [
          { "t": "列級別安全性", "en": "Column-level Security", "ps": "n" },
          { "t": "動態資料遮罩", "en": "Dynamic Data Masking", "ps": "n" }
        ]
      },
      {
        "t": "(D) 使用 Vertex AI 的 AutoML Natural Language 模型訓練一個自定義實體提取器 (Entity Extractor)，識別 PII 並將其移除，再將資料儲存至 Cloud Storage。",
        "en": "(D) Use Vertex AI's AutoML Natural Language model to train a custom Entity Extractor to identify and remove PII, then store the data in Cloud Storage.",
        "wg": [
          { "t": "實體提取器", "en": "Entity Extractor", "ps": "n" },
          { "t": "移除", "en": "remove", "ps": "v" }
        ]
      }
    ],
    "answer": "(B)",
    "why": {
      "t": "選項 (B) 是處理大規模 PII 去識別化的標準架構。Cloud DLP API 專門設計用於精確識別和轉換敏感數據（如遮罩、加密格式保留等），結合 Dataflow 可以實現在數據進入儲存層（BigQuery）之前的「傳輸中 (In-flight)」清洗，這最符合「在寫入之前」的要求，從根本上降低合規風險。選項 (A) 使用正則表達式維護困難且容易遺漏新型態的 PII；選項 (C) 雖然有效，但原始 PII 仍然儲存在 BigQuery 中，對於要求嚴格的合規場景（如 GDPR 的被遺忘權或資料最小化原則），儲存原始 PII 本身就是風險；選項 (D) 訓練自定義 AI 模型成本高昂且不如 DLP API 準確且現成可用。",
      "en": "Option (B) is the standard architecture for handling large-scale PII de-identification. The Cloud DLP API is specifically designed to accurately identify and transform sensitive data (e.g., masking, format-preserving encryption), and combined with Dataflow, it enables 'In-flight' cleansing before data enters the storage layer (BigQuery), which best fits the 'before writing' requirement and fundamentally lowers compliance risk. Option (A) using Regex is hard to maintain and prone to missing new PII types; Option (C) is effective for access control, but the raw PII is still stored in BigQuery, which poses a risk for strict compliance scenarios (like GDPR's right to be forgotten or data minimization principles); Option (D) training a custom AI model is costly and less accurate or ready-to-use than the DLP API.",
      "wg": [
        { "t": "傳輸中", "en": "In-flight", "ps": "adj" },
        { "t": "資料最小化", "en": "data minimization", "ps": "n" }
      ]
    }
  },
  {
    "no": "14",
    "level": "hard",
    "keywords": "Cloud Run, Serverless Security, Hybrid Connectivity, VPC Connector",
    "question": [
      {
        "t": "Altostrat 使用 Cloud Run functions 執行事件驅動的任務，例如視訊轉碼與中繼資料擷取。其中一個函式需要連接至位於地端資料中心的舊有授權伺服器 (Legacy Licensing Server) 以驗證版權。",
        "en": "Altostrat uses Cloud Run functions for event-driven tasks such as video transcoding and metadata extraction. One of the functions needs to connect to a legacy Licensing Server located in the on-premises data center to verify copyrights.",
        "wg": [
          { "t": "授權伺服器", "en": "Licensing Server", "ps": "n" },
          { "t": "版權", "en": "copyrights", "ps": "n" }
        ]
      },
      {
        "t": "出於安全考量，該地端伺服器僅透過 Cloud Interconnect 私有連線與 Google Cloud 的 VPC 溝通，不接受來自網際網路的流量。同時，該 Cloud Run 函式本身必須嚴格禁止從公共網際網路觸發，僅允許由內部的 Cloud Storage 事件觸發。",
        "en": "For security reasons, the on-premises server communicates with the Google Cloud VPC only via a private Cloud Interconnect connection and does not accept traffic from the internet. Additionally, the Cloud Run function itself must be strictly prohibited from being triggered from the public internet, allowing triggers only from internal Cloud Storage events.",
        "wg": [
          { "t": "私有連線", "en": "private connection", "ps": "n" },
          { "t": "禁止", "en": "prohibited", "ps": "v" }
        ]
      },
      {
        "t": "您需要配置 Cloud Run 函式的網路設定以滿足這些需求。請選擇兩項必要的設定。(請選擇兩項)",
        "en": "You need to configure the networking settings for the Cloud Run function to meet these requirements. Choose two necessary settings. (Choose two)",
        "wg": [
          { "t": "網路設定", "en": "networking settings", "ps": "n" },
          { "t": "必要的", "en": "necessary", "ps": "adj" }
        ]
      }
    ],
    "type": "複選題",
    "options": [
      {
        "t": "(A) 將函式的「流量進入 (Ingress)」設定配置為「僅限內部 (Internal Only)」。",
        "en": "(A) Configure the function's 'Ingress' setting to 'Internal Only'.",
        "wg": [
          { "t": "流量進入", "en": "Ingress", "ps": "n" },
          { "t": "僅限內部", "en": "Internal Only", "ps": "n" }
        ]
      },
      {
        "t": "(B) 建立一個無伺服器 VPC 存取連接器 (Serverless VPC Access Connector)，並將函式配置為使用此連接器路由所有流量。",
        "en": "(B) Create a Serverless VPC Access Connector and configure the function to route all traffic using this connector.",
        "wg": [
          { "t": "無伺服器 VPC 存取連接器", "en": "Serverless VPC Access Connector", "ps": "n" },
          { "t": "路由", "en": "route", "ps": "v" }
        ]
      },
      {
        "t": "(C) 在 VPC 中設定 Cloud NAT 閘道，以允許無伺服器函式擁有固定的對外 IP 位址。",
        "en": "(C) Set up a Cloud NAT gateway in the VPC to allow the serverless function to have a fixed outbound IP address.",
        "wg": [
          { "t": "閘道", "en": "gateway", "ps": "n" },
          { "t": "固定的", "en": "fixed", "ps": "adj" }
        ]
      },
      {
        "t": "(D) 使用 Identity-Aware Proxy (IAP) 包裝該函式，並僅授權給具有 `Storage Object Creator` 角色的服務帳戶。",
        "en": "(D) Wrap the function with Identity-Aware Proxy (IAP) and authorize only service accounts with the `Storage Object Creator` role.",
        "wg": [
          { "t": "包裝", "en": "Wrap", "ps": "v" },
          { "t": "授權", "en": "authorize", "ps": "v" }
        ]
      },
      {
        "t": "(E) 配置 VPC Service Controls 邊界，將 Cloud Run 函式專案排除在邊界之外以確保連線。",
        "en": "(E) Configure VPC Service Controls perimeters, excluding the Cloud Run function project from the perimeter to ensure connectivity.",
        "wg": [
          { "t": "排除", "en": "excluding", "ps": "v" },
          { "t": "邊界", "en": "perimeter", "ps": "n" }
        ]
      }
    ],
    "answer": "(A), (B)",
    "why": {
      "t": "選項 (A) 和 (B) 共同滿足了「私有地端連線」與「禁止公開觸發」的雙重需求。選項 (A) 將 Ingress 設定為「Internal Only」確保了函式無法被網際網路上的任何人呼叫，只能被相同專案或 VPC 內的資源（如 Eventarc/Cloud Storage 觸發器）呼叫。選項 (B) 的 Serverless VPC Access Connector (或使用 Direct VPC Egress) 是讓無伺服器環境能夠存取 VPC 內部資源（進而透過 Interconnect 存取地端）的必要元件。選項 (C) Cloud NAT 是用於存取外部網際網路，而非地端私有網路；選項 (D) IAP 是用於控制使用者對 Web 應用程式的存取，不適用於後端事件驅動的流程；選項 (E) 排除在 VPC-SC 之外反而會降低安全性，且與連線需求無直接關係。",
      "en": "Options (A) and (B) together satisfy the dual requirements of 'private on-premises connectivity' and 'prohibiting public triggers'. Option (A) setting Ingress to 'Internal Only' ensures the function cannot be invoked by anyone on the internet, only by resources within the same project or VPC (like Eventarc/Cloud Storage triggers). Option (B) Serverless VPC Access Connector (or using Direct VPC Egress) is the necessary component to allow the serverless environment to access internal VPC resources (and subsequently on-premises via Interconnect). Option (C) Cloud NAT is for accessing the external internet, not private on-premises networks; Option (D) IAP is for controlling user access to web apps, not applicable to backend event-driven flows; Option (E) excluding from VPC-SC would actually lower security and is unrelated to the connectivity requirement.",
      "wg": [
        { "t": "觸發器", "en": "triggers", "ps": "n" },
        { "t": "元件", "en": "component", "ps": "n" }
      ]
    }
  },
  {
    "no": "15",
    "level": "hard",
    "keywords": "Vertex AI, MLOps, Lineage, Auditability",
    "question": [
      {
        "t": "Altostrat 的資料科學團隊正在 Vertex AI 上開發自定義的推薦模型。為了滿足業務需求中的「確保 AI 系統可被稽核 (Auditable) 且其決策可以被解釋」，您需要設計一個 MLOps 流程。",
        "en": "Altostrat's data science team is developing custom recommendation models on Vertex AI. To meet the business requirement to 'ensure that AI systems are auditable and their decisions can be explained', you need to design an MLOps process.",
        "wg": [
          { "t": "推薦模型", "en": "recommendation models", "ps": "n" },
          { "t": "流程", "en": "process", "ps": "n" }
        ]
      },
      {
        "t": "該流程必須能夠自動追蹤模型的血緣關係（Lineage），包括使用了哪些版本的數據集、由哪段程式碼訓練、以及產生了哪些模型構件。此外，對於每個預測結果，系統必須能夠提供特徵歸因 (Feature Attribution) 分析，說明哪些輸入特徵對結果影響最大。",
        "en": "The process must automatically track model Lineage, including which dataset versions were used, which code trained it, and what model artifacts were produced. Additionally, for each prediction result, the system must provide Feature Attribution analysis, explaining which input features most influenced the result.",
        "wg": [
          { "t": "血緣關係", "en": "Lineage", "ps": "n" },
          { "t": "特徵歸因", "en": "Feature Attribution", "ps": "n" }
        ]
      },
      {
        "t": "您應該整合哪些 Vertex AI 元件？",
        "en": "Which Vertex AI components should you integrate?",
        "wg": [
          { "t": "整合", "en": "integrate", "ps": "v" },
          { "t": "元件", "en": "components", "ps": "n" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "(A) 使用 Vertex AI TensorBoard 記錄訓練過程中的損失函數變化；使用 What-If Tool 進行手動的模型公平性分析。",
        "en": "(A) Use Vertex AI TensorBoard to log loss function changes during training; use the What-If Tool for manual model fairness analysis.",
        "wg": [
          { "t": "損失函數", "en": "loss function", "ps": "n" },
          { "t": "公平性分析", "en": "fairness analysis", "ps": "n" }
        ]
      },
      {
        "t": "(B) 使用 Vertex ML Metadata 自動記錄管道執行的參數與構件血緣；配置 Vertex Explainable AI 在部署模型時啟用特徵歸因功能。",
        "en": "(B) Use Vertex ML Metadata to automatically record pipeline execution parameters and artifact lineage; configure Vertex Explainable AI to enable feature attribution when deploying the model.",
        "wg": [
          { "t": "構件", "en": "artifact", "ps": "n" },
          { "t": "部署", "en": "deploying", "ps": "v" }
        ]
      },
      {
        "t": "(C) 將所有訓練數據與模型二進位檔儲存在 Cloud Source Repositories；使用 BigQuery ML 的 `ML.EXPLAIN_PREDICT` 函式來解釋模型。",
        "en": "(C) Store all training data and model binaries in Cloud Source Repositories; use BigQuery ML's `ML.EXPLAIN_PREDICT` function to explain the model.",
        "wg": [
          { "t": "二進位檔", "en": "binaries", "ps": "n" },
          { "t": "解釋", "en": "explain", "ps": "v" }
        ]
      },
      {
        "t": "(D) 使用 Cloud Logging 記錄每一次的 `predict` API 呼叫內容；開發一個 Cloud Run 服務來定期分析日誌中的偏差。",
        "en": "(D) Use Cloud Logging to record the content of every `predict` API call; develop a Cloud Run service to periodically analyze the logs for bias.",
        "wg": [
          { "t": "偏差", "en": "bias", "ps": "n" },
          { "t": "定期", "en": "periodically", "ps": "adv" }
        ]
      }
    ],
    "answer": "(B)",
    "why": {
      "t": "選項 (B) 直接對應了題目中的兩個核心需求：Vertex ML Metadata 是專門用於追蹤機器學習工作流程中的血緣關係（數據 -> 管道 -> 模型）的服務，滿足「可稽核性」；而 Vertex Explainable AI 則提供了自動化的特徵歸因 (Feature Attribution)，滿足「可解釋性」。選項 (A) TensorBoard 僅用於視覺化訓練指標（如準確率），無法追蹤構件血緣；選項 (C) 雖然 BQML 有解釋功能，但題目情境是「自定義模型在 Vertex AI 上開發」，且 Source Repositories 不適合儲存大型二進位模型或數據集；選項 (D) 僅記錄日誌無法提供特徵權重解釋，且缺乏血緣追蹤能力。",
      "en": "Option (B) directly corresponds to the two core requirements: Vertex ML Metadata is the service specifically for tracking lineage (data -> pipeline -> model) in ML workflows, meeting 'auditability'; Vertex Explainable AI provides automated Feature Attribution, meeting 'explainability'. Option (A) TensorBoard is only for visualizing training metrics (like accuracy) and cannot track artifact lineage; Option (C) while BQML has explanation features, the scenario specifies 'custom models developed on Vertex AI', and Source Repositories is ill-suited for storing large binary models or datasets; Option (D) logging alone provides no feature weight explanation and lacks lineage tracking capabilities.",
      "wg": [
        { "t": "核心需求", "en": "core requirements", "ps": "n" },
        { "t": "特徵權重", "en": "feature weight", "ps": "n" }
      ]
    }
  },
  {
    "no": "16",
    "level": "hard",
    "keywords": "Cloud Storage, Cost Optimization, Autoclass, Media Library",
    "question": [
      {
        "t": "Altostrat 的「廣泛媒體庫」儲存了 PB 級的音訊與視訊檔案，且存取模式極度難以預測。某些舊的新聞廣播可能會因為當前事件突然變得熱門（病毒式傳播），而大多數內容則長期處於閒置狀態。",
        "en": "Altostrat's 'extensive media library' stores petabytes of audio and video files with highly unpredictable access patterns. Some old news broadcasts may suddenly become popular (viral) due to current events, while most content remains dormant for long periods.",
        "wg": [
          { "t": "難以預測", "en": "unpredictable", "ps": "adj" },
          { "t": "病毒式傳播", "en": "viral", "ps": "adj" }
        ]
      },
      {
        "t": "業務需求明確指出必須「最佳化雲端儲存成本」，同時不能增加管理負擔或犧牲熱門內容的存取效能。目前的靜態生命週期規則導致了頻繁的資料檢索費用 (Retrieval Fees)，或者將熱門資料錯誤地滯留在昂貴的標準儲存級別。",
        "en": "Business requirements explicitly state the need to 'optimize cloud storage costs' without increasing management overhead or sacrificing access performance for popular content. Current static lifecycle rules result in frequent data retrieval fees or mistakenly keep popular data in expensive Standard storage classes.",
        "wg": [
          { "t": "資料檢索費用", "en": "retrieval fees", "ps": "n" },
          { "t": "滯留", "en": "stranded", "ps": "v" }
        ]
      },
      {
        "t": "您需要選擇一個最「自動化」且「成本效益最高」的儲存策略。",
        "en": "You need to choose the most 'automated' and 'cost-effective' storage strategy.",
        "wg": [
          { "t": "自動化", "en": "automated", "ps": "adj" },
          { "t": "成本效益最高", "en": "cost-effective", "ps": "adj" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "(A) 實作物件生命週期管理 (OLM) 規則，設定物件在上傳 30 天後自動轉移至 Nearline，90 天後轉移至 Coldline；依靠 Cloud CDN 快取來處理病毒式內容的請求。",
        "en": "(A) Implement Object Lifecycle Management (OLM) rules to automatically transition objects to Nearline after 30 days and Coldline after 90 days; rely on Cloud CDN caching to handle requests for viral content.",
        "wg": [
          { "t": "物件生命週期管理", "en": "Object Lifecycle Management", "ps": "n" },
          { "t": "轉移", "en": "transition", "ps": "v" }
        ]
      },
      {
        "t": "(B) 啟用 Cloud Storage Autoclass 功能。該功能會根據每個物件的實際存取頻率，自動在 Standard、Nearline、Coldline 與 Archive 儲存級別之間移動物件，且無須支付檢索費用或提前刪除費用。",
        "en": "(B) Enable the Cloud Storage Autoclass feature. This feature automatically moves objects between Standard, Nearline, Coldline, and Archive storage classes based on each object's actual access frequency, with no retrieval fees or early deletion charges.",
        "wg": [
          { "t": "實際存取頻率", "en": "actual access frequency", "ps": "n" },
          { "t": "提前刪除費用", "en": "early deletion charges", "ps": "n" }
        ]
      },
      {
        "t": "(C) 使用 Cloud Monitoring 指標來識別熱門物件，並編寫 Cloud Run 排程作業 (Scheduled Job) 呼叫 Cloud Storage API，每天動態調整物件的儲存級別。",
        "en": "(C) Use Cloud Monitoring metrics to identify popular objects and write a Cloud Run Scheduled Job to call the Cloud Storage API, dynamically adjusting object storage classes daily.",
        "wg": [
          { "t": "排程作業", "en": "Scheduled Job", "ps": "n" },
          { "t": "動態調整", "en": "dynamically adjusting", "ps": "v" }
        ]
      },
      {
        "t": "(D) 將所有媒體內容預設儲存在 Nearline 儲存空間以節省基礎成本；若偵測到存取量激增，則透過 Storage Transfer Service 將該物件複製到 Standard 儲存桶中提供服務。",
        "en": "(D) Store all media content in Nearline storage by default to save base costs; if a surge in access is detected, use the Storage Transfer Service to copy the object to a Standard bucket for serving.",
        "wg": [
          { "t": "預設儲存", "en": "Store ... by default", "ps": "v" },
          { "t": "激增", "en": "surge", "ps": "n" }
        ]
      }
    ],
    "answer": "(B)",
    "why": {
      "t": "選項 (B) 是解決「存取模式不可預測」且需「最佳化成本」的最佳全自動解決方案。Autoclass 專為此場景設計，它消除了手動分層的複雜性，並且豁免了轉換過程中的檢索費用，這對於可能突然變熱門的歸檔數據至關重要。選項 (A) 的靜態規則無法應對病毒式內容（一旦冷數據變熱，讀取成本極高）；選項 (C) 需要自行開發與維護複雜的腳本，增加了營運負擔；選項 (D) 的資料複製會產生額外的儲存與操作成本，且反應速度不夠即時。",
      "en": "Option (B) is the optimal fully automated solution for 'unpredictable access patterns' and 'cost optimization'. Autoclass is designed for this scenario, eliminating the complexity of manual tiering and waiving retrieval fees during transitions, which is crucial for archived data that might suddenly become popular. Option (A)'s static rules cannot handle viral content (retrieval costs are high once cold data becomes hot); Option (C) requires self-development and maintenance of complex scripts, increasing operational burden; Option (D) data duplication incurs additional storage and operation costs, and the response speed is not real-time enough.",
      "wg": [
        { "t": "手動分層", "en": "manual tiering", "ps": "n" },
        { "t": "豁免", "en": "waiving", "ps": "v" }
      ]
    }
  },
  {
    "no": "17",
    "level": "hard",
    "keywords": "GKE, Security, Workload Identity, IAM",
    "question": [
      {
        "t": "Altostrat 的開發人員正在將多個微服務容器化並部署至 GKE。這些應用程式需要存取 BigQuery、Cloud Storage 和 Cloud Vision API。目前的做法是將 Service Account 的 JSON 金鑰掛載為 Kubernetes Secrets，但資安團隊認為這構成了極大的金鑰洩漏風險。",
        "en": "Altostrat developers are containerizing multiple microservices for deployment to GKE. These applications need to access BigQuery, Cloud Storage, and Cloud Vision API. The current practice involves mounting Service Account JSON keys as Kubernetes Secrets, but the security team identifies this as a significant key leakage risk.",
        "wg": [
          { "t": "掛載", "en": "mounting", "ps": "v" },
          { "t": "金鑰洩漏風險", "en": "key leakage risk", "ps": "n" }
        ]
      },
      {
        "t": "您需要設計一個符合 Google Cloud 最佳實務的身分驗證架構，徹底消除對長效型憑證 (Long-lived credentials) 的依賴，並確保每個微服務僅擁有其執行所需的最小權限 (Least Privilege)。",
        "en": "You need to design an authentication architecture that aligns with Google Cloud best practices, completely eliminating reliance on long-lived credentials, and ensuring each microservice possesses only the Least Privilege required for its execution.",
        "wg": [
          { "t": "長效型憑證", "en": "Long-lived credentials", "ps": "n" },
          { "t": "最小權限", "en": "Least Privilege", "ps": "n" }
        ]
      },
      {
        "t": "該解決方案必須易於管理且對程式碼的更動最小。",
        "en": "The solution must be easy to manage and require minimal code changes.",
        "wg": [
          { "t": "易於管理", "en": "easy to manage", "ps": "adj" },
          { "t": "程式碼的更動", "en": "code changes", "ps": "n" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "(A) 為 GKE 節點集區 (Node Pool) 的服務帳戶授予所有必要的 IAM 角色 (BigQuery Admin, Storage Admin 等)；依靠應用程式預設憑證 (ADC) 自動繼承節點的身分。",
        "en": "(A) Grant all necessary IAM roles (BigQuery Admin, Storage Admin, etc.) to the GKE Node Pool's service account; rely on Application Default Credentials (ADC) to automatically inherit the node's identity.",
        "wg": [
          { "t": "節點集區", "en": "Node Pool", "ps": "n" },
          { "t": "繼承", "en": "inherit", "ps": "v" }
        ]
      },
      {
        "t": "(B) 在 GKE 叢集上啟用 Workload Identity。建立 Kubernetes Service Account (KSA) 並將其與 Google Cloud IAM Service Account (GSA) 綁定；在 Pod 配置中指定該 KSA。",
        "en": "(B) Enable Workload Identity on the GKE cluster. Create a Kubernetes Service Account (KSA) and bind it to a Google Cloud IAM Service Account (GSA); specify this KSA in the Pod configuration.",
        "wg": [
          { "t": "綁定", "en": "bind", "ps": "v" },
          { "t": "Pod 配置", "en": "Pod configuration", "ps": "n" }
        ]
      },
      {
        "t": "(C) 使用 Cloud Key Management Service (KMS) 加密所有的 JSON 金鑰，並將其儲存在 Secret Manager 中；在應用程式啟動時透過 API 呼叫 Secret Manager 取得金鑰並解密。",
        "en": "(C) Use Cloud Key Management Service (KMS) to encrypt all JSON keys and store them in Secret Manager; have the application call Secret Manager API at startup to retrieve and decrypt the keys.",
        "wg": [
          { "t": "解密", "en": "decrypt", "ps": "v" },
          { "t": "啟動時", "en": "at startup", "ps": "adv" }
        ]
      },
      {
        "t": "(D) 部署一個 Sidecar 容器運行 HashiCorp Vault 代理程式，動態生成短期的 Google Cloud Access Tokens 並注入到應用程式的環境變數中。",
        "en": "(D) Deploy a Sidecar container running HashiCorp Vault agent to dynamically generate short-lived Google Cloud Access Tokens and inject them into the application's environment variables.",
        "wg": [
          { "t": "短期的", "en": "short-lived", "ps": "adj" },
          { "t": "注入", "en": "inject", "ps": "v" }
        ]
      }
    ],
    "answer": "(B)",
    "why": {
      "t": "選項 (B) 是 GKE 上安全存取 Google Cloud API 的標準最佳實務。Workload Identity 允許 Kubernetes 服務帳戶扮演 IAM 服務帳戶的角色，無需管理任何金鑰檔案，徹底消除了長效憑證洩漏的風險，並能針對每個 Pod/微服務實施精細的最小權限控制。選項 (A) 違反了最小權限原則，因為該節點上的所有 Pod 都會繼承相同的寬鬆權限；選項 (C) 雖然比 Kubernetes Secrets 安全，但仍涉及金鑰管理（Rotation）與應用程式修改（呼叫 Secret Manager API），不如 Workload Identity 透明；選項 (D) 引入了額外的複雜性（Vault 維護），除非有跨雲需求，否則原生 Workload Identity 是首選。",
      "en": "Option (B) is the standard best practice for securely accessing Google Cloud APIs on GKE. Workload Identity allows Kubernetes Service Accounts to impersonate IAM Service Accounts without managing any key files, completely eliminating the risk of long-lived credential leakage and enabling granular Least Privilege control for each Pod/microservice. Option (A) violates the principle of least privilege as all Pods on that node would inherit the same broad permissions; Option (C) while safer than Kubernetes Secrets, still involves key management (Rotation) and application modification (calling Secret Manager API), making it less transparent than Workload Identity; Option (D) introduces extra complexity (Vault maintenance), and unless there's a multi-cloud requirement, native Workload Identity is preferred.",
      "wg": [
        { "t": "透明", "en": "transparent", "ps": "adj" },
        { "t": "首選", "en": "preferred", "ps": "adj" }
      ]
    }
  },
  {
    "no": "18",
    "level": "hard",
    "keywords": "Cloud Spanner, Global Scale, Consistency, Revenue",
    "question": [
      {
        "t": "Altostrat 的新一代營收平台將處理全球使用者的訂閱狀態、觀看權限 (Entitlements) 以及即時動態定價數據。為了「推動營收成長」並「鞏固領導地位」，該資料庫必須具備全球高可用性 (99.999%)，並且在跨區域寫入時仍能保持強一致性 (Strong Consistency)。",
        "en": "Altostrat's next-generation revenue platform will handle global user subscription status, viewing entitlements, and real-time dynamic pricing data. To 'drive revenue growth' and 'solidify leadership', the database must possess global high availability (99.999%) and maintain Strong Consistency even during cross-region writes.",
        "wg": [
          { "t": "觀看權限", "en": "Entitlements", "ps": "n" },
          { "t": "強一致性", "en": "Strong Consistency", "ps": "n" }
        ]
      },
      {
        "t": "既有的關聯式資料庫在跨區擴展時面臨同步延遲與寫入瓶頸，而 NoSQL 解決方案則無法滿足處理金融交易所需的 ACID 事務特性。您需要推薦一個能夠同時滿足關聯式語義與水平擴展能力的託管資料庫服務。",
        "en": "Existing relational databases face synchronization latency and write bottlenecks when scaling across regions, while NoSQL solutions cannot meet the ACID transaction characteristics required for processing financial transactions. You need to recommend a managed database service that satisfies both relational semantics and horizontal scalability.",
        "wg": [
          { "t": "寫入瓶頸", "en": "write bottlenecks", "ps": "n" },
          { "t": "ACID 事務", "en": "ACID transaction", "ps": "n" }
        ]
      },
      {
        "t": "該選擇必須支援未來的業務快速增長。",
        "en": "The choice must support future rapid business growth.",
        "wg": [
          { "t": "快速增長", "en": "rapid growth", "ps": "n" },
          { "t": "支援", "en": "support", "ps": "v" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "(A) 使用 Cloud Bigtable 配置多叢集路由 (Multi-cluster routing) 以實現高吞吐量；在應用層實作交易邏輯以補償最終一致性 (Eventual Consistency)。",
        "en": "(A) Use Cloud Bigtable configured with Multi-cluster routing for high throughput; implement transaction logic at the application layer to compensate for Eventual Consistency.",
        "wg": [
          { "t": "多叢集路由", "en": "Multi-cluster routing", "ps": "n" },
          { "t": "最終一致性", "en": "Eventual Consistency", "ps": "n" }
        ]
      },
      {
        "t": "(B) 部署 Cloud SQL for PostgreSQL 並啟用跨區域唯讀複本 (Cross-Region Read Replicas)；將所有寫入導向至主要區域，讀取則分散至全球。",
        "en": "(B) Deploy Cloud SQL for PostgreSQL and enable Cross-Region Read Replicas; direct all writes to the primary region and distribute reads globally.",
        "wg": [
          { "t": "唯讀複本", "en": "Read Replicas", "ps": "n" },
          { "t": "分散", "en": "distribute", "ps": "v" }
        ]
      },
      {
        "t": "(C) 使用 Firestore 的多區域模式 (Multi-region mode)。利用其即時更新功能來同步使用者狀態，並依賴其原生的交易支援來處理訂閱變更。",
        "en": "(C) Use Firestore in Multi-region mode. Leverage its real-time updates feature to sync user status and rely on its native transaction support to handle subscription changes.",
        "wg": [
          { "t": "即時更新", "en": "real-time updates", "ps": "n" },
          { "t": "交易支援", "en": "transaction support", "ps": "n" }
        ]
      },
      {
        "t": "(D) 遷移至 Cloud Spanner 並配置多區域實例 (Multi-region instance)。利用其 TrueTime 技術實現的外部一致性 (External Consistency) 來處理全球分散式交易。",
        "en": "(D) Migrate to Cloud Spanner and configure a Multi-region instance. Leverage its External Consistency enabled by TrueTime technology to handle globally distributed transactions.",
        "wg": [
          { "t": "多區域實例", "en": "Multi-region instance", "ps": "n" },
          { "t": "外部一致性", "en": "External Consistency", "ps": "n" }
        ]
      }
    ],
    "answer": "(D)",
    "why": {
      "t": "選項 (D) Cloud Spanner 是唯一能同時提供全球水平擴展、99.999% 高可用性以及強一致性 (ACID) 交易的服務，完美契合「營收平台」對金融級數據準確性與全球規模的需求。選項 (A) Bigtable 不支援跨列交易且為最終一致性，不適合訂閱/支付系統；選項 (B) Cloud SQL 的寫入僅限於單一區域 (Single Writer)，無法滿足「全球寫入高可用」且跨區延遲較高；選項 (C) Firestore 雖然支援交易與多區域，但其寫入吞吐量與查詢複雜度限制使其不如 Spanner 適合大規模的核心營收系統（例如關聯式查詢需求）。",
      "en": "Option (D) Cloud Spanner is the only service that simultaneously provides global horizontal scalability, 99.999% high availability, and Strong Consistency (ACID) transactions, perfectly fitting the 'revenue platform' needs for financial-grade data accuracy and global scale. Option (A) Bigtable does not support cross-row transactions and is eventually consistent, making it unsuitable for subscription/payment systems; Option (B) Cloud SQL limits writes to a single region (Single Writer), failing to meet 'global write high availability' and suffering from cross-region latency; Option (C) Firestore, while supporting transactions and multi-region, has limitations in write throughput and query complexity that make it less suitable than Spanner for a large-scale core revenue system (e.g., relational query needs).",
      "wg": [
        { "t": "水平擴展", "en": "horizontal scalability", "ps": "n" },
        { "t": "關聯式查詢", "en": "relational query", "ps": "n" }
      ]
    }
  },
  {
    "no": "19",
    "level": "hard",
    "keywords": "Private Google Access, Hybrid Connectivity, Security, Data Ingestion",
    "question": [
      {
        "t": "Altostrat 在其舊有地端資料中心擁有一批負責內容轉碼的伺服器。這些伺服器沒有配置公共 IP 位址，且基於安全政策，嚴格禁止透過 NAT 存取公共網際網路。",
        "en": "Altostrat has a fleet of content transcoding servers in its legacy on-premises data center. These servers are not configured with public IP addresses, and per security policy, accessing the public internet via NAT is strictly prohibited.",
        "wg": [
          { "t": "嚴格禁止", "en": "strictly prohibited", "ps": "v" },
          { "t": "公共 IP 位址", "en": "public IP addresses", "ps": "n" }
        ]
      },
      {
        "t": "然而，這些伺服器必須能夠將處理後的媒體檔案上傳至 Google Cloud Storage，並將日誌寫入 Cloud Logging。您已建立了地端與 Google Cloud VPC 之間的 Cloud Interconnect 連線。",
        "en": "However, these servers must be able to upload processed media files to Google Cloud Storage and write logs to Cloud Logging. You have already established a Cloud Interconnect connection between on-premises and the Google Cloud VPC.",
        "wg": [
          { "t": "處理後的", "en": "processed", "ps": "adj" },
          { "t": "建立", "en": "established", "ps": "v" }
        ]
      },
      {
        "t": "您需要配置 VPC 網路以允許這些私有地端伺服器存取 Google API，同時維持「無公共網際網路存取」的合規狀態。",
        "en": "You need to configure the VPC network to allow these private on-premises servers to access Google APIs while maintaining the compliant state of 'no public internet access'.",
        "wg": [
          { "t": "合規狀態", "en": "compliant state", "ps": "n" },
          { "t": "維持", "en": "maintaining", "ps": "v" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "(A) 在 VPC 中部署 Cloud NAT 閘道，並配置防火牆規則僅允許地端 IP 範圍透過 NAT 存取 Google 的公共 IP 範圍。",
        "en": "(A) Deploy a Cloud NAT gateway in the VPC and configure firewall rules to allow only on-premises IP ranges to access Google's public IP ranges via NAT.",
        "wg": [
          { "t": "閘道", "en": "gateway", "ps": "n" },
          { "t": "範圍", "en": "ranges", "ps": "n" }
        ]
      },
      {
        "t": "(B) 為地端主機配置 Private Google Access for On-Premises hosts。在內部 DNS 伺服器中將 `*.googleapis.com` 解析為 `private.googleapis.com` 的虛擬 IP (199.36.153.8/30)，並透過 Interconnect 路由流量。",
        "en": "(B) Configure Private Google Access for On-Premises hosts. On internal DNS servers, resolve `*.googleapis.com` to the Virtual IP of `private.googleapis.com` (199.36.153.8/30), and route traffic via the Interconnect.",
        "wg": [
          { "t": "解析", "en": "resolve", "ps": "v" },
          { "t": "虛擬 IP", "en": "Virtual IP", "ps": "n" }
        ]
      },
      {
        "t": "(C) 建立 VPC Service Controls 邊界，並使用 Access Context Manager 將地端 IP 範圍加入允許清單；這將自動允許地端伺服器無需 IP 即可連接 API。",
        "en": "(C) Create a VPC Service Controls perimeter and use Access Context Manager to add on-premises IP ranges to the allowlist; this will automatically allow on-premises servers to connect to APIs without IPs.",
        "wg": [
          { "t": "允許清單", "en": "allowlist", "ps": "n" },
          { "t": "自動允許", "en": "automatically allow", "ps": "v" }
        ]
      },
      {
        "t": "(D) 在 VPC 中部署一個 Internal HTTP(S) Load Balancer 作為代理，後端指向 Google API 的全域端點；將地端應用程式的代理設定指向該負載平衡器 IP。",
        "en": "(D) Deploy an Internal HTTP(S) Load Balancer in the VPC as a proxy, with the backend pointing to Google API's global endpoints; point the on-premises application's proxy settings to this load balancer IP.",
        "wg": [
          { "t": "代理", "en": "proxy", "ps": "n" },
          { "t": "指向", "en": "point", "ps": "v" }
        ]
      }
    ],
    "answer": "(B)",
    "why": {
      "t": "選項 (B) 是 Google Cloud 提供的標準解決方案，稱為「地端主機的私人 Google 存取權 (Private Google Access for on-premises hosts)」。透過 DNS 劫持將 Google API 流量導向特定的私有 IP (199.36.153.4/30)，讓流量走 Interconnect 私有通道，完全不經過公共網際網路，符合安全政策。選項 (A) Cloud NAT 僅適用於 GCP 內的 VM，且會讓流量透過公共網際網路出口，違反「禁止公共網際網路存取」的政策；選項 (C) VPC-SC 是用於資料防護牆，本身不提供網路連通性 (Connectivity)，仍需基礎網路路徑；選項 (D) 內部負載平衡器不支援直接將 Google API 作為後端服務 (Backend Service)。",
      "en": "Option (B) is the standard solution provided by Google Cloud, known as 'Private Google Access for on-premises hosts'. By using DNS hijacking to direct Google API traffic to specific private IPs (199.36.153.4/30), traffic flows through the private Interconnect channel, completely bypassing the public internet, which aligns with the security policy. Option (A) Cloud NAT applies only to VMs within GCP and would route traffic via public internet egress, violating the 'no public internet access' policy; Option (C) VPC-SC is for data exfiltration protection but does not provide network connectivity itself; Option (D) Internal Load Balancers do not support directly adding Google APIs as Backend Services.",
      "wg": [
        { "t": "劫持", "en": "hijacking", "ps": "v" },
        { "t": "連通性", "en": "Connectivity", "ps": "n" }
      ]
    }
  },
  {
    "no": "20",
    "level": "hard",
    "keywords": "Compliance, Organization Policy, Audit Logging, Governance",
    "question": [
      {
        "t": "Altostrat 的法律團隊要求嚴格的資料主權 (Data Sovereignty) 合規性。所有的媒體資料與使用者個資必須僅儲存在「europe-west2 (倫敦)」與「us-central1」區域。此外，為了因應潛在的法律訴訟，所有的管理活動日誌必須被保留 7 年且不可被竄改。",
        "en": "Altostrat's legal team requires strict Data Sovereignty compliance. All media data and user PII must be stored only in 'europe-west2 (London)' and 'us-central1' regions. Additionally, to prepare for potential litigation, all administrative activity logs must be retained for 7 years and remain tamper-proof.",
        "wg": [
          { "t": "資料主權", "en": "Data Sovereignty", "ps": "n" },
          { "t": "不可被竄改", "en": "tamper-proof", "ps": "adj" }
        ]
      },
      {
        "t": "您需要配置組織層級的控制措施來強制執行這些限制，並確保日誌的長期留存。",
        "en": "You need to configure organization-level controls to enforce these restrictions and ensure long-term log retention.",
        "wg": [
          { "t": "組織層級", "en": "organization-level", "ps": "adj" },
          { "t": "長期留存", "en": "long-term retention", "ps": "n" }
        ]
      },
      {
        "t": "請選擇兩項設定來滿足這些需求。(請選擇兩項)",
        "en": "Choose two settings to meet these requirements. (Choose two)",
        "wg": [
          { "t": "設定", "en": "settings", "ps": "n" },
          { "t": "滿足", "en": "meet", "ps": "v" }
        ]
      }
    ],
    "type": "複選題",
    "options": [
      {
        "t": "(A) 定義一個組織政策 (Organization Policy)，設定 `gcp.resourceLocations` 限制條件，僅允許 `europe-west2` 和 `us-central1`。",
        "en": "(A) Define an Organization Policy, setting the `gcp.resourceLocations` constraint to allow only `europe-west2` and `us-central1`.",
        "wg": [
          { "t": "限制條件", "en": "constraint", "ps": "n" },
          { "t": "僅允許", "en": "allow only", "ps": "v" }
        ]
      },
      {
        "t": "(B) 建立一個 Cloud Logging Sink，將所有審計日誌 (Audit Logs) 匯出至一個啟用「Bucket Lock (Retention Policy)」的 Cloud Storage 儲存桶，設定保留期為 7 年。",
        "en": "(B) Create a Cloud Logging Sink to export all Audit Logs to a Cloud Storage bucket with 'Bucket Lock (Retention Policy)' enabled, setting the retention period to 7 years.",
        "wg": [
          { "t": "匯出", "en": "export", "ps": "v" },
          { "t": "保留期", "en": "retention period", "ps": "n" }
        ]
      },
      {
        "t": "(C) 使用 IAM Conditions，限制所有開發人員只能在名稱包含 `eu-` 或 `us-` 的專案中建立資源。",
        "en": "(C) Use IAM Conditions to restrict all developers to creating resources only in projects with names containing `eu-` or `us-`.",
        "wg": [
          { "t": "限制", "en": "restrict", "ps": "v" },
          { "t": "建立資源", "en": "creating resources", "ps": "n" }
        ]
      },
      {
        "t": "(D) 在 Cloud Logging 中設定自訂指標 (Custom Metric) 來監控資源建立位置，若發現違規區域則觸發 Cloud Function 自動刪除資源。",
        "en": "(D) Configure a Custom Metric in Cloud Logging to monitor resource creation locations; trigger a Cloud Function to automatically delete resources if a violation is detected.",
        "wg": [
          { "t": "違規", "en": "violation", "ps": "n" },
          { "t": "自動刪除", "en": "automatically delete", "ps": "v" }
        ]
      },
      {
        "t": "(E) 將所有日誌匯出至 BigQuery，並使用 `IAM` 角色鎖定該資料集，確保只有法務團隊可以刪除資料。",
        "en": "(E) Export all logs to BigQuery and use `IAM` roles to lock down the dataset, ensuring only the legal team can delete data.",
        "wg": [
          { "t": "鎖定", "en": "lock down", "ps": "v" },
          { "t": "法務團隊", "en": "legal team", "ps": "n" }
        ]
      }
    ],
    "answer": "(A), (B)",
    "why": {
      "t": "選項 (A) 和 (B) 結合提供了預防性控制與合規歸檔。選項 (A) 利用組織政策 (Organization Policy) 的資源位置限制 (Resource Locations constraint) 從根本上「預防」任何使用者在未授權區域建立資源，這是最有效的強制手段。選項 (B) 使用 Cloud Storage 的 Bucket Lock (WORM - Write Once Read Many) 功能來滿足「不可竄改」與「7 年留存」的法律需求。選項 (C) IAM Conditions 管理的是「誰」可以操作，而非資源的物理位置，且依賴專案命名不可靠；選項 (D) 是被動的偵測與補救，會有時間差且增加維運風險；選項 (E) BigQuery 雖然可以存儲日誌，但無法提供像 Storage Bucket Lock 那樣強制的「不可竄改 (Immutability)」保證（管理員仍可能誤刪）。",
      "en": "Options (A) and (B) combined provide preventive control and compliant archiving. Option (A) uses Organization Policy Resource Locations constraint to fundamentally 'prevent' any user from creating resources in unauthorized regions, which is the most effective enforcement mechanism. Option (B) uses Cloud Storage Bucket Lock (WORM - Write Once Read Many) to meet the legal requirements for 'tamper-proof' and '7-year retention'. Option (C) IAM Conditions manage 'who' can act, not the physical location of resources, and relying on project naming is unreliable; Option (D) is passive detection and remediation, introducing latency and operational risk; Option (E) while BigQuery can store logs, it cannot provide the enforced 'Immutability' guarantee (admins could still delete) that Storage Bucket Lock offers.",
      "wg": [
        { "t": "預防性控制", "en": "preventive control", "ps": "n" },
        { "t": "不可竄改", "en": "Immutability", "ps": "n" }
      ]
    }
  },{
    "no": "21",
    "level": "hard",
    "keywords": "GKE, Private Cluster, Security, Hybrid Connectivity",
    "question": [
      {
        "t": "Altostrat 的維運團隊需要透過 `kubectl` 工具從地端工作站管理 Google Cloud 上的 GKE 叢集。基於嚴格的安全政策，所有的 GKE 節點都不能擁有公共 IP 位址，且叢集的控制平面 (Control Plane) 必須禁止來自公共網際網路的存取，僅允許透過 Cloud Interconnect 的私有連線進行存取。",
        "en": "Altostrat's operations team needs to manage GKE clusters on Google Cloud from on-premises workstations using `kubectl`. Based on strict security policies, no GKE nodes can have public IP addresses, and the cluster's Control Plane must prohibit access from the public internet, allowing access only via the private connection of Cloud Interconnect.",
        "wg": [
          { "t": "控制平面", "en": "Control Plane", "ps": "n" },
          { "t": "禁止", "en": "prohibit", "ps": "v" }
        ]
      },
      {
        "t": "您需要配置 GKE 網路設定以滿足這些要求。請選擇兩項必要的設定。(請選擇兩項)",
        "en": "You need to configure GKE networking settings to meet these requirements. Choose two necessary settings. (Choose two)",
        "wg": [
          { "t": "網路設定", "en": "networking settings", "ps": "n" },
          { "t": "配置", "en": "configure", "ps": "v" }
        ]
      }
    ],
    "type": "複選題",
    "options": [
      {
        "t": "(A) 將叢集配置為私有叢集 (Private Cluster)。這將確保工作節點 (Worker Nodes) 僅配置 RFC 1918 私有 IP 位址。",
        "en": "(A) Configure the cluster as a Private Cluster. This ensures Worker Nodes are provisioned with only RFC 1918 private IP addresses.",
        "wg": [
          { "t": "私有叢集", "en": "Private Cluster", "ps": "n" },
          { "t": "工作節點", "en": "Worker Nodes", "ps": "n" }
        ]
      },
      {
        "t": "(B) 啟用主要授權網路 (Master Authorized Networks)，並僅將地端網路的 CIDR 範圍加入允許清單。",
        "en": "(B) Enable Master Authorized Networks and add only the on-premises network CIDR ranges to the allowlist.",
        "wg": [
          { "t": "主要授權網路", "en": "Master Authorized Networks", "ps": "n" },
          { "t": "允許清單", "en": "allowlist", "ps": "n" }
        ]
      },
      {
        "t": "(C) 停用控制平面公共端點存取 (Public Endpoint Access)，僅啟用私有端點 (Private Endpoint)。",
        "en": "(C) Disable Control Plane Public Endpoint Access and enable only the Private Endpoint.",
        "wg": [
          { "t": "公共端點存取", "en": "Public Endpoint Access", "ps": "n" },
          { "t": "私有端點", "en": "Private Endpoint", "ps": "n" }
        ]
      },
      {
        "t": "(D) 在地端網路與 VPC 之間設定 Cloud VPN 以加密流量，並在 VPC 防火牆規則中封鎖所有來自 `0.0.0.0/0` 的流量。",
        "en": "(D) Configure Cloud VPN between the on-premises network and the VPC to encrypt traffic, and block all traffic from `0.0.0.0/0` in VPC firewall rules.",
        "wg": [
          { "t": "加密流量", "en": "encrypt traffic", "ps": "v" },
          { "t": "封鎖", "en": "block", "ps": "v" }
        ]
      },
      {
        "t": "(E) 使用 Identity-Aware Proxy (IAP) 建立至控制平面的 TCP 通道，並授予維運團隊 `IAP-secured Tunnel User` 角色。",
        "en": "(E) Use Identity-Aware Proxy (IAP) to create a TCP tunnel to the control plane and grant the operations team the `IAP-secured Tunnel User` role.",
        "wg": [
          { "t": "TCP 通道", "en": "TCP tunnel", "ps": "n" },
          { "t": "角色", "en": "role", "ps": "n" }
        ]
      }
    ],
    "answer": "(A), (C)",
    "why": {
      "t": "選項 (A) 和 (C) 結合是實現完全隔離且安全的 GKE 叢集的標準配置。選項 (A) 「私有叢集」確保節點沒有公共 IP，滿足了「節點不能擁有公共 IP」的需求。選項 (C) 「停用公共端點」則強制控制平面只能透過私有 IP (Private Endpoint) 存取，這意味著流量必須經過與 VPC 互連的網路（如 Interconnect），從物理上隔絕了網際網路存取，滿足「禁止公共網際網路存取」的最嚴格要求。選項 (B) 雖然也是一種限制方式，但在公共端點已停用的情況下，存取控制主要由網路路由決定；若公共端點未停用，則 (B) 是必要的，但題目要求「禁止」存取，停用端點更為徹底；選項 (D) 使用 VPN 是多餘的（已有 Interconnect）；選項 (E) IAP 適用於存取節點上的 SSH 或內網應用，而非直接代理 Kubernetes API 伺服器流量。",
      "en": "Options (A) and (C) combined are the standard configuration for a fully isolated and secure GKE cluster. Option (A) 'Private Cluster' ensures nodes have no public IPs, meeting the 'no public IPs on nodes' requirement. Option (C) 'Disable Public Endpoint' forces the control plane to be accessible only via Private IP (Private Endpoint), meaning traffic must pass through networks peered with the VPC (like Interconnect), physically isolating it from internet access, which meets the strictest 'prohibit public internet access' requirement. Option (B) is also a restriction method, but if the public endpoint is disabled, access control is primarily determined by network routing; if the public endpoint were enabled, (B) would be necessary, but disabling the endpoint is more thorough for the 'prohibit' requirement; Option (D) using VPN is redundant (Interconnect exists); Option (E) IAP is for SSH to nodes or internal apps, not for directly proxying Kubernetes API server traffic.",
      "wg": [
        { "t": "完全隔離", "en": "fully isolated", "ps": "adj" },
        { "t": "物理上隔絕", "en": "physically isolating", "ps": "v" }
      ]
    }
  },
  {
    "no": "22",
    "level": "hard",
    "keywords": "Cloud Run Jobs, Video Processing, Serverless, Timeout",
    "question": [
      {
        "t": "Altostrat 使用 Cloud Run functions (第 2 代) 來處理使用者上傳的影片轉碼任務。雖然該解決方案對於短影片運作良好，但對於 4K 高畫質長影片，處理時間經常超過 60 分鐘，導致函式逾時 (Timeout) 並終止，造成資源浪費與使用者不滿。",
        "en": "Altostrat uses Cloud Run functions (2nd gen) to handle user-uploaded video transcoding tasks. While effective for short videos, for 4K high-definition long-form videos, processing time frequently exceeds 60 minutes, causing functions to Timeout and terminate, leading to wasted resources and user dissatisfaction.",
        "wg": [
          { "t": "轉碼任務", "en": "transcoding tasks", "ps": "n" },
          { "t": "逾時", "en": "Timeout", "ps": "n" }
        ]
      },
      {
        "t": "您需要建議一個替代的架構，能夠維持「無伺服器 (Serverless)」的營運模式以減少維護負擔，同時能夠可靠地處理執行時間較長的批次運算任務。該方案應允許任務平行執行以加速處理。",
        "en": "You need to recommend an alternative architecture that maintains the 'Serverless' operational model to reduce maintenance overhead, while reliably handling longer-running batch compute tasks. The solution should allow tasks to execute in parallel to accelerate processing.",
        "wg": [
          { "t": "無伺服器", "en": "Serverless", "ps": "adj" },
          { "t": "批次運算任務", "en": "batch compute tasks", "ps": "n" }
        ]
      },
      {
        "t": "您應該採用哪項 Google Cloud 服務？",
        "en": "Which Google Cloud service should you adopt?",
        "wg": [
          { "t": "採用", "en": "adopt", "ps": "v" },
          { "t": "服務", "en": "service", "ps": "n" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "(A) 遷移至 GKE Autopilot，並將轉碼程式碼部署為 Kubernetes Job。這消除了節點管理的負擔，且 Kubernetes Job 沒有嚴格的執行時間限制。",
        "en": "(A) Migrate to GKE Autopilot and deploy the transcoding code as a Kubernetes Job. This eliminates node management overhead, and Kubernetes Jobs do not have strict execution time limits.",
        "wg": [
          { "t": "節點管理", "en": "node management", "ps": "n" },
          { "t": "時間限制", "en": "time limits", "ps": "n" }
        ]
      },
      {
        "t": "(B) 增加 Cloud Run functions 的記憶體配置至 32GB，並向 Google 支援團隊申請將逾時限制延長至 120 分鐘。",
        "en": "(B) Increase the Cloud Run functions memory allocation to 32GB and request the Google support team to extend the timeout limit to 120 minutes.",
        "wg": [
          { "t": "記憶體配置", "en": "memory allocation", "ps": "n" },
          { "t": "延長", "en": "extend", "ps": "v" }
        ]
      },
      {
        "t": "(C) 使用 Cloud Run Jobs。將轉碼邏輯容器化，並將其部署為 Cloud Run 作業。這支援長達 24 小時的執行時間，且可輕鬆進行陣列式平行處理 (Array Jobs)。",
        "en": "(C) Use Cloud Run Jobs. Containerize the transcoding logic and deploy it as a Cloud Run Job. This supports execution times up to 24 hours and allows easy array-based parallel processing.",
        "wg": [
          { "t": "陣列式平行處理", "en": "array-based parallel processing", "ps": "n" },
          { "t": "執行時間", "en": "execution times", "ps": "n" }
        ]
      },
      {
        "t": "(D) 使用 Compute Engine 建立管理執行個體群組 (MIG)，並設定自動擴展政策。利用啟動腳本 (Startup Script) 從 Pub/Sub 拉取轉碼任務。",
        "en": "(D) Use Compute Engine to create a Managed Instance Group (MIG) with autoscaling policies. Use Startup Scripts to pull transcoding tasks from Pub/Sub.",
        "wg": [
          { "t": "管理執行個體群組", "en": "Managed Instance Group", "ps": "n" },
          { "t": "自動擴展", "en": "autoscaling", "ps": "n" }
        ]
      }
    ],
    "answer": "(C)",
    "why": {
      "t": "選項 (C) 是最符合需求的現代化解決方案。Cloud Run Jobs 專為那些不適合 HTTP 請求/回應模式的「非同步」、「長時間執行」任務設計，它保留了 Cloud Run 的無伺服器體驗（無需管理伺服器），但將執行時間限制大幅放寬（目前支援長達 24 小時），且原生支援將大任務分解為多個平行子任務，完美解決了轉碼逾時的問題。選項 (A) 雖然 GKE Autopilot 也是託管的，但引入 Kubernetes API 管理對於單純的批次任務來說增加了不必要的複雜度；選項 (B) Cloud Run 服務 (Services) 的逾時上限是硬性限制 (通常 60 分鐘)，無法透過支援單申請延長；選項 (D) 使用 Compute Engine 屬於 IaaS，違背了維持「無伺服器營運模式」的需求，需要維護 OS 與修補程式。",
      "en": "Option (C) is the most suitable modern solution. Cloud Run Jobs is designed for 'asynchronous', 'long-running' tasks that don't fit the HTTP request/response model. It retains the serverless experience of Cloud Run (no server management) but significantly extends execution time limits (currently supporting up to 24 hours) and natively supports splitting large tasks into multiple parallel subtasks, perfectly solving the transcoding timeout issue. Option (A) while GKE Autopilot is managed, introducing Kubernetes API management adds unnecessary complexity for simple batch tasks; Option (B) Cloud Run Services have a hard timeout limit (typically 60 minutes) that cannot be extended via support ticket; Option (D) using Compute Engine is IaaS, violating the requirement to maintain a 'Serverless operational model' by requiring OS maintenance and patching.",
      "wg": [
        { "t": "非同步", "en": "asynchronous", "ps": "adj" },
        { "t": "硬性限制", "en": "hard limit", "ps": "n" }
      ]
    }
  },
  {
    "no": "23",
    "level": "hard",
    "keywords": "Cloud Bigtable, Personalization, High Throughput, Low Latency",
    "question": [
      {
        "t": "Altostrat 的個人化推薦引擎需要即時存取全球數百萬使用者的「觀看歷史」與「偏好特徵向量」，以便在使用者開啟應用程式的瞬間 (<50ms) 產生推薦列表。該系統每秒需處理數十萬次的寫入（點擊流事件）與讀取請求。",
        "en": "Altostrat's personalized recommendation engine requires real-time access to the 'viewing history' and 'preference feature vectors' of millions of global users to generate recommendation lists the instant the user opens the app (<50ms). The system must handle hundreds of thousands of write (clickstream events) and read requests per second.",
        "wg": [
          { "t": "偏好特徵向量", "en": "preference feature vectors", "ps": "n" },
          { "t": "瞬間", "en": "instant", "ps": "n" }
        ]
      },
      {
        "t": "資料量預計將達到數百 PB。您需要選擇一個能夠在這種規模下維持毫秒級低延遲，並支援高吞吐量寫入的資料庫服務。關聯式語義與交易並非此工作負載的核心需求。",
        "en": "Data volume is expected to reach hundreds of petabytes. You need to choose a database service capable of maintaining millisecond-level low latency and supporting high-throughput writes at this scale. Relational semantics and transactions are not core requirements for this workload.",
        "wg": [
          { "t": "毫秒級", "en": "millisecond-level", "ps": "adj" },
          { "t": "核心需求", "en": "core requirements", "ps": "n" }
        ]
      },
      {
        "t": "哪種儲存解決方案最適合此情境？",
        "en": "Which storage solution is best suited for this scenario?",
        "wg": [
          { "t": "儲存解決方案", "en": "storage solution", "ps": "n" },
          { "t": "適合", "en": "suited", "ps": "v" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "(A) 使用 Cloud Bigtable。設計一個基於 `user_id#timestamp` 的 Row Key 架構，以確保寫入負載均勻分佈並支援高效的時間範圍查詢。",
        "en": "(A) Use Cloud Bigtable. Design a Row Key schema based on `user_id#timestamp` to ensure even distribution of write load and support efficient time-range queries.",
        "wg": [
          { "t": "均勻分佈", "en": "even distribution", "ps": "n" },
          { "t": "時間範圍查詢", "en": "time-range queries", "ps": "n" }
        ]
      },
      {
        "t": "(B) 使用 Firestore (Datastore 模式)。利用其自動索引功能來支援靈活的查詢，並啟用多區域複製以確保全球存取。",
        "en": "(B) Use Firestore (Datastore mode). Leverage its automatic indexing to support flexible queries and enable multi-region replication for global access.",
        "wg": [
          { "t": "自動索引", "en": "automatic indexing", "ps": "n" },
          { "t": "靈活的查詢", "en": "flexible queries", "ps": "n" }
        ]
      },
      {
        "t": "(C) 使用 BigQuery 並啟用串流插入 (Streaming Inserts)。BigQuery 強大的分析能力可以即時計算推薦結果並回傳給前端。",
        "en": "(C) Use BigQuery and enable Streaming Inserts. BigQuery's powerful analytics capabilities can calculate recommendation results in real-time and return them to the frontend.",
        "wg": [
          { "t": "串流插入", "en": "Streaming Inserts", "ps": "n" },
          { "t": "計算", "en": "calculate", "ps": "v" }
        ]
      },
      {
        "t": "(D) 使用 Cloud Spanner。利用其全域強一致性來確保使用者資料在所有區域都是最新的，防止推薦重複的內容。",
        "en": "(D) Use Cloud Spanner. Leverage its global strong consistency to ensure user data is up-to-date in all regions, preventing recommendation of duplicate content.",
        "wg": [
          { "t": "全域強一致性", "en": "global strong consistency", "ps": "n" },
          { "t": "重複的內容", "en": "duplicate content", "ps": "n" }
        ]
      }
    ],
    "answer": "(A)",
    "why": {
      "t": "選項 (A) Cloud Bigtable 是 Google 專為高吞吐量 (Throughput) 與低延遲 (Latency) 工作負載設計的 NoSQL 資料庫，是儲存個人化資料、時間序列事件與特徵向量的理想選擇（且被用於 Google Search, YouTube 等產品）。適當的 Row Key 設計能確保效能隨資料量線性擴展。選項 (B) Firestore 雖然易用，但在每秒數十萬次寫入的高吞吐量場景下成本極高且效能不如 Bigtable；選項 (C) BigQuery 是 OLAP 分析型資料庫，不適合用於毫秒級回應的線上服務 (OLTP/Serving)；選項 (D) Spanner 雖然強大，但對於「非關聯式」、「無需強一致性」且極度追求寫入吞吐量的個人化推薦場景來說，Bigtable 的成本效益與原始效能通常更優。",
      "en": "Option (A) Cloud Bigtable is Google's NoSQL database designed specifically for high throughput and low latency workloads, making it the ideal choice for storing personalization data, time-series events, and feature vectors (used by Google Search, YouTube, etc.). Proper Row Key design ensures linear performance scalability with data volume. Option (B) Firestore is easy to use but becomes cost-prohibitive and less performant than Bigtable in high-throughput scenarios with hundreds of thousands of writes per second; Option (C) BigQuery is an OLAP analytical database, unsuitable for millisecond-response online serving (OLTP/Serving); Option (D) while Spanner is powerful, for 'non-relational', 'no strong consistency required' scenarios demanding extreme write throughput like personalization, Bigtable typically offers better cost-efficiency and raw performance.",
      "wg": [
        { "t": "線性擴展", "en": "linear scalability", "ps": "n" },
        { "t": "成本效益", "en": "cost-efficiency", "ps": "n" }
      ]
    }
  },
  {
    "no": "24",
    "level": "hard",
    "keywords": "Cloud Deploy, GKE Enterprise, Canary Deployment, CI/CD",
    "question": [
      {
        "t": "Altostrat 希望現代化其 CI/CD 流程，特別是針對其旗艦級的生成式 AI 搜尋服務。由於該服務對營收至關重要，任何更新都必須採用「金絲雀部署 (Canary Deployment)」策略：先將 5% 的流量導向新版本，監控錯誤率與延遲，若指標正常則逐步增加流量。",
        "en": "Altostrat wants to modernize its CI/CD processes, specifically for its flagship Generative AI search service. Since this service is critical to revenue, any update must adopt a 'Canary Deployment' strategy: initially route 5% of traffic to the new version, monitor error rates and latency, and gradually increase traffic if metrics are normal.",
        "wg": [
          { "t": "旗艦級", "en": "flagship", "ps": "adj" },
          { "t": "逐步增加", "en": "gradually increase", "ps": "v" }
        ]
      },
      {
        "t": "您需要建立一個全託管的持續交付管道 (Continuous Delivery Pipeline)，該管道需與 GKE Enterprise 深度整合，並能自動執行上述的流量分割與驗收策略，盡量減少手動編寫複雜的腳本。",
        "en": "You need to build a fully managed Continuous Delivery Pipeline that integrates deeply with GKE Enterprise and can automatically execute the aforementioned traffic splitting and verification strategies, minimizing the need for manual complex scripting.",
        "wg": [
          { "t": "持續交付管道", "en": "Continuous Delivery Pipeline", "ps": "n" },
          { "t": "流量分割", "en": "traffic splitting", "ps": "n" }
        ]
      },
      {
        "t": "哪種工具組合最能滿足此需求？",
        "en": "Which combination of tools best meets this requirement?",
        "wg": [
          { "t": "工具組合", "en": "combination of tools", "ps": "n" },
          { "t": "滿足", "en": "meets", "ps": "v" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "(A) 使用 Jenkins 建立 pipeline，並編寫 `kubectl` 腳本來手動調整 Kubernetes Deployment 的 `replicas` 數量以模擬流量分割。",
        "en": "(A) Use Jenkins to create a pipeline and write `kubectl` scripts to manually adjust the number of `replicas` in the Kubernetes Deployment to simulate traffic splitting.",
        "wg": [
          { "t": "模擬", "en": "simulate", "ps": "v" },
          { "t": "手動調整", "en": "manually adjust", "ps": "v" }
        ]
      },
      {
        "t": "(B) 設定 Cloud Build 觸發器，在部署時使用 Spinnaker 進行藍/綠部署 (Blue/Green Deployment)，並透過負載平衡器切換 100% 的流量。",
        "en": "(B) Configure Cloud Build triggers to use Spinnaker for Blue/Green Deployment, switching 100% of the traffic via the load balancer.",
        "wg": [
          { "t": "藍/綠部署", "en": "Blue/Green Deployment", "ps": "n" },
          { "t": "切換", "en": "switching", "ps": "v" }
        ]
      },
      {
        "t": "(C) 使用 Cloud Functions 編寫自定義邏輯來修改 Istio 的 VirtualService 權重，並透過 Cloud Scheduler 定期觸發檢查。",
        "en": "(C) Use Cloud Functions to write custom logic to modify Istio VirtualService weights, and trigger checks periodically via Cloud Scheduler.",
        "wg": [
          { "t": "自定義邏輯", "en": "custom logic", "ps": "n" },
          { "t": "權重", "en": "weights", "ps": "n" }
        ]
      },
      {
        "t": "(D) 使用 Google Cloud Deploy 建立交付管道。定義 `canary` 部署策略，並利用服務網格 (Service Mesh) 或 Gateway API 的功能來自動執行基於權重的流量分割。",
        "en": "(D) Use Google Cloud Deploy to create a delivery pipeline. Define a `canary` deployment strategy and leverage Service Mesh or Gateway API capabilities to automatically execute weight-based traffic splitting.",
        "wg": [
          { "t": "交付管道", "en": "delivery pipeline", "ps": "n" },
          { "t": "基於權重的", "en": "weight-based", "ps": "adj" }
        ]
      }
    ],
    "answer": "(D)",
    "why": {
      "t": "選項 (D) 正確利用了 Google Cloud Deploy，這是 Google 推薦的全託管持續交付服務，專為 GKE 與 Cloud Run 設計。它原生支援 Canary 部署策略，能自動協調服務網格 (如 Cloud Service Mesh) 或 Gateway API 來精確控制流量百分比 (如 5%)，並整合了驗證與自動rollback功能。選項 (A) 使用副本數 (replicas) 來做金絲雀是非常粗糙且不精確的（難以達到精確的 5%）；選項 (B) 藍/綠部署通常是全量切換，不符合「逐步增加流量」的金絲雀定義，且引入 Spinnaker 增加了維運複雜度；選項 (C) 雖然可行，但屬於「手動編寫複雜腳本/邏輯」，不符合使用全託管服務以減少負擔的原則。",
      "en": "Option (D) correctly utilizes Google Cloud Deploy, the recommended fully managed continuous delivery service designed for GKE and Cloud Run. It natively supports Canary deployment strategies, automatically coordinating with Service Mesh (like Cloud Service Mesh) or Gateway API to precisely control traffic percentages (e.g., 5%), and integrates verification and automatic rollback capabilities. Option (A) using replica counts for canary is crude and imprecise (hard to hit exactly 5%); Option (B) Blue/Green deployment is typically an all-at-once switch, not fitting the 'gradually increase' definition of Canary, and introducing Spinnaker adds operational complexity; Option (C) while feasible, falls under 'manual complex scripting/logic', contradicting the principle of using managed services to reduce burden.",
      "wg": [
        { "t": "全託管", "en": "fully managed", "ps": "adj" },
        { "t": "協調", "en": "coordinating", "ps": "v" }
      ]
    }
  },
  {
    "no": "25",
    "level": "hard",
    "keywords": "CMEK, Compliance, Crypto-shredding, Cloud KMS",
    "question": [
      {
        "t": "Altostrat 擁有一些尚未公開的獨家電影內容，這些內容受到極其嚴格的版權保護。法律部門要求，如果發生疑似資料外洩或合約終止，必須能夠立即讓這些儲存在 Cloud Storage 中的資料「無法讀取」，即使資料實際上尚未從磁碟中刪除（加密清除 / Crypto-shredding）。",
        "en": "Altostrat possesses some unreleased exclusive movie content protected by extremely strict copyright. The legal department requires that if a suspected data breach or contract termination occurs, this data stored in Cloud Storage must be made 'unreadable' immediately, even if the data has not yet been physically deleted from the disk (Crypto-shredding).",
        "wg": [
          { "t": "加密清除", "en": "Crypto-shredding", "ps": "n" },
          { "t": "無法讀取", "en": "unreadable", "ps": "adj" }
        ]
      },
      {
        "t": "此外，金鑰的管理與輪替必須由 Altostrat 自行控制，且所有金鑰操作都必須有詳細的稽核紀錄。",
        "en": "Additionally, key management and rotation must be controlled by Altostrat itself, and all key operations must have detailed audit logs.",
        "wg": [
          { "t": "輪替", "en": "rotation", "ps": "n" },
          { "t": "稽核紀錄", "en": "audit logs", "ps": "n" }
        ]
      },
      {
        "t": "您應該實作哪種加密策略？",
        "en": "Which encryption strategy should you implement?",
        "wg": [
          { "t": "加密策略", "en": "encryption strategy", "ps": "n" },
          { "t": "實作", "en": "implement", "ps": "v" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "(A) 使用 Cloud Storage 預設加密 (Google-managed keys)。若需銷毀資料，則發送刪除請求至 Cloud Storage API，依靠 Google 的垃圾回收機制清除資料。",
        "en": "(A) Use Cloud Storage default encryption (Google-managed keys). To destroy data, send a delete request to the Cloud Storage API and rely on Google's garbage collection mechanism to purge data.",
        "wg": [
          { "t": "預設加密", "en": "default encryption", "ps": "n" },
          { "t": "垃圾回收", "en": "garbage collection", "ps": "n" }
        ]
      },
      {
        "t": "(B) 使用客戶管理加密金鑰 (CMEK) 與 Cloud KMS。將儲存桶設定為使用特定的 KMS 金鑰加密。若需銷毀資料，直接在 Cloud KMS 中「銷毀 (Destroy)」或「停用 (Disable)」該金鑰版本。",
        "en": "(B) Use Customer-Managed Encryption Keys (CMEK) with Cloud KMS. Configure the bucket to encrypt using a specific KMS key. To destroy data, directly 'Destroy' or 'Disable' that key version in Cloud KMS.",
        "wg": [
          { "t": "客戶管理加密金鑰", "en": "CMEK", "ps": "n" },
          { "t": "停用", "en": "Disable", "ps": "v" }
        ]
      },
      {
        "t": "(C) 使用客戶提供加密金鑰 (CSEK)。在每次上傳或下載檔案時，由應用程式提供原始的 AES-256 金鑰。若需銷毀資料，則在應用程式端刪除該金鑰。",
        "en": "(C) Use Customer-Supplied Encryption Keys (CSEK). The application provides the raw AES-256 key for every file upload or download. To destroy data, delete the key on the application side.",
        "wg": [
          { "t": "客戶提供加密金鑰", "en": "CSEK", "ps": "n" },
          { "t": "原始的", "en": "raw", "ps": "adj" }
        ]
      },
      {
        "t": "(D) 使用 Cloud External Key Manager (Cloud EKM) 將金鑰儲存在第三方的硬體安全模組 (HSM) 中。若需銷毀資料，則切斷 Google Cloud 與第三方 HSM 的網路連線。",
        "en": "(D) Use Cloud External Key Manager (Cloud EKM) to store keys in a third-party Hardware Security Module (HSM). To destroy data, sever the network connection between Google Cloud and the third-party HSM.",
        "wg": [
          { "t": "硬體安全模組", "en": "Hardware Security Module", "ps": "n" },
          { "t": "切斷", "en": "sever", "ps": "v" }
        ]
      }
    ],
    "answer": "(B)",
    "why": {
      "t": "選項 (B) CMEK 是實現「加密清除 (Crypto-shredding)」的標準且最易管理的方法。透過停用或銷毀 Cloud KMS 中的金鑰，所有使用該金鑰加密的數據將「立即」變得無法解密（無法讀取），這滿足了法律部門的緊急需求。此外，CMEK 提供了完整的 Cloud Audit Logs 整合。選項 (A) 預設加密不允許使用者控制金鑰銷毀，且刪除檔案後的實體清除有延遲；選項 (C) CSEK 雖然可以實現加密清除，但管理負擔極大（應用程式需追蹤每個物件的金鑰），且無法利用 Cloud KMS 的集中式稽核功能；選項 (D) EKM 適用於特定的合規需求（金鑰不能上雲），但對於單純的加密清除需求來說過於複雜且昂貴，CMEK 已足夠。",
      "en": "Option (B) CMEK is the standard and most manageable method for achieving 'Crypto-shredding'. By disabling or destroying the key in Cloud KMS, all data encrypted with that key becomes 'immediately' undecryptable (unreadable), meeting the legal department's urgent requirement. Additionally, CMEK provides full Cloud Audit Logs integration. Option (A) default encryption does not allow user control over key destruction, and physical purging after file deletion has a delay; Option (C) CSEK can achieve crypto-shredding but has massive management overhead (apps must track keys per object) and misses out on Cloud KMS's centralized auditing; Option (D) EKM is for specific compliance needs (keys cannot leave on-prem) but is overly complex and costly for simple crypto-shredding needs where CMEK suffices.",
      "wg": [
        { "t": "集中式稽核", "en": "centralized auditing", "ps": "n" },
        { "t": "無法解密", "en": "undecryptable", "ps": "adj" }
      ]
    }
  },{
    "no": "26",
    "level": "hard",
    "keywords": "Hybrid Connectivity, Cloud DNS, Name Resolution, Forwarding",
    "question": [
      {
        "t": "Altostrat 的 GKE 叢集上部署了新的內容管理微服務，該服務需要透過私有連線存取位於地端資料中心的舊有內容攝取系統 (Legacy Ingestion System)。地端系統使用內部 DNS 名稱 (如 `ingest.corp.altostrat.com`) 進行識別。",
        "en": "New content management microservices deployed on Altostrat's GKE clusters need to access the legacy Ingestion System located in the on-premises data center via a private connection. The on-premises system is identified using internal DNS names (e.g., `ingest.corp.altostrat.com`).",
        "wg": [
          { "t": "內容攝取系統", "en": "Ingestion System", "ps": "n" },
          { "t": "內部 DNS 名稱", "en": "internal DNS names", "ps": "n" }
        ]
      },
      {
        "t": "目前的 GKE Pod 無法解析這些地端主機名稱。您需要配置一個高可用且低延遲的混合 DNS 解析架構，讓 Google Cloud 資源能夠解析地端名稱，同時地端伺服器也能解析 Google Cloud 的資源名稱，且無需在地端維護複雜的 DNS 轉發規則。",
        "en": "Current GKE Pods cannot resolve these on-premises hostnames. You need to configure a highly available and low-latency hybrid DNS resolution architecture that allows Google Cloud resources to resolve on-premises names, and vice versa, without maintaining complex DNS forwarding rules on-premises.",
        "wg": [
          { "t": "解析", "en": "resolve", "ps": "v" },
          { "t": "轉發規則", "en": "forwarding rules", "ps": "n" }
        ]
      },
      {
        "t": "您應該如何設定 Cloud DNS？",
        "en": "How should you configure Cloud DNS?",
        "wg": [
          { "t": "設定", "en": "configure", "ps": "v" },
          { "t": "Cloud DNS", "en": "Cloud DNS", "ps": "n" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "(A) 在 Google Cloud 上建立一個私有區域 (Private Zone) 用於 `corp.altostrat.com`，並手動為每個地端伺服器新增 A 記錄；使用 Cloud Scheduler 定期同步地端 DNS 伺服器的變更。",
        "en": "(A) Create a Private Zone on Google Cloud for `corp.altostrat.com` and manually add A records for each on-premises server; use Cloud Scheduler to periodically synchronize changes from on-premises DNS servers.",
        "wg": [
          { "t": "私有區域", "en": "Private Zone", "ps": "n" },
          { "t": "A 記錄", "en": "A records", "ps": "n" }
        ]
      },
      {
        "t": "(B) 設定 Cloud DNS 轉送區域 (Forwarding Zone)，將 `corp.altostrat.com` 的查詢轉發至地端 DNS 伺服器的 IP 位址；同時配置地端 DNS 伺服器將 Google Cloud 網域的查詢條件轉發 (Conditional Forwarding) 至 Cloud DNS 的入站轉發器 IP (Inbound Forwarder IP)。",
        "en": "(B) Configure a Cloud DNS Forwarding Zone to forward queries for `corp.altostrat.com` to the IP addresses of on-premises DNS servers; simultaneously configure on-premises DNS servers to strictly perform Conditional Forwarding for Google Cloud domains to the Cloud DNS Inbound Forwarder IP.",
        "wg": [
          { "t": "轉送區域", "en": "Forwarding Zone", "ps": "n" },
          { "t": "入站轉發器", "en": "Inbound Forwarder", "ps": "n" }
        ]
      },
      {
        "t": "(C) 在地端安裝 BIND DNS 伺服器並配置為次要區域 (Secondary Zone)，將 Cloud DNS 設定為主要區域 (Primary Zone) 並啟用 DNSSEC；透過 VPN 通道進行區域傳輸 (Zone Transfer)。",
        "en": "(C) Install a BIND DNS server on-premises configured as a Secondary Zone, setting Cloud DNS as the Primary Zone and enabling DNSSEC; perform Zone Transfer via the VPN tunnel.",
        "wg": [
          { "t": "次要區域", "en": "Secondary Zone", "ps": "n" },
          { "t": "區域傳輸", "en": "Zone Transfer", "ps": "n" }
        ]
      },
      {
        "t": "(D) 修改 GKE 叢集的 `CoreDNS` ConfigMap，新增一個存根區域 (Stub Domain) 指向地端 DNS 伺服器；這將允許 Pod 直接查詢地端，無需經過 Cloud DNS。",
        "en": "(D) Modify the GKE cluster's `CoreDNS` ConfigMap to add a Stub Domain pointing to the on-premises DNS servers; this will allow Pods to query on-premises directly without going through Cloud DNS.",
        "wg": [
          { "t": "存根區域", "en": "Stub Domain", "ps": "n" },
          { "t": "無需經過", "en": "without going through", "ps": "v" }
        ]
      }
    ],
    "answer": "(B)",
    "why": {
      "t": "選項 (B) 是 Google Cloud 推薦的混合雲 DNS 解析架構。Cloud DNS 轉送區域 (Forwarding Zone) 讓 GCP 資源能夠查詢地端；而 Cloud DNS 入站轉發政策 (Inbound Server Policy) 提供了一個內部 IP，供地端 DNS 伺服器轉發 GCP 相關的查詢。這實現了雙向解析，且無需手動同步記錄。選項 (A) 手動維護 A 記錄或使用排程同步極不穩定且容易出錯；選項 (C) Cloud DNS 不支援作為傳統 DNS 區域傳輸 (AXFR) 的主要區域；選項 (D) 雖然修改 CoreDNS 技術上可行，但它增加了 GKE 的維運負擔，且無法解決非 GKE 資源 (如 Compute Engine VM) 的解析問題，不是全平台的解決方案。",
      "en": "Option (B) is the recommended hybrid cloud DNS resolution architecture. Cloud DNS Forwarding Zones allow GCP resources to query on-premises; Cloud DNS Inbound Server Policies provide an internal IP for on-premises DNS servers to forward GCP-related queries to. This achieves bi-directional resolution without manual record synchronization. Option (A) manually maintaining A records or using scheduled sync is highly unstable and error-prone; Option (C) Cloud DNS does not support acting as a Primary Zone for traditional DNS Zone Transfers (AXFR); Option (D) while modifying CoreDNS is technically feasible, it adds operational burden to GKE and does not solve resolution issues for non-GKE resources (like Compute Engine VMs), making it not a platform-wide solution.",
      "wg": [
        { "t": "雙向解析", "en": "bi-directional resolution", "ps": "n" },
        { "t": "全平台", "en": "platform-wide", "ps": "adj" }
      ]
    }
  },
  {
    "no": "27",
    "level": "hard",
    "keywords": "Apigee, Monetization, API Management, Security",
    "question": [
      {
        "t": "Altostrat 希望透過向合作夥伴與第三方開發者開放其龐大的媒體內容目錄 API 來「開啟新的營收來源」。業務需求要求必須具備「精細的費率限制 (Rate Limiting)」、「基於使用量的計費 (Usage-based Billing)」以及針對不同合作夥伴層級的「服務品質 (QoS) 管理」。",
        "en": "Altostrat wants to 'unlock new revenue streams' by opening its vast media content catalog API to partners and third-party developers. Business requirements dictate the need for 'granular Rate Limiting', 'Usage-based Billing', and 'Quality of Service (QoS) management' for different partner tiers.",
        "wg": [
          { "t": "費率限制", "en": "Rate Limiting", "ps": "n" },
          { "t": "服務品質", "en": "QoS", "ps": "n" }
        ]
      },
      {
        "t": "此外，安全性至關重要，必須防止 API 金鑰遭竊後的濫用，並能即時偵測異常流量模式。您需要推薦一個能夠滿足這些商業與技術需求的 API 管理平台。",
        "en": "Additionally, security is paramount; the solution must prevent abuse of stolen API keys and detect anomalous traffic patterns in real-time. You need to recommend an API management platform that satisfies these commercial and technical requirements.",
        "wg": [
          { "t": "濫用", "en": "abuse", "ps": "n" },
          { "t": "異常流量", "en": "anomalous traffic", "ps": "n" }
        ]
      },
      {
        "t": "該解決方案應盡量減少自建計費系統的開發工作。",
        "en": "The solution should minimize the development effort of building a custom billing system.",
        "wg": [
          { "t": "自建", "en": "building a custom", "ps": "v" },
          { "t": "計費系統", "en": "billing system", "ps": "n" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "(A) 使用 Cloud Endpoints 部署 API，並啟用 Service Control API 進行配額檢查；將日誌匯出至 BigQuery，並使用 Data Studio 產生使用量報表，再由財務團隊手動開立發票。",
        "en": "(A) Deploy APIs using Cloud Endpoints and enable Service Control API for quota checks; export logs to BigQuery and use Data Studio to generate usage reports, which the finance team uses to manually issue invoices.",
        "wg": [
          { "t": "配額檢查", "en": "quota checks", "ps": "n" },
          { "t": "開立發票", "en": "issue invoices", "ps": "v" }
        ]
      },
      {
        "t": "(B) 採用 Apigee X 平台。利用其內建的貨幣化 (Monetization) 模組來設定費率計畫 (Rate Plans) 與計費報告；使用 Apigee Advanced API Security 來偵測機器人攻擊與異常行為。",
        "en": "(B) Adopt the Apigee X platform. Leverage its built-in Monetization module to configure Rate Plans and billing reports; use Apigee Advanced API Security to detect bot attacks and anomalous behavior.",
        "wg": [
          { "t": "貨幣化", "en": "Monetization", "ps": "n" },
          { "t": "費率計畫", "en": "Rate Plans", "ps": "n" }
        ]
      },
      {
        "t": "(C) 在 GKE Ingress Controller 上設定 NGINX Rate Limiting 註釋；開發一個 Sidecar 服務來攔截每個請求，計算使用量並寫入 Cloud Spanner，以此作為計費依據。",
        "en": "(C) Configure NGINX Rate Limiting annotations on the GKE Ingress Controller; develop a Sidecar service to intercept every request, calculate usage, and write to Cloud Spanner as the basis for billing.",
        "wg": [
          { "t": "攔截", "en": "intercept", "ps": "v" },
          { "t": "計費依據", "en": "basis for billing", "ps": "n" }
        ]
      },
      {
        "t": "(D) 使用 API Gateway 作為前端，結合 Cloud Functions 實作驗證邏輯；利用 Firestore 記錄每個 API Key 的剩餘點數，並在每次請求時進行原子扣款 (Atomic Decrement)。",
        "en": "(D) Use API Gateway as the frontend, combined with Cloud Functions to implement authentication logic; use Firestore to record remaining credits for each API Key and perform an Atomic Decrement on every request.",
        "wg": [
          { "t": "剩餘點數", "en": "remaining credits", "ps": "n" },
          { "t": "原子扣款", "en": "Atomic Decrement", "ps": "n" }
        ]
      }
    ],
    "answer": "(B)",
    "why": {
      "t": "選項 (B) 直接回應了「開啟營收來源」與「減少自建計費系統」的需求。Apigee X 是 Google Cloud 最進階的 API 管理平台，其 Monetization 模組專門用於處理複雜的費率計畫、預付/後付模式與收入分成，且 Advanced API Security 提供了 AI 驅動的異常偵測。選項 (A) Cloud Endpoints 功能較基礎，缺乏原生的計費/發票生成引擎，需要大量手動工作或額外開發；選項 (C) 與 (D) 都涉及大量的客製化開發（Sidecar, Firestore 扣款邏輯），這增加了維護負擔且容易出現並發問題（如計數不準），不符合企業級營收系統的穩定性要求。",
      "en": "Option (B) directly addresses the requirements to 'unlock revenue streams' and 'minimize custom billing system development'. Apigee X is Google Cloud's most advanced API management platform; its Monetization module is specifically designed to handle complex rate plans, prepaid/postpaid models, and revenue sharing, while Advanced API Security provides AI-driven anomaly detection. Option (A) Cloud Endpoints is more basic and lacks a native billing/invoicing engine, requiring significant manual work or extra development; Options (C) and (D) both involve extensive custom development (Sidecar, Firestore deduction logic), increasing maintenance burden and prone to concurrency issues (like inaccurate counting), failing to meet the stability requirements of an enterprise revenue system.",
      "wg": [
        { "t": "進階的", "en": "advanced", "ps": "adj" },
        { "t": "維護負擔", "en": "maintenance burden", "ps": "n" }
      ]
    }
  },
  {
    "no": "28",
    "level": "hard",
    "keywords": "Cloud Armor, DDoS Protection, Security, Global Load Balancing",
    "question": [
      {
        "t": "Altostrat 的公開媒體網站最近遭受了來自特定地理區域的第 7 層 (HTTP) 分散式阻斷服務 (DDoS) 攻擊，導致合法使用者的回應時間變慢。攻擊流量特徵是偽造的 User-Agent 字串與高頻率的 `/login` 請求。",
        "en": "Altostrat's public media website recently suffered a Layer 7 (HTTP) Distributed Denial of Service (DDoS) attack from specific geographic regions, causing slow response times for legitimate users. The attack traffic is characterized by spoofed User-Agent strings and high-frequency `/login` requests.",
        "wg": [
          { "t": "第 7 層", "en": "Layer 7", "ps": "n" },
          { "t": "偽造的", "en": "spoofed", "ps": "adj" }
        ]
      },
      {
        "t": "您需要部署一個安全解決方案來立即緩解這些攻擊，並建立長期的防禦機制以根據風險評分自動攔截可疑流量，同時確保對合法使用者的影響降至最低。",
        "en": "You need to deploy a security solution to immediately mitigate these attacks and establish a long-term defense mechanism that automatically blocks suspicious traffic based on risk scores, while ensuring minimal impact on legitimate users.",
        "wg": [
          { "t": "緩解", "en": "mitigate", "ps": "v" },
          { "t": "攔截", "en": "blocks", "ps": "v" }
        ]
      },
      {
        "t": "您的架構已使用外部 HTTP(S) 負載平衡器。",
        "en": "Your architecture already utilizes an External HTTP(S) Load Balancer.",
        "wg": [
          { "t": "架構", "en": "architecture", "ps": "n" },
          { "t": "已使用", "en": "utilizes", "ps": "v" }
        ]
      },
      {
        "t": "您應該採取哪項行動？",
        "en": "What action should you take?",
        "wg": [
          { "t": "採取", "en": "take", "ps": "v" },
          { "t": "行動", "en": "action", "ps": "n" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "(A) 設定 VPC 防火牆規則以封鎖來自攻擊來源國家的所有 IP 範圍；實作 `iptables` 規則在 VM 層級過濾特定的 User-Agent。",
        "en": "(A) Configure VPC firewall rules to block all IP ranges from the attack source countries; implement `iptables` rules to filter specific User-Agents at the VM level.",
        "wg": [
          { "t": "封鎖", "en": "block", "ps": "v" },
          { "t": "過濾", "en": "filter", "ps": "v" }
        ]
      },
      {
        "t": "(B) 配置 Google Cloud Armor 安全政策並將其連結至負載平衡器的後端服務。使用自定義規則封鎖惡意 User-Agent，並啟用 Adaptive Protection 以自動學習基準流量並建議緩解規則。",
        "en": "(B) Configure Google Cloud Armor security policies and attach them to the load balancer's backend service. Use custom rules to block malicious User-Agents, and enable Adaptive Protection to automatically learn baseline traffic and suggest mitigation rules.",
        "wg": [
          { "t": "安全政策", "en": "security policies", "ps": "n" },
          { "t": "自適應保護", "en": "Adaptive Protection", "ps": "n" }
        ]
      },
      {
        "t": "(C) 將應用程式遷移至 Cloud CDN，並啟用「簽署 Cookie」功能。要求所有使用者必須先解決 reCAPTCHA 挑戰才能獲得 Cookie 以存取網站內容。",
        "en": "(C) Migrate the application to Cloud CDN and enable the 'Signed Cookies' feature. Require all users to solve a reCAPTCHA challenge before obtaining a Cookie to access website content.",
        "wg": [
          { "t": "簽署 Cookie", "en": "Signed Cookies", "ps": "n" },
          { "t": "挑戰", "en": "challenge", "ps": "n" }
        ]
      },
      {
        "t": "(D) 使用 Cloud Functions 編寫一個中間件 (Middleware) 來檢查每個請求的來源 IP 與標頭。若偵測到異常，則呼叫 Compute Engine API 將該 IP 加入 VPC 防火牆黑名單。",
        "en": "(D) Use Cloud Functions to write a middleware to inspect the source IP and headers of every request. If an anomaly is detected, call the Compute Engine API to add the IP to the VPC firewall blocklist.",
        "wg": [
          { "t": "中間件", "en": "Middleware", "ps": "n" },
          { "t": "黑名單", "en": "blocklist", "ps": "n" }
        ]
      }
    ],
    "answer": "(B)",
    "why": {
      "t": "選項 (B) 正確應用了 Google Cloud Armor，這是專為全球負載平衡器設計的 DDoS 防護與 WAF 服務。它能在邊緣 (Edge) 阻擋第 7 層攻擊，避免惡意流量進入後端。Adaptive Protection (自適應保護) 利用機器學習自動偵測異常流量模式（如高頻率登入）並產生防護規則，完美符合「自動攔截可疑流量」的需求。選項 (A) VPC 防火牆是第 3/4 層防護，無法解析 HTTP User-Agent，且封鎖整個國家會誤殺合法使用者；選項 (C) 強制所有使用者進行 reCAPTCHA 會嚴重破壞使用者體驗 (UX)；選項 (D) 使用 Cloud Functions 作為中間件會引入巨大的延遲與成本，且 VPC 防火牆有規則數量限制，無法應對大規模 DDoS IP 列表。",
      "en": "Option (B) correctly applies Google Cloud Armor, the DDoS protection and WAF service designed for Global Load Balancers. It blocks Layer 7 attacks at the edge, preventing malicious traffic from reaching the backend. Adaptive Protection uses machine learning to automatically detect anomalous traffic patterns (like high-frequency logins) and generate protection rules, perfectly fitting the 'automatically block suspicious traffic' requirement. Option (A) VPC firewall provides Layer 3/4 protection and cannot parse HTTP User-Agents, and blocking entire countries affects legitimate users; Option (C) forcing reCAPTCHA on all users severely degrades User Experience (UX); Option (D) using Cloud Functions as middleware introduces massive latency and cost, and VPC firewalls have rule limits, making them unable to handle large-scale DDoS IP lists.",
      "wg": [
        { "t": "邊緣", "en": "edge", "ps": "n" },
        { "t": "誤殺", "en": "false positives", "ps": "n" }
      ]
    }
  },
  {
    "no": "29",
    "level": "hard",
    "keywords": "BigQuery, Slots, Reservations, Performance Isolation",
    "question": [
      {
        "t": "Altostrat 的數據分析團隊與高階管理層共用同一個 BigQuery 資料倉儲。最近，管理層抱怨每週一早上的關鍵業務儀表板載入速度極慢。調查發現，同一時間數據科學團隊正在執行大量且複雜的機器學習訓練查詢，耗盡了所有的運算資源。",
        "en": "Altostrat's data analytics team and executive management share the same BigQuery data warehouse. Recently, management has complained that critical business dashboards load extremely slowly on Monday mornings. Investigation revealed that the data science team is running massive and complex machine learning training queries at the same time, exhausting all compute resources.",
        "wg": [
          { "t": "儀表板", "en": "dashboards", "ps": "n" },
          { "t": "耗盡", "en": "exhausting", "ps": "v" }
        ]
      },
      {
        "t": "您需要重新架構 BigQuery 的資源分配，確保管理層的查詢始終擁有保證的效能 (SLA)，同時允許數據科學團隊在資源閒置時使用額外的算力。解決方案應避免建立孤立的資料複本。",
        "en": "You need to re-architect BigQuery resource allocation to ensure management queries always have guaranteed performance (SLA), while allowing the data science team to use extra compute power when resources are idle. The solution should avoid creating isolated data copies.",
        "wg": [
          { "t": "資源分配", "en": "resource allocation", "ps": "n" },
          { "t": "保證的效能", "en": "guaranteed performance", "ps": "n" }
        ]
      },
      {
        "t": "您應該採取哪種策略？",
        "en": "Which strategy should you adopt?",
        "wg": [
          { "t": "策略", "en": "strategy", "ps": "n" },
          { "t": "採取", "en": "adopt", "ps": "v" }
        ]
      }
    ],
    "type": "單選題",
    "options": [
      {
        "t": "(A) 購買 BigQuery Editions (Enterprise 或 Plus)。建立一個 Reservations 資源，並劃分兩個「指派 (Assignments)」：給管理層專案分配固定的「基準 (Baseline)」插槽 (Slots)；給數據科學專案分配較低的基準插槽，但允許其借用閒置插槽 (Idle Slots)。",
        "en": "(A) Purchase BigQuery Editions (Enterprise or Plus). Create a Reservations resource and define two 'Assignments': allocate fixed 'Baseline' Slots to the management project; allocate lower Baseline Slots to the data science project but allow it to borrow Idle Slots.",
        "wg": [
          { "t": "基準", "en": "Baseline", "ps": "n" },
          { "t": "閒置插槽", "en": "Idle Slots", "ps": "n" }
        ]
      },
      {
        "t": "(B) 將管理層的數據匯出至另一個專用的 BigQuery 專案，並使用「按需計費 (On-demand pricing)」模式，因為按需模式保證每個查詢有 2000 個插槽。",
        "en": "(B) Export management data to another dedicated BigQuery project and use the 'On-demand pricing' model, as the on-demand model guarantees 2000 slots per query.",
        "wg": [
          { "t": "按需計費", "en": "On-demand pricing", "ps": "n" },
          { "t": "專用的", "en": "dedicated", "ps": "adj" }
        ]
      },
      {
        "t": "(C) 為數據科學團隊的查詢設定 `batch` 優先級，並限制其查詢必須在非辦公時間執行；將管理層查詢設定為 `interactive` 優先級。",
        "en": "(C) Set the `batch` priority for data science team queries and restrict their execution to non-business hours; set management queries to `interactive` priority.",
        "wg": [
          { "t": "優先級", "en": "priority", "ps": "n" },
          { "t": "非辦公時間", "en": "non-business hours", "ps": "n" }
        ]
      },
      {
        "t": "(D) 使用 Cloud Spanner 作為管理層儀表板的後端資料庫，因為 Spanner 提供更好的並發控制；保持數據科學團隊使用 BigQuery。",
        "en": "(D) Use Cloud Spanner as the backend database for management dashboards because Spanner offers better concurrency control; keep the data science team on BigQuery.",
        "wg": [
          { "t": "並發控制", "en": "concurrency control", "ps": "n" },
          { "t": "後端資料庫", "en": "backend database", "ps": "n" }
        ]
      }
    ],
    "answer": "(A)",
    "why": {
      "t": "選項 (A) 利用 BigQuery Reservations (現透過 Editions 銷售) 完美解決了「效能隔離」與「資源共享」的矛盾。透過分配專屬插槽 (Baseline Slots) 給管理層專案，保證了其查詢效能不受干擾；同時，「閒置插槽借用」機制允許數據科學團隊在管理層不查詢時使用這些資源，最大化了投資回報 (ROI)。選項 (B) 複製數據違反了「避免建立孤立複本」的要求，且按需模式是共享資源池，無法提供嚴格的 SLA 保證；選項 (C) 僅依靠批次優先級無法保證即時資源隔離（若叢集滿載），且限制工作時間會降低研發效率；選項 (D) 遷移至 Spanner 工程浩大，且 Spanner 不適合執行複雜的 OLAP 聚合分析查詢。",
      "en": "Option (A) uses BigQuery Reservations (now sold via Editions) to perfectly resolve the conflict between 'performance isolation' and 'resource sharing'. By allocating dedicated Baseline Slots to the management project, query performance is guaranteed against interference; meanwhile, the 'idle slot borrowing' mechanism allows the data science team to utilize these resources when management isn't querying, maximizing ROI. Option (B) duplicating data violates the 'avoid isolated copies' requirement, and on-demand is a shared resource pool that cannot provide strict SLA guarantees; Option (C) relying solely on batch priority doesn't guarantee real-time isolation (if the cluster is full) and restricting work hours hampers R&D efficiency; Option (D) migrating to Spanner is a massive engineering effort, and Spanner is ill-suited for complex OLAP aggregation queries.",
      "wg": [
        { "t": "效能隔離", "en": "performance isolation", "ps": "n" },
        { "t": "投資回報", "en": "ROI", "ps": "n" }
      ]
    }
  },
  {
    "no": "30",
    "level": "hard",
    "keywords": "Database Migration, Zero Downtime, Cloud SQL, MySQL",
    "question": [
      {
        "t": "Altostrat 正計劃將一個關鍵的舊有內容索引資料庫 (Legacy Content Indexing Database, MySQL 8.0) 從地端遷移至 Cloud SQL for MySQL。該資料庫支援核心的內容檢索 API，業務要求遷移過程中的停機時間必須少於 5 分鐘。",
        "en": "Altostrat is planning to migrate a critical Legacy Content Indexing Database (MySQL 8.0) from on-premises to Cloud SQL for MySQL. This database supports the core content retrieval API, and business requirements dictate that downtime during migration must be less than 5 minutes.",
        "wg": [
          { "t": "停機時間", "en": "downtime", "ps": "n" },
          { "t": "核心的", "en": "core", "ps": "adj" }
        ]
      },
      {
        "t": "遷移完成後，您需要能夠在發生意外問題時快速將流量切回地端資料庫 (Fallback)，且不遺失遷移後新產生的數據。應用程式已配置為透過私有連線存取資料庫。",
        "en": "After the migration is complete, you need to be able to quickly switch traffic back to the on-premises database (Fallback) in case of unexpected issues, without losing new data generated after the migration. The application is configured to access the database via a private connection.",
        "wg": [
          { "t": "切回", "en": "switch back", "ps": "v" },
          { "t": "遺失", "en": "losing", "ps": "v" }
        ]
      },
      {
        "t": "請選擇兩項步驟來達成此高可用遷移策略。(請選擇兩項)",
        "en": "Choose two steps to achieve this high-availability migration strategy. (Choose two)",
        "wg": [
          { "t": "高可用", "en": "high-availability", "ps": "adj" },
          { "t": "達成", "en": "achieve", "ps": "v" }
        ]
      }
    ],
    "type": "複選題",
    "options": [
      {
        "t": "(A) 使用 Database Migration Service (DMS) 建立遷移作業，並將遷移類型設定為「持續性 (Continuous)」，這將利用 MySQL 的二進位日誌 (binlog) 進行複製。",
        "en": "(A) Use Database Migration Service (DMS) to create a migration job, setting the migration type to 'Continuous', which utilizes MySQL binary logs (binlog) for replication.",
        "wg": [
          { "t": "持續性", "en": "Continuous", "ps": "n" },
          { "t": "二進位日誌", "en": "binary logs", "ps": "n" }
        ]
      },
      {
        "t": "(B) 執行 `mysqldump` 匯出資料，上傳至 Cloud Storage，然後匯入 Cloud SQL。在匯入期間暫停應用程式寫入。",
        "en": "(B) Execute `mysqldump` to export data, upload to Cloud Storage, and import into Cloud SQL. Pause application writes during the import.",
        "wg": [
          { "t": "匯出", "en": "export", "ps": "v" },
          { "t": "暫停", "en": "Pause", "ps": "v" }
        ]
      },
      {
        "t": "(C) 在地端資料庫上設定外部主機 (External Master)，將 Cloud SQL 設定為地端的讀取複本 (Read Replica)。在切換時，將 Cloud SQL 晉升 (Promote) 為主實例。",
        "en": "(C) Configure an External Master on the on-premises database and set Cloud SQL as a Read Replica of the on-premises instance. Promote Cloud SQL to primary instance during cutover.",
        "wg": [
          { "t": "外部主機", "en": "External Master", "ps": "n" },
          { "t": "晉升", "en": "Promote", "ps": "v" }
        ]
      },
      {
        "t": "(D) 在 DMS 中啟用「反向複製 (Reverse Replication)」功能。當 Cloud SQL 晉升為主機後，DMS 會自動將 Cloud SQL 的新寫入同步回地端資料庫。",
        "en": "(D) Enable the 'Reverse Replication' feature in DMS. After Cloud SQL is promoted to primary, DMS automatically synchronizes new writes from Cloud SQL back to the on-premises database.",
        "wg": [
          { "t": "反向複製", "en": "Reverse Replication", "ps": "n" },
          { "t": "同步回", "en": "synchronizes ... back", "ps": "v" }
        ]
      },
      {
        "t": "(E) 使用 Cloud Spanner 作為中介層，利用其雙向寫入功能同時寫入地端與 Cloud SQL，直到確認穩定為止。",
        "en": "(E) Use Cloud Spanner as an intermediary layer, leveraging its bi-directional write capability to write to both on-premises and Cloud SQL simultaneously until stability is confirmed.",
        "wg": [
          { "t": "中介層", "en": "intermediary layer", "ps": "n" },
          { "t": "雙向寫入", "en": "bi-directional write", "ps": "n" }
        ]
      }
    ],
    "answer": "(A), (D)",
    "why": {
      "t": "選項 (A) 和 (D) 構成了使用 Database Migration Service (DMS) 進行最小停機遷移的最佳實踐。選項 (A) 的「持續性」遷移利用 binlog 複製，允許在資料同步期間地端 DB 繼續服務，直到切換瞬間 (Cutover)，這保證了停機時間極短 (< 5 分鐘)。選項 (D) 的「反向複製」是針對「Fallback 且不遺失數據」的關鍵功能；它確保在切換到 Cloud SQL 後，任何新寫入的數據都會即時同步回地端。若 Cloud SQL 出現問題，可以立即將流量切回擁有最新數據的地端 DB。選項 (B) `mysqldump` 需要長時間停機，不符合 < 5 分鐘的要求；選項 (C) 是 DMS 出現之前的手動做法 (External Master)，現在 DMS 自動化了這個流程，且手動設定反向複製極為複雜；選項 (E) 架構過於複雜且不切實際。",
      "en": "Options (A) and (D) constitute the best practice for minimal downtime migration using Database Migration Service (DMS). Option (A) 'Continuous' migration leverages binlog replication, allowing the on-premises DB to continue serving traffic during sync until the Cutover moment, ensuring downtime is minimal (< 5 minutes). Option (D) 'Reverse Replication' is the critical feature for 'Fallback without data loss'; it ensures that after cutting over to Cloud SQL, any new writes are immediately synced back to on-premises. If Cloud SQL fails, traffic can be instantly switched back to the on-premises DB which holds the latest data. Option (B) `mysqldump` requires long downtime, failing the < 5 minutes requirement; Option (C) is the manual approach (External Master) before DMS existed, DMS now automates this, and manually configuring reverse replication is extremely complex; Option (E) is overly complex and impractical.",
      "wg": [
        { "t": "切換瞬間", "en": "Cutover moment", "ps": "n" },
        { "t": "反向複製", "en": "Reverse Replication", "ps": "n" }
      ]
    }
  }
]
